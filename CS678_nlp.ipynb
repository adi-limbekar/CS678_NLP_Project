{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"0ceJz-0b0JnY","executionInfo":{"status":"ok","timestamp":1683512210053,"user_tz":240,"elapsed":48599,"user":{"displayName":"stephen dias","userId":"16065654066476337384"}},"outputId":"b11e79a8-a039-48f7-a2db-893c36759e83","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import torch\n","torch.cuda.empty_cache()\n","\n","# Confirm that the GPU is detected\n","\n","assert torch.cuda.is_available()\n","\n","# Get the GPU device name.\n","device_name = torch.cuda.get_device_name()\n","n_gpu = torch.cuda.device_count()\n","print(f\"Found device: {device_name}, n_gpu: {n_gpu}\")\n","device = torch.device(\"cuda\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"33RpQ5smA4qT","executionInfo":{"status":"ok","timestamp":1683512239587,"user_tz":240,"elapsed":5895,"user":{"displayName":"stephen dias","userId":"16065654066476337384"}},"outputId":"58c7cfb4-ad18-4e59-836a-5e3f2b0e35e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found device: Tesla T4, n_gpu: 1\n"]}]},{"cell_type":"code","source":["!pip install transformers simpletransformers pandas==1.2.5 scikit-learn==0.23.1 tqdm==4.62.3\n","!pip install transformers\n","!pip install -U -q PyDrive"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dlS7q01mA8O8","executionInfo":{"status":"ok","timestamp":1683512478927,"user_tz":240,"elapsed":234436,"user":{"displayName":"stephen dias","userId":"16065654066476337384"}},"outputId":"cfe213fe-53ea-4c5a-f1de-3bd0704f39ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting simpletransformers\n","  Downloading simpletransformers-0.63.11-py3-none-any.whl (250 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.7/250.7 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pandas==1.2.5\n","  Downloading pandas-1.2.5.tar.gz (5.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting scikit-learn==0.23.1\n","  Downloading scikit-learn-0.23.1.tar.gz (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","\n","\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n","\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Using cached transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"]}]},{"cell_type":"code","source":["!pip install simpletransformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1HoIJ62kytQv","executionInfo":{"status":"ok","timestamp":1683512587723,"user_tz":240,"elapsed":31068,"user":{"displayName":"stephen dias","userId":"16065654066476337384"}},"outputId":"3d075c1c-1500-4e30-d6c2-2b0ac8ef63f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting simpletransformers\n","  Using cached simpletransformers-0.63.11-py3-none-any.whl (250 kB)\n","Collecting datasets\n","  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.27.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.10.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.2.2)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.5.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.22.4)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.12.2)\n","Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.13.3)\n","Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (4.65.0)\n","Collecting streamlit\n","  Downloading streamlit-1.22.0-py2.py3-none-any.whl (8.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: transformers>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (4.28.1)\n","Collecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2022.10.31)\n","Collecting wandb>=0.10.32\n","  Downloading wandb-0.15.2-py3-none-any.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->simpletransformers) (0.14.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->simpletransformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->simpletransformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->simpletransformers) (3.12.0)\n","Collecting GitPython!=3.1.29,>=1.0.0\n","  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (8.1.3)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (3.20.3)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.22.1-py2.py3-none-any.whl (203 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.1/203.1 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (5.9.5)\n","Collecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting setproctitle\n","  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Collecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (67.7.2)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (3.4)\n","Collecting dill<0.3.7,>=0.3.0\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (2023.4.0)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting xxhash\n","  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (9.0.0)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2022.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2.8.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->simpletransformers) (3.1.0)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->simpletransformers) (1.2.0)\n","Collecting pympler>=0.9\n","  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (0.10.2)\n","Requirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (4.3)\n","Collecting pydeck>=0.1.dev5\n","  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m112.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (5.3.0)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (13.3.4)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (8.4.0)\n","Requirement already satisfied: altair<5,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (4.2.2)\n","Collecting blinker>=1.0.0\n","  Downloading blinker-1.6.2-py3-none-any.whl (13 kB)\n","Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (6.2)\n","Requirement already satisfied: tenacity<9,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (8.2.2)\n","Collecting importlib-metadata>=1.4\n","  Downloading importlib_metadata-6.6.0-py3-none-any.whl (22 kB)\n","Collecting watchdog\n","  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting validators>=0.2\n","  Downloading validators-0.20.0.tar.gz (30 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (4.5.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (0.40.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (3.4.3)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.54.0)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.0.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (2.17.3)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.8.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.4.0)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (2.3.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (0.7.0)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<5,>=3.2.0->streamlit->simpletransformers) (4.3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<5,>=3.2.0->streamlit->simpletransformers) (3.1.2)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<5,>=3.2.0->streamlit->simpletransformers) (0.12.0)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<5,>=3.2.0->streamlit->simpletransformers) (0.4)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.10.32->simpletransformers) (1.16.0)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (23.1.0)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->simpletransformers) (1.3.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->streamlit->simpletransformers) (3.15.0)\n","Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->streamlit->simpletransformers) (2.2.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->streamlit->simpletransformers) (2.14.0)\n","Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.10/dist-packages (from tzlocal>=1.1->streamlit->simpletransformers) (0.1.0.post0)\n","Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from validators>=0.2->streamlit->simpletransformers) (4.4.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (2.1.2)\n","Collecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<5,>=3.2.0->streamlit->simpletransformers) (0.19.3)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->streamlit->simpletransformers) (0.1.2)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->simpletransformers) (3.2.2)\n","Requirement already satisfied: tzdata in /usr/local/lib/python3.10/dist-packages (from pytz-deprecation-shim->tzlocal>=1.1->streamlit->simpletransformers) (2023.3)\n","Building wheels for collected packages: seqeval, validators, pathtools\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=43032480efe7403f949654519602cb08a93fdd391e93cbc590e580e27623c6bb\n","  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n","  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19579 sha256=1d1eea048371885c3668da6837d074e0cac603de5f2277e6d8c9d625f7e6535f\n","  Stored in directory: /root/.cache/pip/wheels/f2/ed/dd/d3a556ad245ef9dc570c6bcd2f22886d17b0b408dd3bbb9ac3\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=a6e19f1586e00675fd25afa4077604afe7d9f4887677743ec8cb175e6984d123\n","  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n","Successfully built seqeval validators pathtools\n","Installing collected packages: sentencepiece, pathtools, xxhash, watchdog, validators, smmap, setproctitle, sentry-sdk, pympler, multidict, importlib-metadata, frozenlist, docker-pycreds, dill, blinker, async-timeout, yarl, responses, pydeck, multiprocess, gitdb, aiosignal, seqeval, GitPython, aiohttp, wandb, streamlit, datasets, simpletransformers\n","Successfully installed GitPython-3.1.31 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 blinker-1.6.2 datasets-2.12.0 dill-0.3.6 docker-pycreds-0.4.0 frozenlist-1.3.3 gitdb-4.0.10 importlib-metadata-6.6.0 multidict-6.0.4 multiprocess-0.70.14 pathtools-0.1.2 pydeck-0.8.1b0 pympler-1.0.1 responses-0.18.0 sentencepiece-0.1.99 sentry-sdk-1.22.1 seqeval-1.2.2 setproctitle-1.3.2 simpletransformers-0.63.11 smmap-5.0.0 streamlit-1.22.0 validators-0.20.0 wandb-0.15.2 watchdog-3.0.0 xxhash-3.2.0 yarl-1.9.2\n"]}]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/depression-detection-lt-edi-2022/dataset'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aU4YSJKxAkAE","executionInfo":{"status":"ok","timestamp":1683514397717,"user_tz":240,"elapsed":4,"user":{"displayName":"stephen dias","userId":"16065654066476337384"}},"outputId":"c4ac18e9-e2aa-475b-e69c-7659c23021e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1xD1ZIqHdatgu8BZr1nav5pFJeOkI8nOi/depression-detection-lt-edi-2022/dataset\n"]}]},{"cell_type":"markdown","source":["# Cleaning and creating the train and dev datasets"],"metadata":{"id":"qq4GxXzwtxK7"}},{"cell_type":"code","source":["!python preprocess_dataset.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P2DgueRw466d","executionInfo":{"status":"ok","timestamp":1683515517290,"user_tz":240,"elapsed":760,"user":{"displayName":"stephen dias","userId":"16065654066476337384"}},"outputId":"cfaa5216-db8c-4e80-a4bc-cb8dc42feb7d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original train_multilingual_robustness_1\n","severe: 480\n","moderate: 2059\n","not depression: 1488\n","all: 4027\n","-------------------------------------\n","Original train_multilingual_robustness_1 - without duplicates\n","severe: 480\n","moderate: 2059\n","not depression: 1488\n","all: 4027\n","-------------------------------------\n","Original dev\n","severe: 360\n","moderate: 2306\n","not depression: 1830\n","all: 4496\n","-------------------------------------\n","Original dev - without duplicates\n","severe: 360\n","moderate: 2304\n","not depression: 1817\n","all: 4481\n","-------------------------------------\n","Train after preprocessing\n","severe: 470\n","moderate: 2009\n","not depression: 1448\n","all: 3927\n","-------------------------------------\n"]}]},{"cell_type":"code","source":["!python reddit_depression_corpora.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0iWggiZ7A8WU","executionInfo":{"status":"ok","timestamp":1680744194036,"user_tz":240,"elapsed":107256,"user":{"displayName":"Aditya Limbekar","userId":"04179261916482495738"}},"outputId":"d3641b64-bacb-4a81-8df7-f004079c0390"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["100% 50/50 [01:37<00:00,  1.96s/it]\n","depression: 72916 (27.1%)\n","suicidewatch: 34747 (12.9%)\n","lonely: 20105 (7.5%)\n","socialanxiety: 19697 (7.3%)\n","fitness: 10000 (3.7%)\n","EDAnonymous: 10000 (3.7%)\n","guns: 10000 (3.7%)\n","jokes: 10000 (3.7%)\n","legaladvice: 10000 (3.7%)\n","mentalhealth: 10000 (3.7%)\n","personalfinance: 10000 (3.7%)\n","parenting: 10000 (3.7%)\n","relationships: 10000 (3.7%)\n","meditation: 7852 (2.9%)\n","schizophrenia: 6660 (2.5%)\n","healthanxiety: 5436 (2.0%)\n","teaching: 3516 (1.3%)\n","alcoholism: 3087 (1.1%)\n","divorce: 2745 (1.0%)\n","ptsd: 2097 (0.8%)\n","\n","Unique texts: 268858 (train: 263480 / validation: 5378).\n"]}]},{"cell_type":"code","source":["%cd .."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Bho-sN0jcib","executionInfo":{"status":"ok","timestamp":1683515633085,"user_tz":240,"elapsed":414,"user":{"displayName":"stephen dias","userId":"16065654066476337384"}},"outputId":"4c66dddd-acc0-48b4-842e-0ffccecaab3d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1xD1ZIqHdatgu8BZr1nav5pFJeOkI8nOi/depression-detection-lt-edi-2022\n"]}]},{"cell_type":"code","source":["%cd models"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7VEIsNVIKs2U","executionInfo":{"status":"ok","timestamp":1683515633621,"user_tz":240,"elapsed":5,"user":{"displayName":"stephen dias","userId":"16065654066476337384"}},"outputId":"25643db3-1428-4d60-ead9-8cec8212c148"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1xD1ZIqHdatgu8BZr1nav5pFJeOkI8nOi/depression-detection-lt-edi-2022/models\n"]}]},{"cell_type":"code","source":["!python models_list.py"],"metadata":{"id":"2CSThJfU0WQ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python transformers_models.py"],"metadata":{"id":"AoFn98r0J_Q-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# finetune the model"],"metadata":{"id":"gjqcGtqjtq2x"}},{"cell_type":"code","source":["!python finetune.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eVl4AK7CEOyn","executionInfo":{"status":"ok","timestamp":1683518002304,"user_tz":240,"elapsed":2355068,"user":{"displayName":"stephen dias","userId":"16065654066476337384"}},"outputId":"fd204309-4f5a-4506-cb8f-bf6b586a27ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-05-08 03:14:13.275355: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Fine-tuning\troberta\troberta-large\tv1\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  0% 8/3927 [00:05<48:51,  1.34it/s]\n","Epoch 1 of 2:   0% 0/2 [00:00<?, ?it/s]\n","Running Epoch 0 of 2:   0% 0/246 [00:00<?, ?it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1405:   0% 0/246 [00:00<?, ?it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1405:   0% 1/246 [00:01<06:04,  1.49s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.1419:   0% 1/246 [00:01<06:04,  1.49s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.1419:   1% 2/246 [00:02<04:19,  1.06s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.1184:   1% 2/246 [00:02<04:19,  1.06s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.1184:   1% 3/246 [00:03<03:45,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0889:   1% 3/246 [00:03<03:45,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0889:   2% 4/246 [00:03<03:39,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0282:   2% 4/246 [00:04<03:39,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0282:   2% 5/246 [00:04<03:34,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0589:   2% 5/246 [00:05<03:34,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0589:   2% 6/246 [00:05<03:31,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0928:   2% 6/246 [00:06<03:31,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0928:   3% 7/246 [00:06<03:30,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0991:   3% 7/246 [00:06<03:30,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0991:   3% 8/246 [00:07<03:29,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0685:   3% 8/246 [00:07<03:29,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0685:   4% 9/246 [00:08<03:27,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9805:   4% 9/246 [00:08<03:27,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9805:   4% 10/246 [00:09<03:25,  1.15it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1192:   4% 10/246 [00:09<03:25,  1.15it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1192:   4% 11/246 [00:09<03:24,  1.15it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1024:   4% 11/246 [00:10<03:24,  1.15it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1024:   5% 12/246 [00:10<03:24,  1.15it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1105:   5% 12/246 [00:11<03:24,  1.15it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1105:   5% 13/246 [00:11<03:23,  1.15it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0580:   5% 13/246 [00:12<03:23,  1.15it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0580:   6% 14/246 [00:12<03:22,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0204:   6% 14/246 [00:12<03:22,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0204:   6% 15/246 [00:13<03:22,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9600:   6% 15/246 [00:13<03:22,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9600:   7% 16/246 [00:14<03:21,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0800:   7% 16/246 [00:14<03:21,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0800:   7% 17/246 [00:15<03:20,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0608:   7% 17/246 [00:15<03:20,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0608:   7% 18/246 [00:16<03:19,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0104:   7% 18/246 [00:16<03:19,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0104:   8% 19/246 [00:16<03:18,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0660:   8% 19/246 [00:17<03:18,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0660:   8% 20/246 [00:17<03:18,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9901:   8% 20/246 [00:18<03:18,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9901:   9% 21/246 [00:18<03:18,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0067:   9% 21/246 [00:19<03:18,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0067:   9% 22/246 [00:19<03:17,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9926:   9% 22/246 [00:20<03:17,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9926:   9% 23/246 [00:20<03:16,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9958:   9% 23/246 [00:20<03:16,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9958:  10% 24/246 [00:21<03:15,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8530:  10% 24/246 [00:21<03:15,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8530:  10% 25/246 [00:22<03:15,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0684:  10% 25/246 [00:22<03:15,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0684:  11% 26/246 [00:23<03:14,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8432:  11% 26/246 [00:23<03:14,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8432:  11% 27/246 [00:24<03:12,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9794:  11% 27/246 [00:24<03:12,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9794:  11% 28/246 [00:24<03:10,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0651:  11% 28/246 [00:25<03:10,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0651:  12% 29/246 [00:25<03:03,  1.18it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0639:  12% 29/246 [00:26<03:03,  1.18it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0639:  12% 30/246 [00:26<03:05,  1.17it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8149:  12% 30/246 [00:26<03:05,  1.17it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8149:  13% 31/246 [00:27<03:05,  1.16it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7787:  13% 31/246 [00:27<03:05,  1.16it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7787:  13% 32/246 [00:28<03:06,  1.15it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0279:  13% 32/246 [00:28<03:06,  1.15it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0279:  13% 33/246 [00:29<03:06,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8943:  13% 33/246 [00:29<03:06,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8943:  14% 34/246 [00:30<03:05,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0182:  14% 34/246 [00:30<03:05,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0182:  14% 35/246 [00:30<03:05,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8687:  14% 35/246 [00:31<03:05,  1.14it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8687:  15% 36/246 [00:31<03:05,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9792:  15% 36/246 [00:32<03:05,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9792:  15% 37/246 [00:32<03:04,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9292:  15% 37/246 [00:33<03:04,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9292:  15% 38/246 [00:33<03:04,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9158:  15% 38/246 [00:34<03:04,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9158:  16% 39/246 [00:34<03:04,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1289:  16% 39/246 [00:34<03:04,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1289:  16% 40/246 [00:35<03:03,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9505:  16% 40/246 [00:35<03:03,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9505:  17% 41/246 [00:36<03:02,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9183:  17% 41/246 [00:36<03:02,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9183:  17% 42/246 [00:37<03:01,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0785:  17% 42/246 [00:37<03:01,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0785:  17% 43/246 [00:38<03:00,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1994:  17% 43/246 [00:38<03:00,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1994:  18% 44/246 [00:39<03:00,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.3401:  18% 44/246 [00:39<03:00,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.3401:  18% 45/246 [00:39<02:58,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9718:  18% 45/246 [00:40<02:58,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9718:  19% 46/246 [00:40<02:57,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0510:  19% 46/246 [00:41<02:57,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0510:  19% 47/246 [00:41<02:56,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0466:  19% 47/246 [00:42<02:56,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0466:  20% 48/246 [00:42<02:55,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9780:  20% 48/246 [00:42<02:55,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9780:  20% 49/246 [00:43<02:54,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8181:  20% 49/246 [00:43<02:54,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8181:  20% 50/246 [00:44<02:54,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9487:  20% 50/246 [00:44<02:54,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9487:  21% 51/246 [00:45<02:53,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0254:  21% 51/246 [00:45<02:53,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0254:  21% 52/246 [00:46<02:52,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8605:  21% 52/246 [00:46<02:52,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8605:  22% 53/246 [00:47<02:51,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0922:  22% 53/246 [00:47<02:51,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0922:  22% 54/246 [00:47<02:52,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9080:  22% 54/246 [00:48<02:52,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9080:  22% 55/246 [00:48<02:50,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0322:  22% 55/246 [00:49<02:50,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0322:  23% 56/246 [00:49<02:50,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1230:  23% 56/246 [00:50<02:50,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1230:  23% 57/246 [00:50<02:49,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9124:  23% 57/246 [00:51<02:49,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9124:  24% 58/246 [00:51<02:49,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0449:  24% 58/246 [00:51<02:49,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0449:  24% 59/246 [00:52<02:48,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1479:  24% 59/246 [00:52<02:48,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1479:  24% 60/246 [00:53<02:47,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9975:  24% 60/246 [00:53<02:47,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9975:  25% 61/246 [00:54<02:46,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9669:  25% 61/246 [00:54<02:46,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9669:  25% 62/246 [00:55<02:45,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1158:  25% 62/246 [00:55<02:45,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1158:  26% 63/246 [00:56<02:45,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0043:  26% 63/246 [00:56<02:45,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0043:  26% 64/246 [00:56<02:43,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9045:  26% 64/246 [00:57<02:43,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9045:  26% 65/246 [00:57<02:42,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8979:  26% 65/246 [00:58<02:42,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8979:  27% 66/246 [00:58<02:42,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1940:  27% 66/246 [00:59<02:42,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1940:  27% 67/246 [00:59<02:41,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8678:  27% 67/246 [01:00<02:41,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8678:  28% 68/246 [01:00<02:41,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8677:  28% 68/246 [01:00<02:41,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8677:  28% 69/246 [01:01<02:40,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9634:  28% 69/246 [01:01<02:40,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9634:  28% 70/246 [01:02<02:39,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9020:  28% 70/246 [01:02<02:39,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9020:  29% 71/246 [01:03<02:39,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0103:  29% 71/246 [01:03<02:39,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0103:  29% 72/246 [01:04<02:38,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8127:  29% 72/246 [01:04<02:38,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8127:  30% 73/246 [01:05<02:38,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9204:  30% 73/246 [01:05<02:38,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9204:  30% 74/246 [01:06<02:36,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0747:  30% 74/246 [01:06<02:36,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0747:  30% 75/246 [01:06<02:35,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9660:  30% 75/246 [01:07<02:35,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9660:  31% 76/246 [01:07<02:35,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0284:  31% 76/246 [01:08<02:35,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0284:  31% 77/246 [01:08<02:34,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9863:  31% 77/246 [01:09<02:34,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9863:  32% 78/246 [01:09<02:34,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0500:  32% 78/246 [01:10<02:34,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0500:  32% 79/246 [01:10<02:33,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0258:  32% 79/246 [01:11<02:33,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0258:  33% 80/246 [01:11<02:31,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9166:  33% 80/246 [01:11<02:31,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9166:  33% 81/246 [01:12<02:30,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0302:  33% 81/246 [01:12<02:30,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0302:  33% 82/246 [01:13<02:29,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9413:  33% 82/246 [01:13<02:29,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9413:  34% 83/246 [01:14<02:28,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1479:  34% 83/246 [01:14<02:28,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1479:  34% 84/246 [01:15<02:27,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8935:  34% 84/246 [01:15<02:27,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8935:  35% 85/246 [01:16<02:27,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1401:  35% 85/246 [01:16<02:27,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1401:  35% 86/246 [01:17<02:26,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0289:  35% 86/246 [01:17<02:26,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0289:  35% 87/246 [01:17<02:25,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8810:  35% 87/246 [01:18<02:25,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8810:  36% 88/246 [01:18<02:24,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9207:  36% 88/246 [01:19<02:24,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9207:  36% 89/246 [01:19<02:23,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0393:  36% 89/246 [01:20<02:23,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0393:  37% 90/246 [01:20<02:22,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8778:  37% 90/246 [01:21<02:22,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8778:  37% 91/246 [01:21<02:21,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9461:  37% 91/246 [01:21<02:21,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9461:  37% 92/246 [01:22<02:20,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0497:  37% 92/246 [01:22<02:20,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0497:  38% 93/246 [01:23<02:20,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8389:  38% 93/246 [01:23<02:20,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8389:  38% 94/246 [01:24<02:20,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9924:  38% 94/246 [01:24<02:20,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9924:  39% 95/246 [01:25<02:19,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0220:  39% 95/246 [01:25<02:19,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0220:  39% 96/246 [01:26<02:19,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8563:  39% 96/246 [01:26<02:19,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8563:  39% 97/246 [01:27<02:17,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2065:  39% 97/246 [01:27<02:17,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2065:  40% 98/246 [01:28<02:16,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7576:  40% 98/246 [01:28<02:16,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7576:  40% 99/246 [01:28<02:10,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7994:  40% 99/246 [01:29<02:10,  1.13it/s]\u001b[A\n","\n","  0% 0/600 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","  0% 1/600 [00:02<20:20,  2.04s/it]\u001b[A\u001b[A\n","\n","  0% 2/600 [00:02<11:16,  1.13s/it]\n","\n","Epochs 0/2. Running Loss:    0.7994:  41% 100/246 [02:22<40:38, 16.70s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9193:  41% 100/246 [02:22<40:38, 16.70s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9193:  41% 101/246 [02:23<28:53, 11.95s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.7478:  41% 101/246 [02:23<28:53, 11.95s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.7478:  41% 102/246 [02:24<20:45,  8.65s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.3626:  41% 102/246 [02:24<20:45,  8.65s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.3626:  42% 103/246 [02:25<15:06,  6.34s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0090:  42% 103/246 [02:25<15:06,  6.34s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0090:  42% 104/246 [02:26<11:09,  4.72s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8397:  42% 104/246 [02:26<11:09,  4.72s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8397:  43% 105/246 [02:27<08:24,  3.58s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0072:  43% 105/246 [02:27<08:24,  3.58s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0072:  43% 106/246 [02:27<06:29,  2.78s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9646:  43% 106/246 [02:28<06:29,  2.78s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9646:  43% 107/246 [02:28<05:10,  2.24s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8379:  43% 107/246 [02:29<05:10,  2.24s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8379:  44% 108/246 [02:29<04:14,  1.84s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0689:  44% 108/246 [02:30<04:14,  1.84s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0689:  44% 109/246 [02:30<03:35,  1.57s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.7670:  44% 109/246 [02:31<03:35,  1.57s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.7670:  45% 110/246 [02:31<03:08,  1.39s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8138:  45% 110/246 [02:32<03:08,  1.39s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8138:  45% 111/246 [02:32<02:49,  1.26s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9481:  45% 111/246 [02:33<02:49,  1.26s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9481:  46% 112/246 [02:33<02:36,  1.17s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.7942:  46% 112/246 [02:34<02:36,  1.17s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.7942:  46% 113/246 [02:34<02:27,  1.11s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8770:  46% 113/246 [02:35<02:27,  1.11s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8770:  46% 114/246 [02:35<02:20,  1.07s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0693:  46% 114/246 [02:36<02:20,  1.07s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0693:  47% 115/246 [02:36<02:14,  1.02s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9527:  47% 115/246 [02:36<02:14,  1.02s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9527:  47% 116/246 [02:37<02:10,  1.00s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.1189:  47% 116/246 [02:37<02:10,  1.00s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.1189:  48% 117/246 [02:38<02:07,  1.02it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1699:  48% 117/246 [02:38<02:07,  1.02it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1699:  48% 118/246 [02:39<02:04,  1.03it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8742:  48% 118/246 [02:39<02:04,  1.03it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8742:  48% 119/246 [02:40<02:02,  1.03it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7936:  48% 119/246 [02:40<02:02,  1.03it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7936:  49% 120/246 [02:41<02:01,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2971:  49% 120/246 [02:41<02:01,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2971:  49% 121/246 [02:42<01:59,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.3101:  49% 121/246 [02:42<01:59,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.3101:  50% 122/246 [02:43<01:58,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2585:  50% 122/246 [02:43<01:58,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2585:  50% 123/246 [02:44<01:57,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8705:  50% 123/246 [02:44<01:57,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8705:  50% 124/246 [02:45<01:57,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9333:  50% 124/246 [02:45<01:57,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9333:  51% 125/246 [02:46<01:55,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1243:  51% 125/246 [02:46<01:55,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1243:  51% 126/246 [02:47<01:54,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9848:  51% 126/246 [02:47<01:54,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9848:  52% 127/246 [02:47<01:53,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9698:  52% 127/246 [02:48<01:53,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9698:  52% 128/246 [02:48<01:52,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8148:  52% 128/246 [02:49<01:52,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8148:  52% 129/246 [02:49<01:51,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9195:  52% 129/246 [02:50<01:51,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9195:  53% 130/246 [02:50<01:51,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0856:  53% 130/246 [02:51<01:51,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0856:  53% 131/246 [02:51<01:50,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8379:  53% 131/246 [02:52<01:50,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8379:  54% 132/246 [02:52<01:49,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9368:  54% 132/246 [02:53<01:49,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9368:  54% 133/246 [02:53<01:48,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8885:  54% 133/246 [02:54<01:48,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8885:  54% 134/246 [02:54<01:47,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0932:  54% 134/246 [02:55<01:47,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0932:  55% 135/246 [02:55<01:46,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0148:  55% 135/246 [02:56<01:46,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0148:  55% 136/246 [02:56<01:45,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9833:  55% 136/246 [02:57<01:45,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9833:  56% 137/246 [02:57<01:43,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7502:  56% 137/246 [02:57<01:43,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7502:  56% 138/246 [02:58<01:43,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9652:  56% 138/246 [02:58<01:43,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9652:  57% 139/246 [02:59<01:42,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7435:  57% 139/246 [02:59<01:42,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7435:  57% 140/246 [03:00<01:41,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0466:  57% 140/246 [03:00<01:41,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0466:  57% 141/246 [03:01<01:39,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0439:  57% 141/246 [03:01<01:39,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0439:  58% 142/246 [03:02<01:38,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9301:  58% 142/246 [03:02<01:38,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9301:  58% 143/246 [03:03<01:38,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9527:  58% 143/246 [03:03<01:38,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9527:  59% 144/246 [03:04<01:37,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0381:  59% 144/246 [03:04<01:37,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0381:  59% 145/246 [03:05<01:36,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0552:  59% 145/246 [03:05<01:36,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0552:  59% 146/246 [03:06<01:35,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8549:  59% 146/246 [03:06<01:35,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8549:  60% 147/246 [03:07<01:34,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8864:  60% 147/246 [03:07<01:34,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8864:  60% 148/246 [03:08<01:33,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9921:  60% 148/246 [03:08<01:33,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9921:  61% 149/246 [03:08<01:32,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2332:  61% 149/246 [03:09<01:32,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2332:  61% 150/246 [03:09<01:31,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0706:  61% 150/246 [03:10<01:31,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0706:  61% 151/246 [03:10<01:30,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8639:  61% 151/246 [03:11<01:30,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8639:  62% 152/246 [03:11<01:29,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8832:  62% 152/246 [03:12<01:29,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8832:  62% 153/246 [03:12<01:27,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9039:  62% 153/246 [03:13<01:27,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9039:  63% 154/246 [03:13<01:26,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8941:  63% 154/246 [03:14<01:26,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8941:  63% 155/246 [03:14<01:25,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2001:  63% 155/246 [03:15<01:25,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2001:  63% 156/246 [03:15<01:24,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8678:  63% 156/246 [03:15<01:24,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8678:  64% 157/246 [03:16<01:23,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0505:  64% 157/246 [03:16<01:23,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0505:  64% 158/246 [03:17<01:23,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9303:  64% 158/246 [03:17<01:23,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9303:  65% 159/246 [03:18<01:22,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9088:  65% 159/246 [03:18<01:22,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9088:  65% 160/246 [03:19<01:20,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9106:  65% 160/246 [03:19<01:20,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9106:  65% 161/246 [03:20<01:20,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0916:  65% 161/246 [03:20<01:20,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0916:  66% 162/246 [03:21<01:18,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7732:  66% 162/246 [03:21<01:18,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7732:  66% 163/246 [03:22<01:17,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8784:  66% 163/246 [03:22<01:17,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8784:  67% 164/246 [03:23<01:16,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8949:  67% 164/246 [03:23<01:16,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8949:  67% 165/246 [03:24<01:16,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9236:  67% 165/246 [03:24<01:16,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9236:  67% 166/246 [03:24<01:14,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8351:  67% 166/246 [03:25<01:14,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8351:  68% 167/246 [03:25<01:14,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0918:  68% 167/246 [03:26<01:14,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0918:  68% 168/246 [03:26<01:13,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8653:  68% 168/246 [03:27<01:13,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8653:  69% 169/246 [03:27<01:12,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0286:  69% 169/246 [03:28<01:12,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0286:  69% 170/246 [03:28<01:11,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1313:  69% 170/246 [03:29<01:11,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1313:  70% 171/246 [03:29<01:10,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9928:  70% 171/246 [03:30<01:10,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9928:  70% 172/246 [03:30<01:09,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9307:  70% 172/246 [03:30<01:09,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9307:  70% 173/246 [03:31<01:08,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8736:  70% 173/246 [03:31<01:08,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8736:  71% 174/246 [03:32<01:07,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0953:  71% 174/246 [03:32<01:07,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0953:  71% 175/246 [03:33<01:05,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9654:  71% 175/246 [03:33<01:05,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9654:  72% 176/246 [03:34<01:05,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9467:  72% 176/246 [03:34<01:05,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9467:  72% 177/246 [03:35<01:04,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0848:  72% 177/246 [03:35<01:04,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0848:  72% 178/246 [03:36<01:03,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9710:  72% 178/246 [03:36<01:03,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9710:  73% 179/246 [03:37<01:02,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8212:  73% 179/246 [03:37<01:02,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8212:  73% 180/246 [03:38<01:01,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9180:  73% 180/246 [03:38<01:01,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9180:  74% 181/246 [03:38<01:00,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9597:  74% 181/246 [03:39<01:00,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9597:  74% 182/246 [03:39<00:59,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7277:  74% 182/246 [03:40<00:59,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7277:  74% 183/246 [03:40<00:59,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9156:  74% 183/246 [03:41<00:59,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9156:  75% 184/246 [03:41<00:58,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0208:  75% 184/246 [03:42<00:58,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0208:  75% 185/246 [03:42<00:57,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1134:  75% 185/246 [03:43<00:57,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1134:  76% 186/246 [03:43<00:56,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0582:  76% 186/246 [03:44<00:56,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0582:  76% 187/246 [03:44<00:55,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1562:  76% 187/246 [03:45<00:55,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1562:  76% 188/246 [03:45<00:54,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7805:  76% 188/246 [03:45<00:54,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7805:  77% 189/246 [03:46<00:53,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9714:  77% 189/246 [03:46<00:53,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9714:  77% 190/246 [03:47<00:52,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9771:  77% 190/246 [03:47<00:52,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9771:  78% 191/246 [03:48<00:51,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9399:  78% 191/246 [03:48<00:51,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9399:  78% 192/246 [03:49<00:50,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8246:  78% 192/246 [03:49<00:50,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8246:  78% 193/246 [03:50<00:49,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8514:  78% 193/246 [03:50<00:49,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8514:  79% 194/246 [03:51<00:48,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8996:  79% 194/246 [03:51<00:48,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8996:  79% 195/246 [03:52<00:47,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8308:  79% 195/246 [03:52<00:47,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8308:  80% 196/246 [03:53<00:46,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7427:  80% 196/246 [03:53<00:46,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7427:  80% 197/246 [03:53<00:45,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9051:  80% 197/246 [03:54<00:45,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9051:  80% 198/246 [03:54<00:45,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8206:  80% 198/246 [03:55<00:45,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8206:  81% 199/246 [03:55<00:44,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9247:  81% 199/246 [03:56<00:44,  1.07it/s]\u001b[A\n","\n","  0% 0/600 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","  0% 1/600 [00:02<29:04,  2.91s/it]\u001b[A\u001b[A\n","\n","  0% 2/600 [00:03<15:34,  1.56s/it]\n","\n","Epochs 0/2. Running Loss:    0.9247:  81% 200/246 [04:55<14:11, 18.50s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9577:  81% 200/246 [04:55<14:11, 18.50s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9577:  82% 201/246 [04:56<09:54, 13.22s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8621:  82% 201/246 [04:56<09:54, 13.22s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8621:  82% 202/246 [04:57<06:59,  9.53s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8711:  82% 202/246 [04:57<06:59,  9.53s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8711:  83% 203/246 [04:58<04:59,  6.95s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.7694:  83% 203/246 [04:58<04:59,  6.95s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.7694:  83% 204/246 [04:59<03:35,  5.14s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.7225:  83% 204/246 [04:59<03:35,  5.14s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.7225:  83% 205/246 [04:59<02:38,  3.87s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0593:  83% 205/246 [05:00<02:38,  3.87s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0593:  84% 206/246 [05:00<01:59,  2.98s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8183:  84% 206/246 [05:01<01:59,  2.98s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8183:  84% 207/246 [05:01<01:32,  2.36s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9592:  84% 207/246 [05:02<01:32,  2.36s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9592:  85% 208/246 [05:02<01:13,  1.93s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8227:  85% 208/246 [05:03<01:13,  1.93s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8227:  85% 209/246 [05:03<01:00,  1.62s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.7364:  85% 209/246 [05:03<01:00,  1.62s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.7364:  85% 210/246 [05:04<00:51,  1.42s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8806:  85% 210/246 [05:04<00:51,  1.42s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8806:  86% 211/246 [05:05<00:44,  1.27s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9929:  86% 211/246 [05:05<00:44,  1.27s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9929:  86% 212/246 [05:06<00:39,  1.17s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8050:  86% 212/246 [05:06<00:39,  1.17s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8050:  87% 213/246 [05:07<00:36,  1.09s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8621:  87% 213/246 [05:07<00:36,  1.09s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8621:  87% 214/246 [05:08<00:33,  1.04s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9556:  87% 214/246 [05:08<00:33,  1.04s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9556:  87% 215/246 [05:09<00:31,  1.01s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8715:  87% 215/246 [05:09<00:31,  1.01s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8715:  88% 216/246 [05:10<00:29,  1.02it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1674:  88% 216/246 [05:10<00:29,  1.02it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1674:  88% 217/246 [05:10<00:27,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7523:  88% 217/246 [05:11<00:27,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7523:  89% 218/246 [05:11<00:26,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.6659:  89% 218/246 [05:12<00:26,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.6659:  89% 219/246 [05:12<00:25,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8604:  89% 219/246 [05:13<00:25,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8604:  89% 220/246 [05:13<00:24,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8368:  89% 220/246 [05:14<00:24,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8368:  90% 221/246 [05:14<00:23,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7994:  90% 221/246 [05:15<00:23,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7994:  90% 222/246 [05:15<00:22,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.6895:  90% 222/246 [05:15<00:22,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.6895:  91% 223/246 [05:16<00:21,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9419:  91% 223/246 [05:16<00:21,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9419:  91% 224/246 [05:17<00:20,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9162:  91% 224/246 [05:17<00:20,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9162:  91% 225/246 [05:18<00:19,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8647:  91% 225/246 [05:18<00:19,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8647:  92% 226/246 [05:19<00:18,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9440:  92% 226/246 [05:19<00:18,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9440:  92% 227/246 [05:20<00:17,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9504:  92% 227/246 [05:20<00:17,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9504:  93% 228/246 [05:21<00:16,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0001:  93% 228/246 [05:21<00:16,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0001:  93% 229/246 [05:22<00:15,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7615:  93% 229/246 [05:22<00:15,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7615:  93% 230/246 [05:22<00:14,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7648:  93% 230/246 [05:23<00:14,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7648:  94% 231/246 [05:23<00:13,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8517:  94% 231/246 [05:24<00:13,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8517:  94% 232/246 [05:24<00:12,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0237:  94% 232/246 [05:25<00:12,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0237:  95% 233/246 [05:25<00:11,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7206:  95% 233/246 [05:26<00:11,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7206:  95% 234/246 [05:26<00:11,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0425:  95% 234/246 [05:27<00:11,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0425:  96% 235/246 [05:27<00:10,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9472:  96% 235/246 [05:27<00:10,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9472:  96% 236/246 [05:28<00:09,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8097:  96% 236/246 [05:28<00:09,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8097:  96% 237/246 [05:29<00:08,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9883:  96% 237/246 [05:29<00:08,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9883:  97% 238/246 [05:30<00:07,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8929:  97% 238/246 [05:30<00:07,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8929:  97% 239/246 [05:31<00:06,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8540:  97% 239/246 [05:31<00:06,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8540:  98% 240/246 [05:32<00:05,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8417:  98% 240/246 [05:32<00:05,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8417:  98% 241/246 [05:33<00:04,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1409:  98% 241/246 [05:33<00:04,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1409:  98% 242/246 [05:34<00:03,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9373:  98% 242/246 [05:34<00:03,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9373:  99% 243/246 [05:34<00:02,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8985:  99% 243/246 [05:35<00:02,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8985:  99% 244/246 [05:35<00:01,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7704:  99% 244/246 [05:36<00:01,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7704: 100% 245/246 [05:36<00:00,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8047: 100% 245/246 [05:37<00:00,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8047: 100% 246/246 [05:37<00:00,  1.37s/it]\n","\n","  0% 0/600 [00:00<?, ?it/s]\u001b[A\n","  0% 1/600 [00:02<20:51,  2.09s/it]\u001b[A\n","  0% 2/600 [00:02<11:00,  1.10s/it]\n","Epoch 2 of 2:  50% 1/2 [06:33<06:33, 393.83s/it]\n","Running Epoch 1 of 2:   0% 0/246 [00:00<?, ?it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9137:   0% 0/246 [00:00<?, ?it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9137:   0% 1/246 [00:00<03:23,  1.20it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8622:   0% 1/246 [00:01<03:23,  1.20it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8622:   1% 2/246 [00:01<03:32,  1.15it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0187:   1% 2/246 [00:02<03:32,  1.15it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0187:   1% 3/246 [00:02<03:35,  1.13it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8725:   1% 3/246 [00:03<03:35,  1.13it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8725:   2% 4/246 [00:03<03:36,  1.12it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8227:   2% 4/246 [00:03<03:36,  1.12it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8227:   2% 5/246 [00:04<03:37,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0190:   2% 5/246 [00:04<03:37,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0190:   2% 6/246 [00:05<03:36,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6020:   2% 6/246 [00:05<03:36,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6020:   3% 7/246 [00:06<03:34,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1529:   3% 7/246 [00:06<03:34,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1529:   3% 8/246 [00:07<03:35,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9353:   3% 8/246 [00:07<03:35,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9353:   4% 9/246 [00:08<03:34,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7998:   4% 9/246 [00:08<03:34,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7998:   4% 10/246 [00:08<03:34,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8600:   4% 10/246 [00:09<03:34,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8600:   4% 11/246 [00:09<03:33,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8496:   4% 11/246 [00:10<03:33,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8496:   5% 12/246 [00:10<03:33,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0921:   5% 12/246 [00:11<03:33,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0921:   5% 13/246 [00:11<03:32,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7476:   5% 13/246 [00:12<03:32,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7476:   6% 14/246 [00:12<03:31,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7395:   6% 14/246 [00:13<03:31,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7395:   6% 15/246 [00:13<03:30,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8288:   6% 15/246 [00:13<03:30,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8288:   7% 16/246 [00:14<03:29,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8110:   7% 16/246 [00:14<03:29,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8110:   7% 17/246 [00:15<03:28,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9471:   7% 17/246 [00:15<03:28,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9471:   7% 18/246 [00:16<03:27,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9920:   7% 18/246 [00:16<03:27,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9920:   8% 19/246 [00:17<03:26,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8032:   8% 19/246 [00:17<03:26,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8032:   8% 20/246 [00:18<03:27,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1866:   8% 20/246 [00:18<03:27,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1866:   9% 21/246 [00:19<03:24,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7672:   9% 21/246 [00:19<03:24,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7672:   9% 22/246 [00:19<03:23,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9942:   9% 22/246 [00:20<03:23,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9942:   9% 23/246 [00:20<03:22,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8597:   9% 23/246 [00:21<03:22,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8597:  10% 24/246 [00:21<03:21,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8134:  10% 24/246 [00:22<03:21,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8134:  10% 25/246 [00:22<03:20,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8539:  10% 25/246 [00:23<03:20,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8539:  11% 26/246 [00:23<03:19,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8373:  11% 26/246 [00:23<03:19,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8373:  11% 27/246 [00:24<03:20,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0974:  11% 27/246 [00:24<03:20,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0974:  11% 28/246 [00:25<03:19,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9025:  11% 28/246 [00:25<03:19,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9025:  12% 29/246 [00:26<03:18,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9070:  12% 29/246 [00:26<03:18,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9070:  12% 30/246 [00:27<03:10,  1.13it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0137:  12% 30/246 [00:27<03:10,  1.13it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0137:  13% 31/246 [00:28<03:12,  1.12it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9580:  13% 31/246 [00:28<03:12,  1.12it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9580:  13% 32/246 [00:28<03:14,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0936:  13% 32/246 [00:29<03:14,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0936:  13% 33/246 [00:29<03:13,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9132:  13% 33/246 [00:30<03:13,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9132:  14% 34/246 [00:30<03:12,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8605:  14% 34/246 [00:31<03:12,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8605:  14% 35/246 [00:31<03:11,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8469:  14% 35/246 [00:32<03:11,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8469:  15% 36/246 [00:32<03:10,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9565:  15% 36/246 [00:32<03:10,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9565:  15% 37/246 [00:33<03:09,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8661:  15% 37/246 [00:33<03:09,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8661:  15% 38/246 [00:34<03:08,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1275:  15% 38/246 [00:34<03:08,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1275:  16% 39/246 [00:35<03:07,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0643:  16% 39/246 [00:35<03:07,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0643:  16% 40/246 [00:36<03:07,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7890:  16% 40/246 [00:36<03:07,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7890:  17% 41/246 [00:37<03:06,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8391:  17% 41/246 [00:37<03:06,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8391:  17% 42/246 [00:38<03:05,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7927:  17% 42/246 [00:38<03:05,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7927:  17% 43/246 [00:38<03:06,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7795:  17% 43/246 [00:39<03:06,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7795:  18% 44/246 [00:39<03:04,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7129:  18% 44/246 [00:40<03:04,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7129:  18% 45/246 [00:40<03:04,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9249:  18% 45/246 [00:41<03:04,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9249:  19% 46/246 [00:41<03:03,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9841:  19% 46/246 [00:42<03:03,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9841:  19% 47/246 [00:42<03:03,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7698:  19% 47/246 [00:43<03:03,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7698:  20% 48/246 [00:43<03:01,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0776:  20% 48/246 [00:43<03:01,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0776:  20% 49/246 [00:44<03:00,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8286:  20% 49/246 [00:44<03:00,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8286:  20% 50/246 [00:45<02:59,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9591:  20% 50/246 [00:45<02:59,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9591:  21% 51/246 [00:46<02:58,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8848:  21% 51/246 [00:46<02:58,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8848:  21% 52/246 [00:47<02:57,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9383:  21% 52/246 [00:47<02:57,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9383:  22% 53/246 [00:48<02:56,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8764:  22% 53/246 [00:48<02:56,  1.09it/s]\u001b[A\n","\n","  0% 0/600 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","  0% 1/600 [00:01<18:21,  1.84s/it]\u001b[A\u001b[A\n","\n","  0% 2/600 [00:01<09:57,  1.00it/s]\n","\n","Epochs 1/2. Running Loss:    0.8764:  22% 54/246 [01:23<35:52, 11.21s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8289:  22% 54/246 [01:23<35:52, 11.21s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8289:  22% 55/246 [01:24<25:47,  8.10s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8748:  22% 55/246 [01:24<25:47,  8.10s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8748:  23% 56/246 [01:25<18:47,  5.93s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8219:  23% 56/246 [01:25<18:47,  5.93s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8219:  23% 57/246 [01:26<13:56,  4.43s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8790:  23% 57/246 [01:26<13:56,  4.43s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8790:  24% 58/246 [01:26<10:34,  3.38s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7449:  24% 58/246 [01:27<10:34,  3.38s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7449:  24% 59/246 [01:27<08:12,  2.63s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9223:  24% 59/246 [01:28<08:12,  2.63s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9223:  24% 60/246 [01:28<06:33,  2.12s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.0161:  24% 60/246 [01:29<06:33,  2.12s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.0161:  25% 61/246 [01:29<05:24,  1.75s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.0014:  25% 61/246 [01:30<05:24,  1.75s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.0014:  25% 62/246 [01:30<04:35,  1.50s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8231:  25% 62/246 [01:30<04:35,  1.50s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8231:  26% 63/246 [01:31<04:01,  1.32s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9171:  26% 63/246 [01:31<04:01,  1.32s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9171:  26% 64/246 [01:32<03:37,  1.20s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7390:  26% 64/246 [01:32<03:37,  1.20s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7390:  26% 65/246 [01:33<03:22,  1.12s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.6756:  26% 65/246 [01:33<03:22,  1.12s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.6756:  27% 66/246 [01:34<03:09,  1.05s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7623:  27% 66/246 [01:34<03:09,  1.05s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7623:  27% 67/246 [01:35<03:04,  1.03s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9512:  27% 67/246 [01:35<03:04,  1.03s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9512:  28% 68/246 [01:36<02:53,  1.02it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8673:  28% 68/246 [01:36<02:53,  1.02it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8673:  28% 69/246 [01:36<02:49,  1.04it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8189:  28% 69/246 [01:37<02:49,  1.04it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8189:  28% 70/246 [01:38<02:53,  1.01it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7984:  28% 70/246 [01:38<02:53,  1.01it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7984:  29% 71/246 [01:38<02:50,  1.03it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0015:  29% 71/246 [01:39<02:50,  1.03it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0015:  29% 72/246 [01:39<02:44,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0088:  29% 72/246 [01:40<02:44,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0088:  30% 73/246 [01:40<02:44,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7709:  30% 73/246 [01:41<02:44,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7709:  30% 74/246 [01:41<02:41,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7105:  30% 74/246 [01:42<02:41,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7105:  30% 75/246 [01:42<02:39,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8832:  30% 75/246 [01:43<02:39,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8832:  31% 76/246 [01:43<02:39,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1397:  31% 76/246 [01:43<02:39,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1397:  31% 77/246 [01:44<02:35,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8997:  31% 77/246 [01:44<02:35,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8997:  32% 78/246 [01:45<02:34,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9746:  32% 78/246 [01:45<02:34,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9746:  32% 79/246 [01:46<02:32,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7849:  32% 79/246 [01:46<02:32,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7849:  33% 80/246 [01:47<02:31,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8783:  33% 80/246 [01:47<02:31,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8783:  33% 81/246 [01:48<02:31,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9722:  33% 81/246 [01:48<02:31,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9722:  33% 82/246 [01:49<02:30,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9166:  33% 82/246 [01:49<02:30,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9166:  34% 83/246 [01:49<02:28,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0228:  34% 83/246 [01:50<02:28,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0228:  34% 84/246 [01:50<02:28,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7784:  34% 84/246 [01:51<02:28,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7784:  35% 85/246 [01:51<02:27,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9752:  35% 85/246 [01:52<02:27,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9752:  35% 86/246 [01:52<02:26,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8290:  35% 86/246 [01:53<02:26,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8290:  35% 87/246 [01:53<02:25,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8709:  35% 87/246 [01:54<02:25,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8709:  36% 88/246 [01:54<02:25,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8443:  36% 88/246 [01:54<02:25,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8443:  36% 89/246 [01:55<02:24,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9068:  36% 89/246 [01:55<02:24,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9068:  37% 90/246 [01:56<02:23,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8987:  37% 90/246 [01:56<02:23,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8987:  37% 91/246 [01:57<02:23,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8228:  37% 91/246 [01:57<02:23,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8228:  37% 92/246 [01:58<02:21,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8408:  37% 92/246 [01:58<02:21,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8408:  38% 93/246 [01:59<02:21,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7343:  38% 93/246 [01:59<02:21,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7343:  38% 94/246 [02:00<02:18,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9744:  38% 94/246 [02:00<02:18,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9744:  39% 95/246 [02:00<02:18,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8378:  39% 95/246 [02:01<02:18,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8378:  39% 96/246 [02:01<02:16,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9770:  39% 96/246 [02:02<02:16,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9770:  39% 97/246 [02:02<02:17,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6562:  39% 97/246 [02:03<02:17,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6562:  40% 98/246 [02:03<02:15,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7005:  40% 98/246 [02:04<02:15,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7005:  40% 99/246 [02:04<02:14,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9561:  40% 99/246 [02:05<02:14,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9561:  41% 100/246 [02:05<02:13,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7942:  41% 100/246 [02:05<02:13,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7942:  41% 101/246 [02:06<02:13,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8673:  41% 101/246 [02:06<02:13,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8673:  41% 102/246 [02:07<02:12,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0564:  41% 102/246 [02:07<02:12,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0564:  42% 103/246 [02:08<02:11,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7846:  42% 103/246 [02:08<02:11,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7846:  42% 104/246 [02:09<02:10,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6606:  42% 104/246 [02:09<02:10,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6606:  43% 105/246 [02:10<02:09,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8027:  43% 105/246 [02:10<02:09,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8027:  43% 106/246 [02:11<02:10,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8094:  43% 106/246 [02:11<02:10,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8094:  43% 107/246 [02:12<02:07,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8397:  43% 107/246 [02:12<02:07,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8397:  44% 108/246 [02:12<02:07,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8157:  44% 108/246 [02:13<02:07,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8157:  44% 109/246 [02:13<02:06,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8683:  44% 109/246 [02:14<02:06,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8683:  45% 110/246 [02:14<02:06,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7863:  45% 110/246 [02:15<02:06,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7863:  45% 111/246 [02:15<02:04,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8877:  45% 111/246 [02:16<02:04,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8877:  46% 112/246 [02:16<02:03,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8084:  46% 112/246 [02:17<02:03,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8084:  46% 113/246 [02:17<02:03,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8097:  46% 113/246 [02:17<02:03,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8097:  46% 114/246 [02:18<02:01,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7546:  46% 114/246 [02:18<02:01,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7546:  47% 115/246 [02:19<02:00,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6744:  47% 115/246 [02:19<02:00,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6744:  47% 116/246 [02:20<01:59,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7673:  47% 116/246 [02:20<01:59,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7673:  48% 117/246 [02:21<01:59,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0064:  48% 117/246 [02:21<01:59,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0064:  48% 118/246 [02:22<01:57,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8107:  48% 118/246 [02:22<01:57,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8107:  48% 119/246 [02:23<01:56,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.2916:  48% 119/246 [02:23<01:56,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.2916:  49% 120/246 [02:23<01:56,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8356:  49% 120/246 [02:24<01:56,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8356:  49% 121/246 [02:24<01:54,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9077:  49% 121/246 [02:25<01:54,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9077:  50% 122/246 [02:25<01:53,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8981:  50% 122/246 [02:26<01:53,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8981:  50% 123/246 [02:26<01:53,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7941:  50% 123/246 [02:27<01:53,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7941:  50% 124/246 [02:27<01:52,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0999:  50% 124/246 [02:28<01:52,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0999:  51% 125/246 [02:28<01:51,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7222:  51% 125/246 [02:28<01:51,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7222:  51% 126/246 [02:29<01:50,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8134:  51% 126/246 [02:29<01:50,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8134:  52% 127/246 [02:30<01:50,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8951:  52% 127/246 [02:30<01:50,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8951:  52% 128/246 [02:31<01:49,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7829:  52% 128/246 [02:31<01:49,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7829:  52% 129/246 [02:32<01:48,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8014:  52% 129/246 [02:32<01:48,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8014:  53% 130/246 [02:33<01:47,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7885:  53% 130/246 [02:33<01:47,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7885:  53% 131/246 [02:34<01:46,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6874:  53% 131/246 [02:34<01:46,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6874:  54% 132/246 [02:35<01:45,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9819:  54% 132/246 [02:35<01:45,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9819:  54% 133/246 [02:36<01:44,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9095:  54% 133/246 [02:36<01:44,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9095:  54% 134/246 [02:36<01:43,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6591:  54% 134/246 [02:37<01:43,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6591:  55% 135/246 [02:37<01:42,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8450:  55% 135/246 [02:38<01:42,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8450:  55% 136/246 [02:38<01:41,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.2565:  55% 136/246 [02:39<01:41,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.2565:  56% 137/246 [02:39<01:40,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9926:  56% 137/246 [02:40<01:40,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9926:  56% 138/246 [02:40<01:39,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7799:  56% 138/246 [02:41<01:39,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7799:  57% 139/246 [02:41<01:38,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9875:  57% 139/246 [02:41<01:38,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9875:  57% 140/246 [02:42<01:37,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.2840:  57% 140/246 [02:42<01:37,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.2840:  57% 141/246 [02:43<01:37,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8501:  57% 141/246 [02:43<01:37,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8501:  58% 142/246 [02:44<01:36,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7550:  58% 142/246 [02:44<01:36,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7550:  58% 143/246 [02:45<01:35,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7473:  58% 143/246 [02:45<01:35,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7473:  59% 144/246 [02:46<01:34,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8659:  59% 144/246 [02:46<01:34,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8659:  59% 145/246 [02:47<01:35,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8689:  59% 145/246 [02:47<01:35,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8689:  59% 146/246 [02:48<01:34,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8476:  59% 146/246 [02:48<01:34,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8476:  60% 147/246 [02:49<01:33,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9487:  60% 147/246 [02:49<01:33,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9487:  60% 148/246 [02:50<01:33,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6675:  60% 148/246 [02:50<01:33,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6675:  61% 149/246 [02:50<01:30,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7076:  61% 149/246 [02:51<01:30,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7076:  61% 150/246 [02:51<01:29,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1028:  61% 150/246 [02:52<01:29,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1028:  61% 151/246 [02:52<01:28,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7954:  61% 151/246 [02:53<01:28,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7954:  62% 152/246 [02:53<01:27,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7899:  62% 152/246 [02:54<01:27,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7899:  62% 153/246 [02:54<01:26,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0848:  62% 153/246 [02:55<01:26,  1.08it/s]\u001b[A\n","\n","  0% 0/600 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","  0% 2/600 [00:02<10:22,  1.04s/it]\n","\n","Epochs 1/2. Running Loss:    1.0848:  63% 154/246 [03:28<16:27, 10.73s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.3095:  63% 154/246 [03:28<16:27, 10.73s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.3095:  63% 155/246 [03:29<11:47,  7.77s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.5942:  63% 155/246 [03:29<11:47,  7.77s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.5942:  63% 156/246 [03:30<08:34,  5.71s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7111:  63% 156/246 [03:30<08:34,  5.71s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7111:  64% 157/246 [03:30<06:20,  4.27s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7101:  64% 157/246 [03:31<06:20,  4.27s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7101:  64% 158/246 [03:31<04:47,  3.27s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8203:  64% 158/246 [03:32<04:47,  3.27s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8203:  65% 159/246 [03:32<03:42,  2.55s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7639:  65% 159/246 [03:33<03:42,  2.55s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7639:  65% 160/246 [03:33<02:57,  2.06s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.0343:  65% 160/246 [03:34<02:57,  2.06s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.0343:  65% 161/246 [03:34<02:25,  1.71s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.0934:  65% 161/246 [03:34<02:25,  1.71s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.0934:  66% 162/246 [03:35<02:03,  1.47s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.1410:  66% 162/246 [03:35<02:03,  1.47s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.1410:  66% 163/246 [03:36<01:49,  1.32s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7772:  66% 163/246 [03:36<01:49,  1.32s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7772:  67% 164/246 [03:37<01:37,  1.19s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.6346:  67% 164/246 [03:37<01:37,  1.19s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.6346:  67% 165/246 [03:38<01:29,  1.11s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9187:  67% 165/246 [03:38<01:29,  1.11s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9187:  67% 166/246 [03:39<01:24,  1.05s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7442:  67% 166/246 [03:39<01:24,  1.05s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7442:  68% 167/246 [03:40<01:19,  1.01s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9622:  68% 167/246 [03:40<01:19,  1.01s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9622:  68% 168/246 [03:41<01:17,  1.00it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6106:  68% 168/246 [03:41<01:17,  1.00it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6106:  69% 169/246 [03:41<01:15,  1.02it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7089:  69% 169/246 [03:42<01:15,  1.02it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7089:  69% 170/246 [03:42<01:12,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9909:  69% 170/246 [03:43<01:12,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9909:  70% 171/246 [03:43<01:11,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8228:  70% 171/246 [03:44<01:11,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8228:  70% 172/246 [03:44<01:10,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9250:  70% 172/246 [03:45<01:10,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9250:  70% 173/246 [03:45<01:08,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9474:  70% 173/246 [03:46<01:08,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9474:  71% 174/246 [03:46<01:07,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9157:  71% 174/246 [03:46<01:07,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9157:  71% 175/246 [03:47<01:05,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9101:  71% 175/246 [03:47<01:05,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9101:  72% 176/246 [03:48<01:04,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7949:  72% 176/246 [03:48<01:04,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7949:  72% 177/246 [03:49<01:03,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9612:  72% 177/246 [03:49<01:03,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9612:  72% 178/246 [03:50<01:02,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8358:  72% 178/246 [03:50<01:02,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8358:  73% 179/246 [03:51<01:01,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5644:  73% 179/246 [03:51<01:01,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5644:  73% 180/246 [03:52<01:00,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9577:  73% 180/246 [03:52<01:00,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9577:  74% 181/246 [03:52<00:59,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7356:  74% 181/246 [03:53<00:59,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7356:  74% 182/246 [03:53<00:58,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9366:  74% 182/246 [03:54<00:58,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9366:  74% 183/246 [03:54<00:57,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0403:  74% 183/246 [03:55<00:57,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0403:  75% 184/246 [03:55<00:56,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7924:  75% 184/246 [03:56<00:56,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7924:  75% 185/246 [03:56<00:56,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8784:  75% 185/246 [03:57<00:56,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8784:  76% 186/246 [03:57<00:55,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8100:  76% 186/246 [03:57<00:55,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8100:  76% 187/246 [03:58<00:54,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6749:  76% 187/246 [03:58<00:54,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6749:  76% 188/246 [03:59<00:53,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8534:  76% 188/246 [03:59<00:53,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8534:  77% 189/246 [04:00<00:53,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7236:  77% 189/246 [04:00<00:53,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7236:  77% 190/246 [04:01<00:51,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.2139:  77% 190/246 [04:01<00:51,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.2139:  78% 191/246 [04:02<00:50,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8934:  78% 191/246 [04:02<00:50,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8934:  78% 192/246 [04:03<00:49,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5526:  78% 192/246 [04:03<00:49,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5526:  78% 193/246 [04:04<00:49,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6853:  78% 193/246 [04:04<00:49,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6853:  79% 194/246 [04:04<00:47,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8212:  79% 194/246 [04:05<00:47,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8212:  79% 195/246 [04:05<00:47,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8722:  79% 195/246 [04:06<00:47,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8722:  80% 196/246 [04:06<00:45,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9164:  80% 196/246 [04:07<00:45,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9164:  80% 197/246 [04:07<00:45,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8259:  80% 197/246 [04:08<00:45,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8259:  80% 198/246 [04:08<00:44,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9122:  80% 198/246 [04:09<00:44,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9122:  81% 199/246 [04:09<00:43,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5956:  81% 199/246 [04:09<00:43,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5956:  81% 200/246 [04:10<00:42,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9386:  81% 200/246 [04:10<00:42,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9386:  82% 201/246 [04:11<00:41,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9394:  82% 201/246 [04:11<00:41,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9394:  82% 202/246 [04:12<00:38,  1.14it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8411:  82% 202/246 [04:12<00:38,  1.14it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8411:  83% 203/246 [04:13<00:38,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6798:  83% 203/246 [04:13<00:38,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6798:  83% 204/246 [04:14<00:38,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9053:  83% 204/246 [04:14<00:38,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9053:  83% 205/246 [04:15<00:37,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9002:  83% 205/246 [04:15<00:37,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9002:  84% 206/246 [04:15<00:36,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6310:  84% 206/246 [04:16<00:36,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6310:  84% 207/246 [04:16<00:36,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7738:  84% 207/246 [04:17<00:36,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7738:  85% 208/246 [04:17<00:34,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9148:  85% 208/246 [04:18<00:34,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9148:  85% 209/246 [04:18<00:34,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7755:  85% 209/246 [04:19<00:34,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7755:  85% 210/246 [04:19<00:33,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7583:  85% 210/246 [04:20<00:33,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7583:  86% 211/246 [04:20<00:32,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8321:  86% 211/246 [04:21<00:32,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8321:  86% 212/246 [04:21<00:31,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7452:  86% 212/246 [04:21<00:31,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7452:  87% 213/246 [04:22<00:30,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9241:  87% 213/246 [04:22<00:30,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9241:  87% 214/246 [04:23<00:29,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7866:  87% 214/246 [04:23<00:29,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7866:  87% 215/246 [04:24<00:28,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8862:  87% 215/246 [04:24<00:28,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8862:  88% 216/246 [04:25<00:27,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1117:  88% 216/246 [04:25<00:27,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1117:  88% 217/246 [04:26<00:26,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9288:  88% 217/246 [04:26<00:26,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9288:  89% 218/246 [04:27<00:25,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8672:  89% 218/246 [04:27<00:25,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8672:  89% 219/246 [04:28<00:25,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8858:  89% 219/246 [04:28<00:25,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8858:  89% 220/246 [04:28<00:24,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8052:  89% 220/246 [04:29<00:24,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8052:  90% 221/246 [04:29<00:23,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8779:  90% 221/246 [04:30<00:23,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8779:  90% 222/246 [04:30<00:22,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6874:  90% 222/246 [04:31<00:22,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6874:  91% 223/246 [04:31<00:21,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6596:  91% 223/246 [04:32<00:21,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6596:  91% 224/246 [04:32<00:20,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6369:  91% 224/246 [04:33<00:20,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6369:  91% 225/246 [04:33<00:19,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9889:  91% 225/246 [04:33<00:19,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9889:  92% 226/246 [04:34<00:18,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6216:  92% 226/246 [04:34<00:18,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6216:  92% 227/246 [04:35<00:17,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8375:  92% 227/246 [04:35<00:17,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8375:  93% 228/246 [04:36<00:16,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7748:  93% 228/246 [04:36<00:16,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7748:  93% 229/246 [04:37<00:15,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6968:  93% 229/246 [04:37<00:15,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6968:  93% 230/246 [04:38<00:15,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8581:  93% 230/246 [04:38<00:15,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8581:  94% 231/246 [04:39<00:14,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5537:  94% 231/246 [04:39<00:14,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5537:  94% 232/246 [04:40<00:13,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8377:  94% 232/246 [04:40<00:13,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8377:  95% 233/246 [04:41<00:12,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9880:  95% 233/246 [04:41<00:12,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9880:  95% 234/246 [04:41<00:11,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8889:  95% 234/246 [04:42<00:11,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8889:  96% 235/246 [04:42<00:10,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8036:  96% 235/246 [04:43<00:10,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8036:  96% 236/246 [04:43<00:09,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7590:  96% 236/246 [04:44<00:09,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7590:  96% 237/246 [04:44<00:08,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6276:  96% 237/246 [04:45<00:08,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6276:  97% 238/246 [04:45<00:07,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9493:  97% 238/246 [04:46<00:07,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9493:  97% 239/246 [04:46<00:06,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8612:  97% 239/246 [04:47<00:06,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8612:  98% 240/246 [04:47<00:05,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8347:  98% 240/246 [04:47<00:05,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8347:  98% 241/246 [04:48<00:04,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0258:  98% 241/246 [04:48<00:04,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0258:  98% 242/246 [04:49<00:03,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7711:  98% 242/246 [04:49<00:03,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7711:  99% 243/246 [04:50<00:02,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0624:  99% 243/246 [04:50<00:02,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0624:  99% 244/246 [04:51<00:01,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5956:  99% 244/246 [04:51<00:01,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5956: 100% 245/246 [04:52<00:00,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7289: 100% 245/246 [04:52<00:00,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7289: 100% 246/246 [04:52<00:00,  1.19s/it]\n","\n","  0% 0/600 [00:00<?, ?it/s]\u001b[A\n","  0% 1/600 [00:02<24:53,  2.49s/it]\u001b[A\n","  0% 2/600 [00:02<13:42,  1.38s/it]\n","Epoch 2 of 2: 100% 2/2 [12:52<00:00, 386.04s/it]\n","Fine-tuning\troberta\troberta-large\tv2\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  0% 8/3927 [00:07<1:04:44,  1.01it/s]\n","Epoch 1 of 2:   0% 0/2 [00:00<?, ?it/s]\n","Running Epoch 0 of 2:   0% 0/246 [00:00<?, ?it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2437:   0% 0/246 [00:00<?, ?it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2437:   0% 1/246 [00:00<04:02,  1.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2343:   0% 1/246 [00:01<04:02,  1.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2343:   1% 2/246 [00:01<03:44,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2947:   1% 2/246 [00:02<03:44,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2947:   1% 3/246 [00:02<03:36,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2839:   1% 3/246 [00:03<03:36,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2839:   2% 4/246 [00:03<03:34,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2404:   2% 4/246 [00:03<03:34,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2404:   2% 5/246 [00:04<03:34,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2208:   2% 5/246 [00:04<03:34,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2208:   2% 6/246 [00:05<03:33,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2515:   2% 6/246 [00:05<03:33,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2515:   3% 7/246 [00:06<03:24,  1.17it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.3841:   3% 7/246 [00:06<03:24,  1.17it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.3841:   3% 8/246 [00:07<03:27,  1.15it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2011:   3% 8/246 [00:07<03:27,  1.15it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2011:   4% 9/246 [00:07<03:30,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2615:   4% 9/246 [00:08<03:30,  1.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2615:   4% 10/246 [00:08<03:30,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1919:   4% 10/246 [00:09<03:30,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1919:   4% 11/246 [00:09<03:30,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2625:   4% 11/246 [00:10<03:30,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2625:   5% 12/246 [00:10<03:29,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2292:   5% 12/246 [00:11<03:29,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2292:   5% 13/246 [00:11<03:29,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1660:   5% 13/246 [00:12<03:29,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1660:   6% 14/246 [00:12<03:29,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1502:   6% 14/246 [00:12<03:29,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1502:   6% 15/246 [00:13<03:29,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1948:   6% 15/246 [00:13<03:29,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1948:   7% 16/246 [00:14<03:28,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0868:   7% 16/246 [00:14<03:28,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0868:   7% 17/246 [00:15<03:27,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9793:   7% 17/246 [00:15<03:27,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9793:   7% 18/246 [00:16<03:31,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2576:   7% 18/246 [00:16<03:31,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2576:   8% 19/246 [00:17<03:28,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0403:   8% 19/246 [00:17<03:28,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0403:   8% 20/246 [00:18<03:27,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0245:   8% 20/246 [00:18<03:27,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0245:   9% 21/246 [00:18<03:27,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2811:   9% 21/246 [00:19<03:27,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2811:   9% 22/246 [00:19<03:26,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9941:   9% 22/246 [00:20<03:26,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9941:   9% 23/246 [00:20<03:26,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1623:   9% 23/246 [00:21<03:26,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1623:  10% 24/246 [00:21<03:25,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2115:  10% 24/246 [00:22<03:25,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2115:  10% 25/246 [00:22<03:26,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8708:  10% 25/246 [00:23<03:26,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8708:  11% 26/246 [00:23<03:22,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1608:  11% 26/246 [00:23<03:22,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1608:  11% 27/246 [00:24<03:22,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2471:  11% 27/246 [00:24<03:22,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2471:  11% 28/246 [00:25<03:21,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8606:  11% 28/246 [00:25<03:21,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8606:  12% 29/246 [00:26<03:20,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9712:  12% 29/246 [00:26<03:20,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9712:  12% 30/246 [00:27<03:12,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0641:  12% 30/246 [00:27<03:12,  1.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0641:  13% 31/246 [00:28<03:16,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.4514:  13% 31/246 [00:28<03:16,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.4514:  13% 32/246 [00:29<03:17,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1527:  13% 32/246 [00:29<03:17,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1527:  13% 33/246 [00:30<03:17,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1461:  13% 33/246 [00:30<03:17,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1461:  14% 34/246 [00:30<03:16,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9937:  14% 34/246 [00:31<03:16,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9937:  14% 35/246 [00:31<03:15,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0337:  14% 35/246 [00:32<03:15,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0337:  15% 36/246 [00:32<03:15,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1596:  15% 36/246 [00:33<03:15,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1596:  15% 37/246 [00:33<03:14,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9860:  15% 37/246 [00:34<03:14,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9860:  15% 38/246 [00:34<03:14,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1526:  15% 38/246 [00:35<03:14,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1526:  16% 39/246 [00:35<03:14,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9478:  16% 39/246 [00:36<03:14,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9478:  16% 40/246 [00:36<03:13,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0528:  16% 40/246 [00:36<03:13,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0528:  17% 41/246 [00:37<03:14,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2151:  17% 41/246 [00:37<03:14,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2151:  17% 42/246 [00:38<03:13,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0827:  17% 42/246 [00:38<03:13,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0827:  17% 43/246 [00:39<03:11,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9664:  17% 43/246 [00:39<03:11,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9664:  18% 44/246 [00:40<03:10,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9667:  18% 44/246 [00:40<03:10,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9667:  18% 45/246 [00:41<03:08,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9387:  18% 45/246 [00:41<03:08,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9387:  19% 46/246 [00:42<03:08,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1309:  19% 46/246 [00:42<03:08,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1309:  19% 47/246 [00:43<03:08,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0461:  19% 47/246 [00:43<03:08,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0461:  20% 48/246 [00:44<03:07,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0499:  20% 48/246 [00:44<03:07,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0499:  20% 49/246 [00:45<03:06,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0762:  20% 49/246 [00:45<03:06,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0762:  20% 50/246 [00:46<03:05,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0252:  20% 50/246 [00:46<03:05,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0252:  21% 51/246 [00:47<03:04,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1147:  21% 51/246 [00:47<03:04,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1147:  21% 52/246 [00:47<03:03,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0599:  21% 52/246 [00:48<03:03,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0599:  22% 53/246 [00:48<03:02,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1883:  22% 53/246 [00:49<03:02,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1883:  22% 54/246 [00:49<03:01,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0130:  22% 54/246 [00:50<03:01,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0130:  22% 55/246 [00:50<03:02,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0805:  22% 55/246 [00:51<03:02,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0805:  23% 56/246 [00:51<03:01,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1318:  23% 56/246 [00:52<03:01,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1318:  23% 57/246 [00:52<03:01,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0633:  23% 57/246 [00:53<03:01,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0633:  24% 58/246 [00:53<02:59,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9034:  24% 58/246 [00:54<02:59,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9034:  24% 59/246 [00:54<02:59,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0941:  24% 59/246 [00:55<02:59,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0941:  24% 60/246 [00:55<02:58,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1487:  24% 60/246 [00:56<02:58,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1487:  25% 61/246 [00:56<02:58,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8956:  25% 61/246 [00:56<02:58,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8956:  25% 62/246 [00:57<02:56,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9111:  25% 62/246 [00:57<02:56,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9111:  26% 63/246 [00:58<02:54,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0562:  26% 63/246 [00:58<02:54,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0562:  26% 64/246 [00:59<02:53,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0303:  26% 64/246 [00:59<02:53,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0303:  26% 65/246 [01:00<02:51,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0717:  26% 65/246 [01:00<02:51,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0717:  27% 66/246 [01:01<02:43,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9988:  27% 66/246 [01:01<02:43,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9988:  27% 67/246 [01:02<02:45,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7888:  27% 67/246 [01:02<02:45,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7888:  28% 68/246 [01:03<02:45,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9235:  28% 68/246 [01:03<02:45,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9235:  28% 69/246 [01:04<02:46,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0544:  28% 69/246 [01:04<02:46,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0544:  28% 70/246 [01:04<02:44,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0007:  28% 70/246 [01:05<02:44,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0007:  29% 71/246 [01:05<02:44,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9567:  29% 71/246 [01:06<02:44,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9567:  29% 72/246 [01:06<02:44,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0560:  29% 72/246 [01:07<02:44,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0560:  30% 73/246 [01:07<02:43,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1750:  30% 73/246 [01:08<02:43,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1750:  30% 74/246 [01:08<02:43,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8692:  30% 74/246 [01:09<02:43,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8692:  30% 75/246 [01:09<02:42,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0175:  30% 75/246 [01:10<02:42,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0175:  31% 76/246 [01:10<02:41,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0302:  31% 76/246 [01:11<02:41,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0302:  31% 77/246 [01:11<02:40,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8869:  31% 77/246 [01:12<02:40,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8869:  32% 78/246 [01:12<02:39,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8374:  32% 78/246 [01:13<02:39,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8374:  32% 79/246 [01:13<02:38,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9958:  32% 79/246 [01:13<02:38,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9958:  33% 80/246 [01:14<02:37,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7573:  33% 80/246 [01:14<02:37,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7573:  33% 81/246 [01:15<02:35,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0224:  33% 81/246 [01:15<02:35,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0224:  33% 82/246 [01:16<02:34,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9498:  33% 82/246 [01:16<02:34,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9498:  34% 83/246 [01:17<02:32,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0115:  34% 83/246 [01:17<02:32,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0115:  34% 84/246 [01:18<02:31,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1092:  34% 84/246 [01:18<02:31,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1092:  35% 85/246 [01:19<02:29,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0289:  35% 85/246 [01:19<02:29,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0289:  35% 86/246 [01:20<02:28,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9681:  35% 86/246 [01:20<02:28,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9681:  35% 87/246 [01:20<02:27,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8941:  35% 87/246 [01:21<02:27,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8941:  36% 88/246 [01:21<02:26,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0907:  36% 88/246 [01:22<02:26,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0907:  36% 89/246 [01:22<02:26,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7985:  36% 89/246 [01:23<02:26,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7985:  37% 90/246 [01:23<02:25,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0255:  37% 90/246 [01:24<02:25,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0255:  37% 91/246 [01:24<02:25,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9152:  37% 91/246 [01:25<02:25,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9152:  37% 92/246 [01:25<02:23,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0345:  37% 92/246 [01:26<02:23,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0345:  38% 93/246 [01:26<02:23,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0619:  38% 93/246 [01:27<02:23,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0619:  38% 94/246 [01:27<02:20,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9312:  38% 94/246 [01:27<02:20,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9312:  39% 95/246 [01:28<02:20,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0126:  39% 95/246 [01:28<02:20,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0126:  39% 96/246 [01:29<02:19,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7410:  39% 96/246 [01:29<02:19,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7410:  39% 97/246 [01:30<02:18,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7956:  39% 97/246 [01:30<02:18,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7956:  40% 98/246 [01:31<02:16,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7333:  40% 98/246 [01:31<02:16,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7333:  40% 99/246 [01:32<02:16,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0897:  40% 99/246 [01:32<02:16,  1.08it/s]\u001b[A\n","\n","  0% 0/600 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","  0% 1/600 [00:01<18:32,  1.86s/it]\u001b[A\u001b[A\n","\n","  0% 2/600 [00:02<10:13,  1.03s/it]\n","\n","Epochs 0/2. Running Loss:    1.0897:  41% 100/246 [02:26<41:23, 17.01s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8315:  41% 100/246 [02:27<41:23, 17.01s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8315:  41% 101/246 [02:27<29:42, 12.29s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.2763:  41% 101/246 [02:28<29:42, 12.29s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.2763:  41% 102/246 [02:28<21:19,  8.89s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9953:  41% 102/246 [02:29<21:19,  8.89s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9953:  42% 103/246 [02:29<15:30,  6.50s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9785:  42% 103/246 [02:30<15:30,  6.50s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9785:  42% 104/246 [02:30<11:25,  4.83s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0976:  42% 104/246 [02:31<11:25,  4.83s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0976:  43% 105/246 [02:31<08:37,  3.67s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9927:  43% 105/246 [02:32<08:37,  3.67s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9927:  43% 106/246 [02:32<06:38,  2.84s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9671:  43% 106/246 [02:33<06:38,  2.84s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9671:  43% 107/246 [02:33<05:15,  2.27s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9920:  43% 107/246 [02:34<05:15,  2.27s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9920:  44% 108/246 [02:34<04:17,  1.87s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0520:  44% 108/246 [02:34<04:17,  1.87s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0520:  44% 109/246 [02:35<03:37,  1.59s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0819:  44% 109/246 [02:35<03:37,  1.59s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0819:  45% 110/246 [02:36<03:08,  1.39s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8601:  45% 110/246 [02:36<03:08,  1.39s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8601:  45% 111/246 [02:37<02:50,  1.26s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8319:  45% 111/246 [02:37<02:50,  1.26s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8319:  46% 112/246 [02:38<02:35,  1.16s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9720:  46% 112/246 [02:38<02:35,  1.16s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9720:  46% 113/246 [02:39<02:25,  1.09s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.1333:  46% 113/246 [02:39<02:25,  1.09s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.1333:  46% 114/246 [02:40<02:17,  1.04s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.7995:  46% 114/246 [02:40<02:17,  1.04s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.7995:  47% 115/246 [02:41<02:12,  1.01s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0536:  47% 115/246 [02:41<02:12,  1.01s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0536:  47% 116/246 [02:42<02:08,  1.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0529:  47% 116/246 [02:42<02:08,  1.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0529:  48% 117/246 [02:42<02:04,  1.03it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9297:  48% 117/246 [02:43<02:04,  1.03it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9297:  48% 118/246 [02:43<02:02,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9599:  48% 118/246 [02:44<02:02,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9599:  48% 119/246 [02:44<02:00,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8833:  48% 119/246 [02:45<02:00,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8833:  49% 120/246 [02:45<01:58,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0374:  49% 120/246 [02:46<01:58,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0374:  49% 121/246 [02:46<01:56,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8746:  49% 121/246 [02:47<01:56,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8746:  50% 122/246 [02:47<01:55,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9683:  50% 122/246 [02:47<01:55,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9683:  50% 123/246 [02:48<01:54,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8975:  50% 123/246 [02:48<01:54,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8975:  50% 124/246 [02:49<01:53,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0211:  50% 124/246 [02:49<01:53,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0211:  51% 125/246 [02:50<01:52,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9015:  51% 125/246 [02:50<01:52,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9015:  51% 126/246 [02:51<01:52,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0096:  51% 126/246 [02:51<01:52,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0096:  52% 127/246 [02:52<01:50,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9383:  52% 127/246 [02:52<01:50,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9383:  52% 128/246 [02:53<01:49,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1056:  52% 128/246 [02:53<01:49,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1056:  52% 129/246 [02:54<01:49,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8357:  52% 129/246 [02:54<01:49,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8357:  53% 130/246 [02:55<01:49,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0957:  53% 130/246 [02:55<01:49,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0957:  53% 131/246 [02:55<01:47,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0058:  53% 131/246 [02:56<01:47,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0058:  54% 132/246 [02:56<01:46,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9430:  54% 132/246 [02:57<01:46,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9430:  54% 133/246 [02:57<01:47,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9464:  54% 133/246 [02:58<01:47,  1.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9464:  54% 134/246 [02:58<01:45,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0092:  54% 134/246 [02:59<01:45,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0092:  55% 135/246 [02:59<01:42,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9393:  55% 135/246 [03:00<01:42,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9393:  55% 136/246 [03:00<01:41,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0417:  55% 136/246 [03:01<01:41,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0417:  56% 137/246 [03:01<01:40,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8837:  56% 137/246 [03:01<01:40,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8837:  56% 138/246 [03:02<01:39,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8012:  56% 138/246 [03:02<01:39,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8012:  57% 139/246 [03:03<01:38,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8668:  57% 139/246 [03:03<01:38,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8668:  57% 140/246 [03:04<01:37,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8755:  57% 140/246 [03:04<01:37,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8755:  57% 141/246 [03:05<01:36,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1631:  57% 141/246 [03:05<01:36,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1631:  58% 142/246 [03:06<01:35,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8756:  58% 142/246 [03:06<01:35,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8756:  58% 143/246 [03:07<01:34,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8154:  58% 143/246 [03:07<01:34,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8154:  59% 144/246 [03:07<01:34,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7759:  59% 144/246 [03:08<01:34,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7759:  59% 145/246 [03:08<01:32,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0457:  59% 145/246 [03:09<01:32,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0457:  59% 146/246 [03:09<01:32,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8739:  59% 146/246 [03:10<01:32,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8739:  60% 147/246 [03:10<01:31,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8627:  60% 147/246 [03:11<01:31,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8627:  60% 148/246 [03:11<01:30,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1773:  60% 148/246 [03:12<01:30,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1773:  61% 149/246 [03:12<01:29,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7687:  61% 149/246 [03:13<01:29,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7687:  61% 150/246 [03:13<01:29,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0019:  61% 150/246 [03:13<01:29,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0019:  61% 151/246 [03:14<01:27,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8436:  61% 151/246 [03:14<01:27,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8436:  62% 152/246 [03:15<01:26,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9397:  62% 152/246 [03:15<01:26,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9397:  62% 153/246 [03:16<01:26,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8023:  62% 153/246 [03:16<01:26,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8023:  63% 154/246 [03:17<01:24,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8842:  63% 154/246 [03:17<01:24,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8842:  63% 155/246 [03:18<01:24,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0566:  63% 155/246 [03:18<01:24,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0566:  63% 156/246 [03:19<01:23,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1662:  63% 156/246 [03:19<01:23,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1662:  64% 157/246 [03:20<01:22,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8872:  64% 157/246 [03:20<01:22,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8872:  64% 158/246 [03:20<01:21,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9868:  64% 158/246 [03:21<01:21,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9868:  65% 159/246 [03:21<01:20,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7916:  65% 159/246 [03:22<01:20,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7916:  65% 160/246 [03:22<01:19,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9720:  65% 160/246 [03:23<01:19,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9720:  65% 161/246 [03:23<01:18,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7741:  65% 161/246 [03:24<01:18,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7741:  66% 162/246 [03:24<01:17,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8331:  66% 162/246 [03:25<01:17,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8331:  66% 163/246 [03:25<01:16,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8222:  66% 163/246 [03:25<01:16,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8222:  67% 164/246 [03:26<01:15,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0268:  67% 164/246 [03:26<01:15,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0268:  67% 165/246 [03:27<01:15,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9093:  67% 165/246 [03:27<01:15,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9093:  67% 166/246 [03:28<01:14,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8638:  67% 166/246 [03:28<01:14,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8638:  68% 167/246 [03:29<01:14,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0875:  68% 167/246 [03:29<01:14,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0875:  68% 168/246 [03:30<01:13,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8509:  68% 168/246 [03:30<01:13,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8509:  69% 169/246 [03:31<01:11,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7475:  69% 169/246 [03:31<01:11,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7475:  69% 170/246 [03:32<01:10,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0141:  69% 170/246 [03:32<01:10,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0141:  70% 171/246 [03:33<01:09,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7811:  70% 171/246 [03:33<01:09,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7811:  70% 172/246 [03:33<01:08,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8411:  70% 172/246 [03:34<01:08,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8411:  70% 173/246 [03:34<01:07,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0055:  70% 173/246 [03:35<01:07,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0055:  71% 174/246 [03:35<01:06,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0979:  71% 174/246 [03:36<01:06,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0979:  71% 175/246 [03:36<01:05,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8925:  71% 175/246 [03:37<01:05,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8925:  72% 176/246 [03:37<01:05,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9155:  72% 176/246 [03:38<01:05,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9155:  72% 177/246 [03:38<01:04,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9535:  72% 177/246 [03:38<01:04,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9535:  72% 178/246 [03:39<01:02,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9498:  72% 178/246 [03:39<01:02,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9498:  73% 179/246 [03:40<01:02,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8776:  73% 179/246 [03:40<01:02,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8776:  73% 180/246 [03:41<01:01,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0905:  73% 180/246 [03:41<01:01,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0905:  74% 181/246 [03:42<01:00,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9209:  74% 181/246 [03:42<01:00,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9209:  74% 182/246 [03:43<01:00,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8788:  74% 182/246 [03:43<01:00,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8788:  74% 183/246 [03:44<00:58,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9409:  74% 183/246 [03:44<00:58,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9409:  75% 184/246 [03:45<00:57,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7987:  75% 184/246 [03:45<00:57,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7987:  75% 185/246 [03:46<00:56,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7243:  75% 185/246 [03:46<00:56,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7243:  76% 186/246 [03:46<00:56,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0495:  76% 186/246 [03:47<00:56,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0495:  76% 187/246 [03:47<00:55,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7941:  76% 187/246 [03:48<00:55,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7941:  76% 188/246 [03:48<00:54,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8208:  76% 188/246 [03:49<00:54,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8208:  77% 189/246 [03:49<00:53,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0841:  77% 189/246 [03:50<00:53,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0841:  77% 190/246 [03:50<00:52,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9862:  77% 190/246 [03:51<00:52,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9862:  78% 191/246 [03:51<00:51,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9361:  78% 191/246 [03:52<00:51,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9361:  78% 192/246 [03:52<00:50,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1714:  78% 192/246 [03:53<00:50,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1714:  78% 193/246 [03:53<00:49,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8568:  78% 193/246 [03:53<00:49,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8568:  79% 194/246 [03:54<00:48,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9858:  79% 194/246 [03:54<00:48,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9858:  79% 195/246 [03:55<00:47,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7256:  79% 195/246 [03:55<00:47,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7256:  80% 196/246 [03:56<00:46,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9753:  80% 196/246 [03:56<00:46,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9753:  80% 197/246 [03:57<00:45,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9963:  80% 197/246 [03:57<00:45,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9963:  80% 198/246 [03:58<00:44,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8629:  80% 198/246 [03:58<00:44,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8629:  81% 199/246 [03:59<00:44,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8966:  81% 199/246 [03:59<00:44,  1.07it/s]\u001b[A\n","\n","  0% 0/600 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","  0% 2/600 [00:02<12:52,  1.29s/it]\n","\n","Epochs 0/2. Running Loss:    0.8966:  81% 200/246 [04:58<14:08, 18.45s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8682:  81% 200/246 [04:58<14:08, 18.45s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8682:  82% 201/246 [04:59<09:52, 13.18s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8658:  82% 201/246 [04:59<09:52, 13.18s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8658:  82% 202/246 [05:00<06:57,  9.49s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.7365:  82% 202/246 [05:00<06:57,  9.49s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.7365:  83% 203/246 [05:01<04:57,  6.92s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8313:  83% 203/246 [05:01<04:57,  6.92s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8313:  83% 204/246 [05:02<03:34,  5.11s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8625:  83% 204/246 [05:02<03:34,  5.11s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8625:  83% 205/246 [05:02<02:37,  3.85s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8983:  83% 205/246 [05:03<02:37,  3.85s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8983:  84% 206/246 [05:03<01:58,  2.97s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0021:  84% 206/246 [05:04<01:58,  2.97s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0021:  84% 207/246 [05:04<01:31,  2.34s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8233:  84% 207/246 [05:05<01:31,  2.34s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8233:  85% 208/246 [05:05<01:12,  1.92s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.1232:  85% 208/246 [05:06<01:12,  1.92s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.1232:  85% 209/246 [05:06<00:59,  1.61s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9522:  85% 209/246 [05:06<00:59,  1.61s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9522:  85% 210/246 [05:07<00:50,  1.40s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0142:  85% 210/246 [05:07<00:50,  1.40s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0142:  86% 211/246 [05:08<00:43,  1.25s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0019:  86% 211/246 [05:08<00:43,  1.25s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0019:  86% 212/246 [05:09<00:39,  1.15s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0350:  86% 212/246 [05:09<00:39,  1.15s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0350:  87% 213/246 [05:10<00:35,  1.07s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.2398:  87% 213/246 [05:10<00:35,  1.07s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.2398:  87% 214/246 [05:11<00:32,  1.02s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0093:  87% 214/246 [05:11<00:32,  1.02s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0093:  87% 215/246 [05:11<00:30,  1.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8761:  87% 215/246 [05:12<00:30,  1.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8761:  88% 216/246 [05:12<00:28,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8645:  88% 216/246 [05:13<00:28,  1.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8645:  88% 217/246 [05:13<00:27,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7471:  88% 217/246 [05:14<00:27,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7471:  89% 218/246 [05:14<00:26,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0577:  89% 218/246 [05:15<00:26,  1.07it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0577:  89% 219/246 [05:15<00:24,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9918:  89% 219/246 [05:16<00:24,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9918:  89% 220/246 [05:16<00:24,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9604:  89% 220/246 [05:16<00:24,  1.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9604:  90% 221/246 [05:17<00:22,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7934:  90% 221/246 [05:17<00:22,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7934:  90% 222/246 [05:18<00:21,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9821:  90% 222/246 [05:18<00:21,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9821:  91% 223/246 [05:19<00:21,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8730:  91% 223/246 [05:19<00:21,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8730:  91% 224/246 [05:20<00:20,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1311:  91% 224/246 [05:20<00:20,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1311:  91% 225/246 [05:21<00:19,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9162:  91% 225/246 [05:21<00:19,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9162:  92% 226/246 [05:21<00:18,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9963:  92% 226/246 [05:22<00:18,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9963:  92% 227/246 [05:22<00:17,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9340:  92% 227/246 [05:23<00:17,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9340:  93% 228/246 [05:23<00:16,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8840:  93% 228/246 [05:24<00:16,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8840:  93% 229/246 [05:24<00:15,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8460:  93% 229/246 [05:25<00:15,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8460:  93% 230/246 [05:25<00:14,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0186:  93% 230/246 [05:26<00:14,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0186:  94% 231/246 [05:26<00:13,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7674:  94% 231/246 [05:26<00:13,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7674:  94% 232/246 [05:27<00:12,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9075:  94% 232/246 [05:27<00:12,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9075:  95% 233/246 [05:28<00:11,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8103:  95% 233/246 [05:28<00:11,  1.11it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8103:  95% 234/246 [05:29<00:10,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9375:  95% 234/246 [05:29<00:10,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9375:  96% 235/246 [05:30<00:10,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8518:  96% 235/246 [05:30<00:10,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8518:  96% 236/246 [05:31<00:09,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8568:  96% 236/246 [05:31<00:09,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8568:  96% 237/246 [05:31<00:08,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0040:  96% 237/246 [05:32<00:08,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0040:  97% 238/246 [05:32<00:07,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2347:  97% 238/246 [05:33<00:07,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2347:  97% 239/246 [05:33<00:06,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9257:  97% 239/246 [05:34<00:06,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9257:  98% 240/246 [05:34<00:05,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9598:  98% 240/246 [05:35<00:05,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9598:  98% 241/246 [05:35<00:04,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8889:  98% 241/246 [05:36<00:04,  1.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8889:  98% 242/246 [05:36<00:03,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1769:  98% 242/246 [05:36<00:03,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1769:  99% 243/246 [05:37<00:02,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0277:  99% 243/246 [05:37<00:02,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0277:  99% 244/246 [05:38<00:01,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9082:  99% 244/246 [05:38<00:01,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9082: 100% 245/246 [05:39<00:00,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8743: 100% 245/246 [05:39<00:00,  1.09it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8743: 100% 246/246 [05:39<00:00,  1.38s/it]\n","\n","  0% 0/600 [00:00<?, ?it/s]\u001b[A\n","  0% 1/600 [00:02<20:06,  2.01s/it]\u001b[A\n","  0% 2/600 [00:02<10:58,  1.10s/it]\n","Epoch 2 of 2:  50% 1/2 [06:35<06:35, 395.08s/it]\n","Running Epoch 1 of 2:   0% 0/246 [00:00<?, ?it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7698:   0% 0/246 [00:00<?, ?it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7698:   0% 1/246 [00:00<03:20,  1.22it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8082:   0% 1/246 [00:01<03:20,  1.22it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8082:   1% 2/246 [00:01<03:33,  1.14it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0883:   1% 2/246 [00:02<03:33,  1.14it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0883:   1% 3/246 [00:02<03:35,  1.13it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1069:   1% 3/246 [00:03<03:35,  1.13it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1069:   2% 4/246 [00:03<03:35,  1.12it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0627:   2% 4/246 [00:03<03:35,  1.12it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0627:   2% 5/246 [00:04<03:36,  1.12it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7868:   2% 5/246 [00:04<03:36,  1.12it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7868:   2% 6/246 [00:05<03:34,  1.12it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0837:   2% 6/246 [00:05<03:34,  1.12it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0837:   3% 7/246 [00:06<03:35,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.2708:   3% 7/246 [00:06<03:35,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.2708:   3% 8/246 [00:07<03:33,  1.12it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9075:   3% 8/246 [00:07<03:33,  1.12it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9075:   4% 9/246 [00:08<03:32,  1.12it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7846:   4% 9/246 [00:08<03:32,  1.12it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7846:   4% 10/246 [00:08<03:31,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0750:   4% 10/246 [00:09<03:31,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0750:   4% 11/246 [00:09<03:31,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1580:   4% 11/246 [00:10<03:31,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1580:   5% 12/246 [00:10<03:30,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6607:   5% 12/246 [00:11<03:30,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6607:   5% 13/246 [00:11<03:29,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7933:   5% 13/246 [00:12<03:29,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7933:   6% 14/246 [00:12<03:29,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9496:   6% 14/246 [00:12<03:29,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9496:   6% 15/246 [00:13<03:29,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7332:   6% 15/246 [00:13<03:29,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7332:   7% 16/246 [00:14<03:28,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9614:   7% 16/246 [00:14<03:28,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9614:   7% 17/246 [00:15<03:26,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0326:   7% 17/246 [00:15<03:26,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0326:   7% 18/246 [00:16<03:26,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0127:   7% 18/246 [00:16<03:26,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0127:   8% 19/246 [00:17<03:25,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9725:   8% 19/246 [00:17<03:25,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9725:   8% 20/246 [00:17<03:24,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9974:   8% 20/246 [00:18<03:24,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9974:   9% 21/246 [00:18<03:25,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8017:   9% 21/246 [00:19<03:25,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8017:   9% 22/246 [00:19<03:22,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0429:   9% 22/246 [00:20<03:22,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0429:   9% 23/246 [00:20<03:21,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0986:   9% 23/246 [00:21<03:21,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0986:  10% 24/246 [00:21<03:20,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9357:  10% 24/246 [00:21<03:20,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9357:  10% 25/246 [00:22<03:19,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9052:  10% 25/246 [00:22<03:19,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9052:  11% 26/246 [00:23<03:19,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8902:  11% 26/246 [00:23<03:19,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8902:  11% 27/246 [00:24<03:19,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8973:  11% 27/246 [00:24<03:19,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8973:  11% 28/246 [00:25<03:18,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9550:  11% 28/246 [00:25<03:18,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9550:  12% 29/246 [00:26<03:17,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7300:  12% 29/246 [00:26<03:17,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7300:  12% 30/246 [00:27<03:15,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8192:  12% 30/246 [00:27<03:15,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8192:  13% 31/246 [00:27<03:16,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8075:  13% 31/246 [00:28<03:16,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8075:  13% 32/246 [00:28<03:16,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7371:  13% 32/246 [00:29<03:16,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7371:  13% 33/246 [00:29<03:14,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0077:  13% 33/246 [00:30<03:14,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0077:  14% 34/246 [00:30<03:12,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9323:  14% 34/246 [00:31<03:12,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9323:  14% 35/246 [00:31<03:12,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8325:  14% 35/246 [00:32<03:12,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8325:  15% 36/246 [00:32<03:11,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0922:  15% 36/246 [00:32<03:11,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0922:  15% 37/246 [00:33<03:11,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8952:  15% 37/246 [00:33<03:11,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8952:  15% 38/246 [00:34<03:10,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6935:  15% 38/246 [00:34<03:10,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6935:  16% 39/246 [00:35<03:08,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8644:  16% 39/246 [00:35<03:08,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8644:  16% 40/246 [00:36<03:07,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7523:  16% 40/246 [00:36<03:07,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7523:  17% 41/246 [00:37<03:06,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.2388:  17% 41/246 [00:37<03:06,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.2388:  17% 42/246 [00:38<03:06,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6912:  17% 42/246 [00:38<03:06,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6912:  17% 43/246 [00:38<03:04,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6774:  17% 43/246 [00:39<03:04,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6774:  18% 44/246 [00:39<03:03,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9984:  18% 44/246 [00:40<03:03,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9984:  18% 45/246 [00:40<03:02,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1779:  18% 45/246 [00:41<03:02,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1779:  19% 46/246 [00:41<03:02,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8601:  19% 46/246 [00:42<03:02,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8601:  19% 47/246 [00:42<03:01,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1147:  19% 47/246 [00:42<03:01,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1147:  20% 48/246 [00:43<03:00,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0308:  20% 48/246 [00:43<03:00,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0308:  20% 49/246 [00:44<02:58,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0715:  20% 49/246 [00:44<02:58,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0715:  20% 50/246 [00:45<02:59,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9412:  20% 50/246 [00:45<02:59,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9412:  21% 51/246 [00:46<02:59,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8240:  21% 51/246 [00:46<02:59,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8240:  21% 52/246 [00:47<02:59,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7805:  21% 52/246 [00:47<02:59,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7805:  22% 53/246 [00:47<02:51,  1.12it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0760:  22% 53/246 [00:48<02:51,  1.12it/s]\u001b[A\n","\n","  0% 0/600 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","  0% 1/600 [00:03<31:43,  3.18s/it]\u001b[A\u001b[A\n","\n","  0% 2/600 [00:03<17:19,  1.74s/it]\n","\n","Epochs 1/2. Running Loss:    1.0760:  22% 54/246 [01:23<36:10, 11.31s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8118:  22% 54/246 [01:23<36:10, 11.31s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8118:  22% 55/246 [01:24<26:00,  8.17s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9172:  22% 55/246 [01:24<26:00,  8.17s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9172:  23% 56/246 [01:25<19:00,  6.00s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9783:  23% 56/246 [01:25<19:00,  6.00s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9783:  23% 57/246 [01:26<14:05,  4.47s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9814:  23% 57/246 [01:26<14:05,  4.47s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9814:  24% 58/246 [01:27<10:41,  3.41s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8768:  24% 58/246 [01:27<10:41,  3.41s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8768:  24% 59/246 [01:28<08:18,  2.67s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.1331:  24% 59/246 [01:28<08:18,  2.67s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.1331:  24% 60/246 [01:29<06:38,  2.14s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7789:  24% 60/246 [01:29<06:38,  2.14s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7789:  25% 61/246 [01:29<05:26,  1.77s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.0063:  25% 61/246 [01:30<05:26,  1.77s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.0063:  25% 62/246 [01:30<04:38,  1.52s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8055:  25% 62/246 [01:31<04:38,  1.52s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8055:  26% 63/246 [01:31<04:03,  1.33s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7642:  26% 63/246 [01:32<04:03,  1.33s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7642:  26% 64/246 [01:32<03:38,  1.20s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9917:  26% 64/246 [01:33<03:38,  1.20s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9917:  26% 65/246 [01:33<03:22,  1.12s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7692:  26% 65/246 [01:33<03:22,  1.12s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7692:  27% 66/246 [01:34<03:09,  1.05s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9221:  27% 66/246 [01:34<03:09,  1.05s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9221:  27% 67/246 [01:35<03:01,  1.02s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8710:  27% 67/246 [01:35<03:01,  1.02s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8710:  28% 68/246 [01:36<02:55,  1.02it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1087:  28% 68/246 [01:36<02:55,  1.02it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1087:  28% 69/246 [01:37<02:50,  1.04it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8695:  28% 69/246 [01:37<02:50,  1.04it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8695:  28% 70/246 [01:38<02:46,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8494:  28% 70/246 [01:38<02:46,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8494:  29% 71/246 [01:39<02:43,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8495:  29% 71/246 [01:39<02:43,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8495:  29% 72/246 [01:39<02:41,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0850:  29% 72/246 [01:40<02:41,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0850:  30% 73/246 [01:41<02:45,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1253:  30% 73/246 [01:41<02:45,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1253:  30% 74/246 [01:41<02:42,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6586:  30% 74/246 [01:42<02:42,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6586:  30% 75/246 [01:42<02:39,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0769:  30% 75/246 [01:43<02:39,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0769:  31% 76/246 [01:43<02:40,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8319:  31% 76/246 [01:44<02:40,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8319:  31% 77/246 [01:44<02:37,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8859:  31% 77/246 [01:45<02:37,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8859:  32% 78/246 [01:45<02:38,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8560:  32% 78/246 [01:46<02:38,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8560:  32% 79/246 [01:46<02:40,  1.04it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6738:  32% 79/246 [01:47<02:40,  1.04it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6738:  33% 80/246 [01:47<02:35,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0126:  33% 80/246 [01:47<02:35,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0126:  33% 81/246 [01:48<02:36,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9866:  33% 81/246 [01:48<02:36,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9866:  33% 82/246 [01:49<02:32,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0244:  33% 82/246 [01:49<02:32,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0244:  34% 83/246 [01:50<02:30,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9574:  34% 83/246 [01:50<02:30,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9574:  34% 84/246 [01:51<02:29,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7684:  34% 84/246 [01:51<02:29,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7684:  35% 85/246 [01:52<02:28,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0053:  35% 85/246 [01:52<02:28,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0053:  35% 86/246 [01:53<02:26,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7350:  35% 86/246 [01:53<02:26,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7350:  35% 87/246 [01:53<02:26,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9814:  35% 87/246 [01:54<02:26,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9814:  36% 88/246 [01:54<02:24,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0187:  36% 88/246 [01:55<02:24,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0187:  36% 89/246 [01:55<02:24,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0305:  36% 89/246 [01:56<02:24,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0305:  37% 90/246 [01:56<02:22,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9006:  37% 90/246 [01:57<02:22,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9006:  37% 91/246 [01:57<02:22,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8850:  37% 91/246 [01:58<02:22,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8850:  37% 92/246 [01:58<02:20,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8244:  37% 92/246 [01:58<02:20,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8244:  38% 93/246 [01:59<02:20,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6985:  38% 93/246 [01:59<02:20,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6985:  38% 94/246 [02:00<02:19,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9217:  38% 94/246 [02:00<02:19,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9217:  39% 95/246 [02:01<02:19,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9635:  39% 95/246 [02:01<02:19,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9635:  39% 96/246 [02:02<02:18,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6935:  39% 96/246 [02:02<02:18,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6935:  39% 97/246 [02:03<02:17,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0439:  39% 97/246 [02:03<02:17,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0439:  40% 98/246 [02:04<02:16,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8673:  40% 98/246 [02:04<02:16,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8673:  40% 99/246 [02:05<02:18,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8083:  40% 99/246 [02:05<02:18,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8083:  41% 100/246 [02:05<02:14,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8440:  41% 100/246 [02:06<02:14,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8440:  41% 101/246 [02:06<02:13,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0122:  41% 101/246 [02:07<02:13,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0122:  41% 102/246 [02:07<02:13,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1754:  41% 102/246 [02:08<02:13,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1754:  42% 103/246 [02:08<02:11,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9841:  42% 103/246 [02:09<02:11,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9841:  42% 104/246 [02:09<02:10,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1021:  42% 104/246 [02:10<02:10,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1021:  43% 105/246 [02:10<02:10,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7447:  43% 105/246 [02:10<02:10,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7447:  43% 106/246 [02:11<02:08,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7602:  43% 106/246 [02:11<02:08,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7602:  43% 107/246 [02:12<02:07,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7752:  43% 107/246 [02:12<02:07,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7752:  44% 108/246 [02:13<02:06,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8922:  44% 108/246 [02:13<02:06,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8922:  44% 109/246 [02:14<02:05,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6735:  44% 109/246 [02:14<02:05,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6735:  45% 110/246 [02:15<02:05,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9121:  45% 110/246 [02:15<02:05,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9121:  45% 111/246 [02:16<02:04,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8878:  45% 111/246 [02:16<02:04,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8878:  46% 112/246 [02:17<02:03,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1137:  46% 112/246 [02:17<02:03,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1137:  46% 113/246 [02:17<02:02,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7761:  46% 113/246 [02:18<02:02,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7761:  46% 114/246 [02:18<02:02,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7800:  46% 114/246 [02:19<02:02,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7800:  47% 115/246 [02:19<02:00,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9268:  47% 115/246 [02:20<02:00,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9268:  47% 116/246 [02:20<02:00,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7929:  47% 116/246 [02:21<02:00,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7929:  48% 117/246 [02:21<01:59,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9285:  48% 117/246 [02:22<01:59,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9285:  48% 118/246 [02:22<01:58,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7491:  48% 118/246 [02:22<01:58,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7491:  48% 119/246 [02:23<01:57,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8776:  48% 119/246 [02:23<01:57,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8776:  49% 120/246 [02:24<01:56,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7489:  49% 120/246 [02:24<01:56,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7489:  49% 121/246 [02:25<01:56,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6179:  49% 121/246 [02:25<01:56,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6179:  50% 122/246 [02:26<01:53,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7188:  50% 122/246 [02:26<01:53,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7188:  50% 123/246 [02:27<01:52,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7816:  50% 123/246 [02:27<01:52,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7816:  50% 124/246 [02:28<01:52,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8079:  50% 124/246 [02:28<01:52,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8079:  51% 125/246 [02:29<01:51,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8212:  51% 125/246 [02:29<01:51,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8212:  51% 126/246 [02:29<01:50,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9222:  51% 126/246 [02:30<01:50,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9222:  52% 127/246 [02:30<01:49,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7943:  52% 127/246 [02:31<01:49,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7943:  52% 128/246 [02:31<01:49,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9664:  52% 128/246 [02:32<01:49,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9664:  52% 129/246 [02:32<01:47,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8837:  52% 129/246 [02:33<01:47,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8837:  53% 130/246 [02:33<01:46,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9111:  53% 130/246 [02:34<01:46,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9111:  53% 131/246 [02:34<01:46,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8060:  53% 131/246 [02:34<01:46,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8060:  54% 132/246 [02:35<01:45,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.2489:  54% 132/246 [02:35<01:45,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.2489:  54% 133/246 [02:36<01:44,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6683:  54% 133/246 [02:36<01:44,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6683:  54% 134/246 [02:37<01:43,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8510:  54% 134/246 [02:37<01:43,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8510:  55% 135/246 [02:38<01:42,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7303:  55% 135/246 [02:38<01:42,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7303:  55% 136/246 [02:39<01:42,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8097:  55% 136/246 [02:39<01:42,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8097:  56% 137/246 [02:40<01:40,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9631:  56% 137/246 [02:40<01:40,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9631:  56% 138/246 [02:40<01:39,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7625:  56% 138/246 [02:41<01:39,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7625:  57% 139/246 [02:41<01:38,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6969:  57% 139/246 [02:42<01:38,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6969:  57% 140/246 [02:42<01:37,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7023:  57% 140/246 [02:43<01:37,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7023:  57% 141/246 [02:43<01:36,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6030:  57% 141/246 [02:44<01:36,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6030:  58% 142/246 [02:44<01:35,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6976:  58% 142/246 [02:45<01:35,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6976:  58% 143/246 [02:45<01:34,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1361:  58% 143/246 [02:45<01:34,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1361:  59% 144/246 [02:46<01:33,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7721:  59% 144/246 [02:46<01:33,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7721:  59% 145/246 [02:47<01:32,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7784:  59% 145/246 [02:47<01:32,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7784:  59% 146/246 [02:48<01:31,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8660:  59% 146/246 [02:48<01:31,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8660:  60% 147/246 [02:49<01:30,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8300:  60% 147/246 [02:49<01:30,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8300:  60% 148/246 [02:50<01:29,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8276:  60% 148/246 [02:50<01:29,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8276:  61% 149/246 [02:51<01:29,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8813:  61% 149/246 [02:51<01:29,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8813:  61% 150/246 [02:51<01:28,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0910:  61% 150/246 [02:52<01:28,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0910:  61% 151/246 [02:52<01:27,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8320:  61% 151/246 [02:53<01:27,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8320:  62% 152/246 [02:53<01:26,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7198:  62% 152/246 [02:54<01:26,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7198:  62% 153/246 [02:54<01:25,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8736:  62% 153/246 [02:55<01:25,  1.08it/s]\u001b[A\n","\n","  0% 0/600 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","  0% 1/600 [00:02<25:14,  2.53s/it]\u001b[A\u001b[A\n","\n","  0% 2/600 [00:02<13:55,  1.40s/it]\n","\n","Epochs 1/2. Running Loss:    0.8736:  63% 154/246 [04:07<34:13, 22.32s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.1343:  63% 154/246 [04:07<34:13, 22.32s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.1343:  63% 155/246 [04:07<24:04, 15.88s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.2121:  63% 155/246 [04:08<24:04, 15.88s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.2121:  63% 156/246 [04:08<17:04, 11.38s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.6438:  63% 156/246 [04:09<17:04, 11.38s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.6438:  64% 157/246 [04:09<12:13,  8.24s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7046:  64% 157/246 [04:10<12:13,  8.24s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7046:  64% 158/246 [04:10<08:51,  6.04s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.6507:  64% 158/246 [04:10<08:51,  6.04s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.6507:  65% 159/246 [04:11<06:31,  4.50s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7185:  65% 159/246 [04:11<06:31,  4.50s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7185:  65% 160/246 [04:12<04:54,  3.42s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.0461:  65% 160/246 [04:12<04:54,  3.42s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.0461:  65% 161/246 [04:13<03:46,  2.66s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9870:  65% 161/246 [04:13<03:46,  2.66s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9870:  66% 162/246 [04:14<02:59,  2.13s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7682:  66% 162/246 [04:14<02:59,  2.13s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7682:  66% 163/246 [04:15<02:26,  1.76s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8526:  66% 163/246 [04:15<02:26,  1.76s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8526:  67% 164/246 [04:15<02:03,  1.50s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9342:  67% 164/246 [04:16<02:03,  1.50s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9342:  67% 165/246 [04:16<01:46,  1.32s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.0139:  67% 165/246 [04:17<01:46,  1.32s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.0139:  67% 166/246 [04:17<01:35,  1.19s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8134:  67% 166/246 [04:18<01:35,  1.19s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8134:  68% 167/246 [04:18<01:27,  1.11s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7309:  68% 167/246 [04:19<01:27,  1.11s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7309:  68% 168/246 [04:19<01:21,  1.04s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8368:  68% 168/246 [04:19<01:21,  1.04s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8368:  69% 169/246 [04:20<01:17,  1.00s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7686:  69% 169/246 [04:20<01:17,  1.00s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7686:  69% 170/246 [04:21<01:14,  1.03it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9418:  69% 170/246 [04:21<01:14,  1.03it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9418:  70% 171/246 [04:22<01:11,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8683:  70% 171/246 [04:22<01:11,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8683:  70% 172/246 [04:23<01:09,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6981:  70% 172/246 [04:23<01:09,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6981:  70% 173/246 [04:24<01:07,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1850:  70% 173/246 [04:24<01:07,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1850:  71% 174/246 [04:25<01:07,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7405:  71% 174/246 [04:25<01:07,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7405:  71% 175/246 [04:25<01:05,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7781:  71% 175/246 [04:26<01:05,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7781:  72% 176/246 [04:26<01:04,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8367:  72% 176/246 [04:27<01:04,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8367:  72% 177/246 [04:27<01:03,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6544:  72% 177/246 [04:28<01:03,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6544:  72% 178/246 [04:28<01:02,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9013:  72% 178/246 [04:29<01:02,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9013:  73% 179/246 [04:29<01:01,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0025:  73% 179/246 [04:29<01:01,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0025:  73% 180/246 [04:30<01:00,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.2524:  73% 180/246 [04:30<01:00,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.2524:  74% 181/246 [04:31<00:59,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8282:  74% 181/246 [04:31<00:59,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8282:  74% 182/246 [04:32<00:55,  1.15it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7710:  74% 182/246 [04:32<00:55,  1.15it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7710:  74% 183/246 [04:33<00:55,  1.13it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7031:  74% 183/246 [04:33<00:55,  1.13it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7031:  75% 184/246 [04:34<00:55,  1.12it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9019:  75% 184/246 [04:34<00:55,  1.12it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9019:  75% 185/246 [04:34<00:54,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8707:  75% 185/246 [04:35<00:54,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8707:  76% 186/246 [04:35<00:54,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6803:  76% 186/246 [04:36<00:54,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6803:  76% 187/246 [04:36<00:53,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8325:  76% 187/246 [04:37<00:53,  1.11it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8325:  76% 188/246 [04:37<00:52,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9749:  76% 188/246 [04:38<00:52,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9749:  77% 189/246 [04:38<00:51,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9264:  77% 189/246 [04:38<00:51,  1.10it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9264:  77% 190/246 [04:39<00:51,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8369:  77% 190/246 [04:39<00:51,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8369:  78% 191/246 [04:40<00:50,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8373:  78% 191/246 [04:40<00:50,  1.09it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8373:  78% 192/246 [04:41<00:49,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9864:  78% 192/246 [04:41<00:49,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9864:  78% 193/246 [04:42<00:48,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9239:  78% 193/246 [04:42<00:48,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9239:  79% 194/246 [04:43<00:47,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7878:  79% 194/246 [04:43<00:47,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7878:  79% 195/246 [04:44<00:47,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8386:  79% 195/246 [04:44<00:47,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8386:  80% 196/246 [04:45<00:46,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1214:  80% 196/246 [04:45<00:46,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1214:  80% 197/246 [04:45<00:45,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8870:  80% 197/246 [04:46<00:45,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8870:  80% 198/246 [04:46<00:44,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0458:  80% 198/246 [04:47<00:44,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0458:  81% 199/246 [04:47<00:43,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8654:  81% 199/246 [04:48<00:43,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8654:  81% 200/246 [04:48<00:42,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9000:  81% 200/246 [04:49<00:42,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9000:  82% 201/246 [04:49<00:41,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8507:  82% 201/246 [04:50<00:41,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8507:  82% 202/246 [04:50<00:40,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7592:  82% 202/246 [04:51<00:40,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7592:  83% 203/246 [04:51<00:39,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8736:  83% 203/246 [04:51<00:39,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8736:  83% 204/246 [04:52<00:39,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7438:  83% 204/246 [04:52<00:39,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7438:  83% 205/246 [04:53<00:38,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1072:  83% 205/246 [04:53<00:38,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1072:  84% 206/246 [04:54<00:37,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7666:  84% 206/246 [04:54<00:37,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7666:  84% 207/246 [04:55<00:36,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9784:  84% 207/246 [04:55<00:36,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9784:  85% 208/246 [04:56<00:35,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8450:  85% 208/246 [04:56<00:35,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8450:  85% 209/246 [04:57<00:34,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9128:  85% 209/246 [04:57<00:34,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9128:  85% 210/246 [04:58<00:33,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9945:  85% 210/246 [04:58<00:33,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9945:  86% 211/246 [04:59<00:32,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7955:  86% 211/246 [04:59<00:32,  1.07it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7955:  86% 212/246 [05:00<00:32,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8983:  86% 212/246 [05:00<00:32,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8983:  87% 213/246 [05:00<00:31,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7358:  87% 213/246 [05:01<00:31,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7358:  87% 214/246 [05:01<00:30,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8593:  87% 214/246 [05:02<00:30,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8593:  87% 215/246 [05:02<00:29,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9368:  87% 215/246 [05:03<00:29,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9368:  88% 216/246 [05:03<00:28,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8903:  88% 216/246 [05:04<00:28,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8903:  88% 217/246 [05:04<00:27,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0660:  88% 217/246 [05:05<00:27,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0660:  89% 218/246 [05:05<00:26,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1829:  89% 218/246 [05:06<00:26,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1829:  89% 219/246 [05:06<00:25,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5892:  89% 219/246 [05:07<00:25,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5892:  89% 220/246 [05:07<00:24,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9240:  89% 220/246 [05:08<00:24,  1.06it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9240:  90% 221/246 [05:08<00:23,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9542:  90% 221/246 [05:08<00:23,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9542:  90% 222/246 [05:09<00:22,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8261:  90% 222/246 [05:09<00:22,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8261:  91% 223/246 [05:10<00:21,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7648:  91% 223/246 [05:10<00:21,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7648:  91% 224/246 [05:11<00:20,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6665:  91% 224/246 [05:11<00:20,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6665:  91% 225/246 [05:12<00:20,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9041:  91% 225/246 [05:12<00:20,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9041:  92% 226/246 [05:13<00:18,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1355:  92% 226/246 [05:13<00:18,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1355:  92% 227/246 [05:14<00:18,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.3383:  92% 227/246 [05:14<00:18,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.3383:  93% 228/246 [05:15<00:17,  1.04it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9117:  93% 228/246 [05:15<00:17,  1.04it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9117:  93% 229/246 [05:16<00:16,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6823:  93% 229/246 [05:16<00:16,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6823:  93% 230/246 [05:17<00:15,  1.04it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7754:  93% 230/246 [05:17<00:15,  1.04it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7754:  94% 231/246 [05:18<00:14,  1.04it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9378:  94% 231/246 [05:18<00:14,  1.04it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9378:  94% 232/246 [05:19<00:13,  1.04it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9680:  94% 232/246 [05:19<00:13,  1.04it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9680:  95% 233/246 [05:20<00:12,  1.03it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9252:  95% 233/246 [05:20<00:12,  1.03it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9252:  95% 234/246 [05:21<00:11,  1.04it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8839:  95% 234/246 [05:21<00:11,  1.04it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8839:  96% 235/246 [05:21<00:10,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7373:  96% 235/246 [05:22<00:10,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7373:  96% 236/246 [05:22<00:09,  1.04it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0176:  96% 236/246 [05:23<00:09,  1.04it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0176:  96% 237/246 [05:23<00:08,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7633:  96% 237/246 [05:24<00:08,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7633:  97% 238/246 [05:24<00:07,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7768:  97% 238/246 [05:25<00:07,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7768:  97% 239/246 [05:25<00:06,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9638:  97% 239/246 [05:26<00:06,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9638:  98% 240/246 [05:26<00:05,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9904:  98% 240/246 [05:27<00:05,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9904:  98% 241/246 [05:27<00:04,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7503:  98% 241/246 [05:28<00:04,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7503:  98% 242/246 [05:28<00:03,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9801:  98% 242/246 [05:29<00:03,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9801:  99% 243/246 [05:29<00:02,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9563:  99% 243/246 [05:29<00:02,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9563:  99% 244/246 [05:30<00:01,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7331:  99% 244/246 [05:30<00:01,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7331: 100% 245/246 [05:31<00:00,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8381: 100% 245/246 [05:31<00:00,  1.05it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8381: 100% 246/246 [05:32<00:00,  1.35s/it]\n","\n","  0% 0/600 [00:00<?, ?it/s]\u001b[A\n","  0% 1/600 [00:03<39:48,  3.99s/it]\u001b[A\n","  0% 2/600 [00:04<22:16,  2.24s/it]\n","Epoch 2 of 2: 100% 2/2 [12:44<00:00, 382.09s/it]\n","Fine-tuning\tbert\tbert-base-multilingual-cased\tv1\n","Downloading pytorch_model.bin: 100% 714M/714M [00:11<00:00, 62.8MB/s]\n","Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  0% 8/3927 [00:06<55:06,  1.19it/s]\n","Epoch 1 of 2:   0% 0/2 [00:00<?, ?it/s]\n","Running Epoch 0 of 2:   0% 0/246 [00:00<?, ?it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0851:   0% 0/246 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n","\n","Epochs 0/2. Running Loss:    1.0851:   0% 1/246 [00:00<01:43,  2.36it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1492:   0% 1/246 [00:00<01:43,  2.36it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1492:   1% 2/246 [00:01<02:21,  1.72it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1340:   1% 2/246 [00:01<02:21,  1.72it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1340:   1% 3/246 [00:01<01:49,  2.21it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1683:   1% 3/246 [00:01<01:49,  2.21it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1683:   2% 4/246 [00:01<01:39,  2.43it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1180:   2% 4/246 [00:01<01:39,  2.43it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1180:   2% 5/246 [00:02<01:33,  2.57it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1109:   2% 5/246 [00:02<01:33,  2.57it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1109:   2% 6/246 [00:02<01:30,  2.67it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1209:   2% 6/246 [00:02<01:30,  2.67it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1209:   3% 7/246 [00:02<01:28,  2.69it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1233:   3% 7/246 [00:02<01:28,  2.69it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1233:   3% 8/246 [00:03<01:28,  2.69it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1319:   3% 8/246 [00:03<01:28,  2.69it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1319:   4% 9/246 [00:03<01:26,  2.73it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0818:   4% 9/246 [00:03<01:26,  2.73it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0818:   4% 10/246 [00:03<01:26,  2.73it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1123:   4% 10/246 [00:04<01:26,  2.73it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1123:   4% 11/246 [00:04<01:27,  2.68it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0665:   4% 11/246 [00:04<01:27,  2.68it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0665:   5% 12/246 [00:04<01:25,  2.74it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0629:   5% 12/246 [00:04<01:25,  2.74it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0629:   5% 13/246 [00:05<01:25,  2.71it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0129:   5% 13/246 [00:05<01:25,  2.71it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0129:   6% 14/246 [00:05<01:22,  2.81it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1613:   6% 14/246 [00:05<01:22,  2.81it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1613:   6% 15/246 [00:05<01:17,  2.99it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0873:   6% 15/246 [00:05<01:17,  2.99it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0873:   7% 16/246 [00:05<01:13,  3.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0665:   7% 16/246 [00:06<01:13,  3.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0665:   7% 17/246 [00:06<01:16,  3.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0528:   7% 17/246 [00:06<01:16,  3.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0528:   7% 18/246 [00:06<01:16,  2.99it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0999:   7% 18/246 [00:06<01:16,  2.99it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0999:   8% 19/246 [00:06<01:16,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0835:   8% 19/246 [00:07<01:16,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0835:   8% 20/246 [00:07<01:16,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0286:   8% 20/246 [00:07<01:16,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0286:   9% 21/246 [00:07<01:16,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0385:   9% 21/246 [00:07<01:16,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0385:   9% 22/246 [00:08<01:16,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0151:   9% 22/246 [00:08<01:16,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0151:   9% 23/246 [00:08<01:16,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1104:   9% 23/246 [00:08<01:16,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1104:  10% 24/246 [00:08<01:16,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9362:  10% 24/246 [00:08<01:16,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9362:  10% 25/246 [00:09<01:16,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9634:  10% 25/246 [00:09<01:16,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9634:  11% 26/246 [00:09<01:15,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8929:  11% 26/246 [00:09<01:15,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8929:  11% 27/246 [00:09<01:15,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9268:  11% 27/246 [00:09<01:15,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9268:  11% 28/246 [00:10<01:14,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9431:  11% 28/246 [00:10<01:14,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9431:  12% 29/246 [00:10<01:14,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9570:  12% 29/246 [00:10<01:14,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9570:  12% 30/246 [00:10<01:14,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0322:  12% 30/246 [00:10<01:14,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0322:  13% 31/246 [00:11<01:13,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9933:  13% 31/246 [00:11<01:13,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9933:  13% 32/246 [00:11<01:13,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9148:  13% 32/246 [00:11<01:13,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9148:  13% 33/246 [00:11<01:13,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1004:  13% 33/246 [00:11<01:13,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1004:  14% 34/246 [00:12<01:12,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9585:  14% 34/246 [00:12<01:12,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9585:  14% 35/246 [00:12<01:12,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0114:  14% 35/246 [00:12<01:12,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0114:  15% 36/246 [00:12<01:12,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9492:  15% 36/246 [00:12<01:12,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9492:  15% 37/246 [00:13<01:11,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8710:  15% 37/246 [00:13<01:11,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8710:  15% 38/246 [00:13<01:12,  2.89it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8976:  15% 38/246 [00:13<01:12,  2.89it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8976:  16% 39/246 [00:13<01:11,  2.89it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8720:  16% 39/246 [00:14<01:11,  2.89it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8720:  16% 40/246 [00:14<01:11,  2.89it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9297:  16% 40/246 [00:14<01:11,  2.89it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9297:  17% 41/246 [00:14<01:11,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9063:  17% 41/246 [00:14<01:11,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9063:  17% 42/246 [00:14<01:10,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8905:  17% 42/246 [00:15<01:10,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8905:  17% 43/246 [00:15<01:10,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9211:  17% 43/246 [00:15<01:10,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9211:  18% 44/246 [00:15<01:10,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0895:  18% 44/246 [00:15<01:10,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0895:  18% 45/246 [00:15<01:09,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0429:  18% 45/246 [00:16<01:09,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0429:  19% 46/246 [00:16<01:09,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8718:  19% 46/246 [00:16<01:09,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8718:  19% 47/246 [00:16<01:09,  2.85it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0157:  19% 47/246 [00:16<01:09,  2.85it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0157:  20% 48/246 [00:17<01:09,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8738:  20% 48/246 [00:17<01:09,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8738:  20% 49/246 [00:17<01:09,  2.85it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9027:  20% 49/246 [00:17<01:09,  2.85it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9027:  20% 50/246 [00:17<01:08,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9296:  20% 50/246 [00:17<01:08,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9296:  21% 51/246 [00:18<01:08,  2.83it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9272:  21% 51/246 [00:18<01:08,  2.83it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9272:  21% 52/246 [00:18<01:08,  2.85it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7335:  21% 52/246 [00:18<01:08,  2.85it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7335:  22% 53/246 [00:18<01:07,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2077:  22% 53/246 [00:18<01:07,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2077:  22% 54/246 [00:19<01:07,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0644:  22% 54/246 [00:19<01:07,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0644:  22% 55/246 [00:19<01:06,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7746:  22% 55/246 [00:19<01:06,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7746:  23% 56/246 [00:19<01:06,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8443:  23% 56/246 [00:19<01:06,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8443:  23% 57/246 [00:20<01:05,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0913:  23% 57/246 [00:20<01:05,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0913:  24% 58/246 [00:20<01:05,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8912:  24% 58/246 [00:20<01:05,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8912:  24% 59/246 [00:20<01:05,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0620:  24% 59/246 [00:21<01:05,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0620:  24% 60/246 [00:21<01:05,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9167:  24% 60/246 [00:21<01:05,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9167:  25% 61/246 [00:21<01:04,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9274:  25% 61/246 [00:21<01:04,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9274:  25% 62/246 [00:21<01:04,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8653:  25% 62/246 [00:22<01:04,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8653:  26% 63/246 [00:22<01:03,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9764:  26% 63/246 [00:22<01:03,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9764:  26% 64/246 [00:22<01:03,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2232:  26% 64/246 [00:22<01:03,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2232:  26% 65/246 [00:22<01:03,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0492:  26% 65/246 [00:23<01:03,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0492:  27% 66/246 [00:23<01:02,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1129:  27% 66/246 [00:23<01:02,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1129:  27% 67/246 [00:23<01:02,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9064:  27% 67/246 [00:23<01:02,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9064:  28% 68/246 [00:23<01:02,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9497:  28% 68/246 [00:24<01:02,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9497:  28% 69/246 [00:24<01:01,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0057:  28% 69/246 [00:24<01:01,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0057:  28% 70/246 [00:24<01:01,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9798:  28% 70/246 [00:24<01:01,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9798:  29% 71/246 [00:25<01:01,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0964:  29% 71/246 [00:25<01:01,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0964:  29% 72/246 [00:25<01:00,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1100:  29% 72/246 [00:25<01:00,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1100:  30% 73/246 [00:25<01:00,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8755:  30% 73/246 [00:25<01:00,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8755:  30% 74/246 [00:26<00:59,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0607:  30% 74/246 [00:26<00:59,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0607:  30% 75/246 [00:26<00:59,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8837:  30% 75/246 [00:26<00:59,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8837:  31% 76/246 [00:26<00:59,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9139:  31% 76/246 [00:26<00:59,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9139:  31% 77/246 [00:27<00:58,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9440:  31% 77/246 [00:27<00:58,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9440:  32% 78/246 [00:27<00:58,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9516:  32% 78/246 [00:27<00:58,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9516:  32% 79/246 [00:27<00:57,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8559:  32% 79/246 [00:27<00:57,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8559:  33% 80/246 [00:28<00:57,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0924:  33% 80/246 [00:28<00:57,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0924:  33% 81/246 [00:28<00:57,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0998:  33% 81/246 [00:28<00:57,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0998:  33% 82/246 [00:28<00:56,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9091:  33% 82/246 [00:29<00:56,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9091:  34% 83/246 [00:29<00:56,  2.89it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2181:  34% 83/246 [00:29<00:56,  2.89it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2181:  34% 84/246 [00:29<00:56,  2.89it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9774:  34% 84/246 [00:29<00:56,  2.89it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9774:  35% 85/246 [00:29<00:55,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7838:  35% 85/246 [00:30<00:55,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7838:  35% 86/246 [00:30<00:55,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8805:  35% 86/246 [00:30<00:55,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8805:  35% 87/246 [00:30<00:55,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9312:  35% 87/246 [00:30<00:55,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9312:  36% 88/246 [00:30<00:55,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7875:  36% 88/246 [00:31<00:55,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7875:  36% 89/246 [00:31<00:54,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0337:  36% 89/246 [00:31<00:54,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0337:  37% 90/246 [00:31<00:54,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9456:  37% 90/246 [00:31<00:54,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9456:  37% 91/246 [00:31<00:53,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0920:  37% 91/246 [00:32<00:53,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0920:  37% 92/246 [00:32<00:53,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8321:  37% 92/246 [00:32<00:53,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8321:  38% 93/246 [00:32<00:53,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9756:  38% 93/246 [00:32<00:53,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9756:  38% 94/246 [00:33<00:53,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8198:  38% 94/246 [00:33<00:53,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8198:  39% 95/246 [00:33<00:52,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8971:  39% 95/246 [00:33<00:52,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8971:  39% 96/246 [00:33<00:52,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9444:  39% 96/246 [00:33<00:52,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9444:  39% 97/246 [00:34<00:52,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9480:  39% 97/246 [00:34<00:52,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9480:  40% 98/246 [00:34<00:51,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8536:  40% 98/246 [00:34<00:51,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8536:  40% 99/246 [00:34<00:51,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0407:  40% 99/246 [00:34<00:51,  2.86it/s]\u001b[A\n","\n","  0% 0/600 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","  0% 1/600 [00:01<13:37,  1.36s/it]\u001b[A\u001b[A\n","\n","  0% 2/600 [00:01<07:55,  1.26it/s]\n","\n","Epochs 0/2. Running Loss:    1.0407:  41% 100/246 [01:24<37:05, 15.24s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.7940:  41% 100/246 [01:24<37:05, 15.24s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.7940:  41% 101/246 [01:25<26:04, 10.79s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.1872:  41% 101/246 [01:25<26:04, 10.79s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.1872:  41% 102/246 [01:25<18:22,  7.66s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9863:  41% 102/246 [01:25<18:22,  7.66s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9863:  42% 103/246 [01:25<13:02,  5.47s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9359:  42% 103/246 [01:26<13:02,  5.47s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9359:  42% 104/246 [01:26<09:18,  3.93s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.1240:  42% 104/246 [01:26<09:18,  3.93s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.1240:  43% 105/246 [01:26<06:42,  2.85s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.2098:  43% 105/246 [01:26<06:42,  2.85s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.2098:  43% 106/246 [01:26<04:55,  2.11s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8379:  43% 106/246 [01:27<04:55,  2.11s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8379:  43% 107/246 [01:27<03:41,  1.59s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9843:  43% 107/246 [01:27<03:41,  1.59s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9843:  44% 108/246 [01:27<02:46,  1.21s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8750:  44% 108/246 [01:27<02:46,  1.21s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8750:  44% 109/246 [01:27<02:09,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8700:  44% 109/246 [01:28<02:09,  1.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8700:  45% 110/246 [01:28<01:43,  1.31it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9124:  45% 110/246 [01:28<01:43,  1.31it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9124:  45% 111/246 [01:28<01:26,  1.57it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8607:  45% 111/246 [01:28<01:26,  1.57it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8607:  46% 112/246 [01:28<01:13,  1.82it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9223:  46% 112/246 [01:29<01:13,  1.82it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9223:  46% 113/246 [01:29<01:04,  2.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9334:  46% 113/246 [01:29<01:04,  2.05it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9334:  46% 114/246 [01:29<00:58,  2.26it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8838:  46% 114/246 [01:29<00:58,  2.26it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8838:  47% 115/246 [01:30<00:53,  2.44it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9091:  47% 115/246 [01:30<00:53,  2.44it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9091:  47% 116/246 [01:30<00:50,  2.58it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8968:  47% 116/246 [01:30<00:50,  2.58it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8968:  48% 117/246 [01:30<00:48,  2.67it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9829:  48% 117/246 [01:30<00:48,  2.67it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9829:  48% 118/246 [01:31<00:46,  2.76it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9600:  48% 118/246 [01:31<00:46,  2.76it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9600:  48% 119/246 [01:31<00:44,  2.83it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8382:  48% 119/246 [01:31<00:44,  2.83it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8382:  49% 120/246 [01:31<00:43,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8758:  49% 120/246 [01:31<00:43,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8758:  49% 121/246 [01:32<00:43,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9314:  49% 121/246 [01:32<00:43,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9314:  50% 122/246 [01:32<00:42,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0364:  50% 122/246 [01:32<00:42,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0364:  50% 123/246 [01:32<00:41,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8449:  50% 123/246 [01:32<00:41,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8449:  50% 124/246 [01:33<00:41,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9415:  50% 124/246 [01:33<00:41,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9415:  51% 125/246 [01:33<00:40,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9062:  51% 125/246 [01:33<00:40,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9062:  51% 126/246 [01:33<00:40,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9086:  51% 126/246 [01:33<00:40,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9086:  52% 127/246 [01:34<00:40,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7640:  52% 127/246 [01:34<00:40,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7640:  52% 128/246 [01:34<00:39,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0752:  52% 128/246 [01:34<00:39,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0752:  52% 129/246 [01:34<00:39,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0130:  52% 129/246 [01:34<00:39,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0130:  53% 130/246 [01:35<00:39,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0483:  53% 130/246 [01:35<00:39,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0483:  53% 131/246 [01:35<00:38,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9613:  53% 131/246 [01:35<00:38,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9613:  54% 132/246 [01:35<00:38,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0540:  54% 132/246 [01:35<00:38,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0540:  54% 133/246 [01:36<00:38,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8169:  54% 133/246 [01:36<00:38,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8169:  54% 134/246 [01:36<00:37,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9479:  54% 134/246 [01:36<00:37,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9479:  55% 135/246 [01:36<00:37,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9767:  55% 135/246 [01:36<00:37,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9767:  55% 136/246 [01:37<00:36,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0240:  55% 136/246 [01:37<00:36,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0240:  56% 137/246 [01:37<00:36,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9742:  56% 137/246 [01:37<00:36,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9742:  56% 138/246 [01:37<00:36,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0662:  56% 138/246 [01:37<00:36,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0662:  57% 139/246 [01:38<00:36,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0130:  57% 139/246 [01:38<00:36,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0130:  57% 140/246 [01:38<00:35,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.3816:  57% 140/246 [01:38<00:35,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.3816:  57% 141/246 [01:38<00:35,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.6501:  57% 141/246 [01:38<00:35,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.6501:  58% 142/246 [01:39<00:34,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8435:  58% 142/246 [01:39<00:34,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8435:  58% 143/246 [01:39<00:35,  2.89it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9199:  58% 143/246 [01:39<00:35,  2.89it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9199:  59% 144/246 [01:39<00:35,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7274:  59% 144/246 [01:39<00:35,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7274:  59% 145/246 [01:40<00:34,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8311:  59% 145/246 [01:40<00:34,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8311:  59% 146/246 [01:40<00:34,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7995:  59% 146/246 [01:40<00:34,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7995:  60% 147/246 [01:40<00:33,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7958:  60% 147/246 [01:40<00:33,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7958:  60% 148/246 [01:41<00:33,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8299:  60% 148/246 [01:41<00:33,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8299:  61% 149/246 [01:41<00:33,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0173:  61% 149/246 [01:41<00:33,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0173:  61% 150/246 [01:41<00:33,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8973:  61% 150/246 [01:41<00:33,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8973:  61% 151/246 [01:42<00:31,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9926:  61% 151/246 [01:42<00:31,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9926:  62% 152/246 [01:42<00:32,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1242:  62% 152/246 [01:42<00:32,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1242:  62% 153/246 [01:42<00:31,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0292:  62% 153/246 [01:43<00:31,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0292:  63% 154/246 [01:43<00:31,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0333:  63% 154/246 [01:43<00:31,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0333:  63% 155/246 [01:43<00:30,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9187:  63% 155/246 [01:43<00:30,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9187:  63% 156/246 [01:43<00:30,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8718:  63% 156/246 [01:44<00:30,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8718:  64% 157/246 [01:44<00:30,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8398:  64% 157/246 [01:44<00:30,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8398:  64% 158/246 [01:44<00:29,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9793:  64% 158/246 [01:44<00:29,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9793:  65% 159/246 [01:44<00:29,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9373:  65% 159/246 [01:45<00:29,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9373:  65% 160/246 [01:45<00:29,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8320:  65% 160/246 [01:45<00:29,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8320:  65% 161/246 [01:45<00:29,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9663:  65% 161/246 [01:45<00:29,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9663:  66% 162/246 [01:45<00:29,  2.85it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7639:  66% 162/246 [01:46<00:29,  2.85it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7639:  66% 163/246 [01:46<00:29,  2.85it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0851:  66% 163/246 [01:46<00:29,  2.85it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0851:  67% 164/246 [01:46<00:28,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8579:  67% 164/246 [01:46<00:28,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8579:  67% 165/246 [01:46<00:27,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9728:  67% 165/246 [01:47<00:27,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9728:  67% 166/246 [01:47<00:27,  2.89it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7003:  67% 166/246 [01:47<00:27,  2.89it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7003:  68% 167/246 [01:47<00:27,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2255:  68% 167/246 [01:47<00:27,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2255:  68% 168/246 [01:47<00:26,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7661:  68% 168/246 [01:48<00:26,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7661:  69% 169/246 [01:48<00:26,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0099:  69% 169/246 [01:48<00:26,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0099:  69% 170/246 [01:48<00:25,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8421:  69% 170/246 [01:48<00:25,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8421:  70% 171/246 [01:49<00:25,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0413:  70% 171/246 [01:49<00:25,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0413:  70% 172/246 [01:49<00:25,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1029:  70% 172/246 [01:49<00:25,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1029:  70% 173/246 [01:49<00:24,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9319:  70% 173/246 [01:49<00:24,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9319:  71% 174/246 [01:50<00:24,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7261:  71% 174/246 [01:50<00:24,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7261:  71% 175/246 [01:50<00:24,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7729:  71% 175/246 [01:50<00:24,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7729:  72% 176/246 [01:50<00:23,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1339:  72% 176/246 [01:50<00:23,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1339:  72% 177/246 [01:51<00:23,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9738:  72% 177/246 [01:51<00:23,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9738:  72% 178/246 [01:51<00:23,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8908:  72% 178/246 [01:51<00:23,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8908:  73% 179/246 [01:51<00:22,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9721:  73% 179/246 [01:51<00:22,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9721:  73% 180/246 [01:52<00:22,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7475:  73% 180/246 [01:52<00:22,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7475:  74% 181/246 [01:52<00:22,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8717:  74% 181/246 [01:52<00:22,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8717:  74% 182/246 [01:52<00:21,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9654:  74% 182/246 [01:52<00:21,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9654:  74% 183/246 [01:53<00:21,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9539:  74% 183/246 [01:53<00:21,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9539:  75% 184/246 [01:53<00:21,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8332:  75% 184/246 [01:53<00:21,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8332:  75% 185/246 [01:53<00:20,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9071:  75% 185/246 [01:53<00:20,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9071:  76% 186/246 [01:54<00:20,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8528:  76% 186/246 [01:54<00:20,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8528:  76% 187/246 [01:54<00:20,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7034:  76% 187/246 [01:54<00:20,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7034:  76% 188/246 [01:54<00:19,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1307:  76% 188/246 [01:54<00:19,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1307:  77% 189/246 [01:55<00:19,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8442:  77% 189/246 [01:55<00:19,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8442:  77% 190/246 [01:55<00:18,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9349:  77% 190/246 [01:55<00:18,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9349:  78% 191/246 [01:55<00:18,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9248:  78% 191/246 [01:55<00:18,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9248:  78% 192/246 [01:56<00:18,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9202:  78% 192/246 [01:56<00:18,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9202:  78% 193/246 [01:56<00:18,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9426:  78% 193/246 [01:56<00:18,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9426:  79% 194/246 [01:56<00:17,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0095:  79% 194/246 [01:56<00:17,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0095:  79% 195/246 [01:57<00:17,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8694:  79% 195/246 [01:57<00:17,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8694:  80% 196/246 [01:57<00:17,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0171:  80% 196/246 [01:57<00:17,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0171:  80% 197/246 [01:57<00:16,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8526:  80% 197/246 [01:58<00:16,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8526:  80% 198/246 [01:58<00:16,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8014:  80% 198/246 [01:58<00:16,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8014:  81% 199/246 [01:58<00:16,  2.83it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8494:  81% 199/246 [01:58<00:16,  2.83it/s]\u001b[A\n","\n","  0% 0/600 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","  0% 1/600 [00:01<16:57,  1.70s/it]\u001b[A\u001b[A\n","\n","  0% 2/600 [00:01<09:39,  1.03it/s]\n","\n","Epochs 0/2. Running Loss:    0.8494:  81% 200/246 [02:28<07:06,  9.28s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8391:  81% 200/246 [02:28<07:06,  9.28s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8391:  82% 201/246 [02:29<04:57,  6.61s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9241:  82% 201/246 [02:29<04:57,  6.61s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9241:  82% 202/246 [02:29<03:27,  4.73s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8262:  82% 202/246 [02:29<03:27,  4.73s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8262:  83% 203/246 [02:29<02:26,  3.41s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9584:  83% 203/246 [02:29<02:26,  3.41s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9584:  83% 204/246 [02:30<01:44,  2.49s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.6674:  83% 204/246 [02:30<01:44,  2.49s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.6674:  83% 205/246 [02:30<01:15,  1.84s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8050:  83% 205/246 [02:30<01:15,  1.84s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8050:  84% 206/246 [02:30<00:55,  1.39s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.1665:  84% 206/246 [02:30<00:55,  1.39s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.1665:  84% 207/246 [02:31<00:41,  1.07s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.7383:  84% 207/246 [02:31<00:41,  1.07s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.7383:  85% 208/246 [02:31<00:32,  1.17it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9304:  85% 208/246 [02:31<00:32,  1.17it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9304:  85% 209/246 [02:31<00:25,  1.43it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.6939:  85% 209/246 [02:31<00:25,  1.43it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.6939:  85% 210/246 [02:32<00:21,  1.68it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0042:  85% 210/246 [02:32<00:21,  1.68it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0042:  86% 211/246 [02:32<00:18,  1.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9955:  86% 211/246 [02:32<00:18,  1.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9955:  86% 212/246 [02:32<00:15,  2.16it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8577:  86% 212/246 [02:32<00:15,  2.16it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8577:  87% 213/246 [02:33<00:14,  2.27it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9501:  87% 213/246 [02:33<00:14,  2.27it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9501:  87% 214/246 [02:33<00:12,  2.46it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9586:  87% 214/246 [02:33<00:12,  2.46it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9586:  87% 215/246 [02:33<00:11,  2.61it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0355:  87% 215/246 [02:33<00:11,  2.61it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0355:  88% 216/246 [02:34<00:11,  2.70it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8794:  88% 216/246 [02:34<00:11,  2.70it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8794:  88% 217/246 [02:34<00:10,  2.78it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9409:  88% 217/246 [02:34<00:10,  2.78it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9409:  89% 218/246 [02:34<00:09,  2.85it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9535:  89% 218/246 [02:34<00:09,  2.85it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9535:  89% 219/246 [02:35<00:09,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8277:  89% 219/246 [02:35<00:09,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8277:  89% 220/246 [02:35<00:09,  2.76it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1359:  89% 220/246 [02:35<00:09,  2.76it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1359:  90% 221/246 [02:35<00:08,  2.84it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8671:  90% 221/246 [02:36<00:08,  2.84it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8671:  90% 222/246 [02:36<00:08,  2.78it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8577:  90% 222/246 [02:36<00:08,  2.78it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8577:  91% 223/246 [02:36<00:08,  2.84it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9427:  91% 223/246 [02:36<00:08,  2.84it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9427:  91% 224/246 [02:36<00:07,  2.89it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8952:  91% 224/246 [02:37<00:07,  2.89it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8952:  91% 225/246 [02:37<00:07,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7888:  91% 225/246 [02:37<00:07,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7888:  92% 226/246 [02:37<00:06,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7971:  92% 226/246 [02:37<00:06,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7971:  92% 227/246 [02:37<00:06,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8150:  92% 227/246 [02:38<00:06,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8150:  93% 228/246 [02:38<00:06,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8470:  93% 228/246 [02:38<00:06,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8470:  93% 229/246 [02:38<00:05,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0524:  93% 229/246 [02:38<00:05,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0524:  93% 230/246 [02:38<00:05,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1352:  93% 230/246 [02:39<00:05,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1352:  94% 231/246 [02:39<00:05,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9069:  94% 231/246 [02:39<00:05,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9069:  94% 232/246 [02:39<00:04,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9508:  94% 232/246 [02:39<00:04,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9508:  95% 233/246 [02:39<00:04,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9370:  95% 233/246 [02:40<00:04,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9370:  95% 234/246 [02:40<00:04,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0341:  95% 234/246 [02:40<00:04,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0341:  96% 235/246 [02:40<00:03,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9503:  96% 235/246 [02:40<00:03,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9503:  96% 236/246 [02:41<00:03,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0200:  96% 236/246 [02:41<00:03,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0200:  96% 237/246 [02:41<00:03,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7828:  96% 237/246 [02:41<00:03,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7828:  97% 238/246 [02:41<00:02,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9799:  97% 238/246 [02:41<00:02,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9799:  97% 239/246 [02:42<00:02,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1150:  97% 239/246 [02:42<00:02,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1150:  98% 240/246 [02:42<00:02,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8388:  98% 240/246 [02:42<00:02,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8388:  98% 241/246 [02:42<00:01,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8520:  98% 241/246 [02:42<00:01,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8520:  98% 242/246 [02:43<00:01,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9726:  98% 242/246 [02:43<00:01,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9726:  99% 243/246 [02:43<00:01,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1845:  99% 243/246 [02:43<00:01,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1845:  99% 244/246 [02:43<00:00,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7596:  99% 244/246 [02:43<00:00,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7596: 100% 245/246 [02:44<00:00,  2.99it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9021: 100% 245/246 [02:44<00:00,  2.99it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9021: 100% 246/246 [02:44<00:00,  1.50it/s]\n","\n","  0% 0/600 [00:00<?, ?it/s]\u001b[A\n","  0% 1/600 [00:01<16:10,  1.62s/it]\u001b[A\n","  0% 2/600 [00:01<09:08,  1.09it/s]\n","Epoch 2 of 2:  50% 1/2 [03:13<03:13, 193.15s/it]\n","Running Epoch 1 of 2:   0% 0/246 [00:00<?, ?it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0980:   0% 0/246 [00:00<?, ?it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0980:   0% 1/246 [00:00<01:32,  2.66it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1053:   0% 1/246 [00:00<01:32,  2.66it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1053:   1% 2/246 [00:00<01:25,  2.87it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9047:   1% 2/246 [00:00<01:25,  2.87it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9047:   1% 3/246 [00:01<01:23,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0153:   1% 3/246 [00:01<01:23,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0153:   2% 4/246 [00:01<01:23,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7905:   2% 4/246 [00:01<01:23,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7905:   2% 5/246 [00:01<01:21,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9439:   2% 5/246 [00:01<01:21,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9439:   2% 6/246 [00:02<01:22,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8538:   2% 6/246 [00:02<01:22,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8538:   3% 7/246 [00:02<01:20,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9201:   3% 7/246 [00:02<01:20,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9201:   3% 8/246 [00:02<01:20,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8409:   3% 8/246 [00:02<01:20,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8409:   4% 9/246 [00:03<01:19,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9636:   4% 9/246 [00:03<01:19,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9636:   4% 10/246 [00:03<01:19,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8146:   4% 10/246 [00:03<01:19,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8146:   4% 11/246 [00:03<01:18,  3.00it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9091:   4% 11/246 [00:03<01:18,  3.00it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9091:   5% 12/246 [00:04<01:17,  3.01it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9716:   5% 12/246 [00:04<01:17,  3.01it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9716:   5% 13/246 [00:04<01:18,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9557:   5% 13/246 [00:04<01:18,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9557:   6% 14/246 [00:04<01:17,  3.00it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7404:   6% 14/246 [00:04<01:17,  3.00it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7404:   6% 15/246 [00:05<01:17,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9373:   6% 15/246 [00:05<01:17,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9373:   7% 16/246 [00:05<01:17,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0053:   7% 16/246 [00:05<01:17,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0053:   7% 17/246 [00:05<01:16,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8510:   7% 17/246 [00:05<01:16,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8510:   7% 18/246 [00:06<01:16,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8562:   7% 18/246 [00:06<01:16,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8562:   8% 19/246 [00:06<01:16,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7857:   8% 19/246 [00:06<01:16,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7857:   8% 20/246 [00:06<01:15,  3.00it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8739:   8% 20/246 [00:06<01:15,  3.00it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8739:   9% 21/246 [00:07<01:15,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9443:   9% 21/246 [00:07<01:15,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9443:   9% 22/246 [00:07<01:14,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8777:   9% 22/246 [00:07<01:14,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8777:   9% 23/246 [00:07<01:14,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8641:   9% 23/246 [00:07<01:14,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8641:  10% 24/246 [00:08<01:14,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8724:  10% 24/246 [00:08<01:14,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8724:  10% 25/246 [00:08<01:13,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8825:  10% 25/246 [00:08<01:13,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8825:  11% 26/246 [00:08<01:13,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0983:  11% 26/246 [00:08<01:13,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0983:  11% 27/246 [00:09<01:13,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7774:  11% 27/246 [00:09<01:13,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7774:  11% 28/246 [00:09<01:12,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9394:  11% 28/246 [00:09<01:12,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9394:  12% 29/246 [00:09<01:12,  3.00it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7843:  12% 29/246 [00:09<01:12,  3.00it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7843:  12% 30/246 [00:10<01:12,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7107:  12% 30/246 [00:10<01:12,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7107:  13% 31/246 [00:10<01:12,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9828:  13% 31/246 [00:10<01:12,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9828:  13% 32/246 [00:10<01:12,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0846:  13% 32/246 [00:10<01:12,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0846:  13% 33/246 [00:11<01:11,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7890:  13% 33/246 [00:11<01:11,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7890:  14% 34/246 [00:11<01:11,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7372:  14% 34/246 [00:11<01:11,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7372:  14% 35/246 [00:11<01:10,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0379:  14% 35/246 [00:11<01:10,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0379:  15% 36/246 [00:12<01:10,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1758:  15% 36/246 [00:12<01:10,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1758:  15% 37/246 [00:12<01:10,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0625:  15% 37/246 [00:12<01:10,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0625:  15% 38/246 [00:12<01:09,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9396:  15% 38/246 [00:12<01:09,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9396:  16% 39/246 [00:13<01:09,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0117:  16% 39/246 [00:13<01:09,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0117:  16% 40/246 [00:13<01:09,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0838:  16% 40/246 [00:13<01:09,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0838:  17% 41/246 [00:13<01:09,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0683:  17% 41/246 [00:13<01:09,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0683:  17% 42/246 [00:14<01:09,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9227:  17% 42/246 [00:14<01:09,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9227:  17% 43/246 [00:14<01:08,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9099:  17% 43/246 [00:14<01:08,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9099:  18% 44/246 [00:14<01:08,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9822:  18% 44/246 [00:14<01:08,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9822:  18% 45/246 [00:15<01:08,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7423:  18% 45/246 [00:15<01:08,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7423:  19% 46/246 [00:15<01:07,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7082:  19% 46/246 [00:15<01:07,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7082:  19% 47/246 [00:15<01:07,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8730:  19% 47/246 [00:15<01:07,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8730:  20% 48/246 [00:16<01:07,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0249:  20% 48/246 [00:16<01:07,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0249:  20% 49/246 [00:16<01:07,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7650:  20% 49/246 [00:16<01:07,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7650:  20% 50/246 [00:16<01:06,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8844:  20% 50/246 [00:16<01:06,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8844:  21% 51/246 [00:17<01:06,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0176:  21% 51/246 [00:17<01:06,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0176:  21% 52/246 [00:17<01:05,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6917:  21% 52/246 [00:17<01:05,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6917:  22% 53/246 [00:17<01:05,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7099:  22% 53/246 [00:18<01:05,  2.94it/s]\u001b[A\n","\n","  0% 0/600 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","  0% 2/600 [00:01<08:56,  1.12it/s]\n","\n","Epochs 1/2. Running Loss:    0.7099:  22% 54/246 [00:45<27:22,  8.55s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9775:  22% 54/246 [00:45<27:22,  8.55s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9775:  22% 55/246 [00:45<19:23,  6.09s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7530:  22% 55/246 [00:46<19:23,  6.09s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7530:  23% 56/246 [00:46<13:49,  4.37s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.0179:  23% 56/246 [00:46<13:49,  4.37s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.0179:  23% 57/246 [00:46<09:56,  3.16s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7948:  23% 57/246 [00:46<09:56,  3.16s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7948:  24% 58/246 [00:46<07:14,  2.31s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.0625:  24% 58/246 [00:47<07:14,  2.31s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.0625:  24% 59/246 [00:47<05:21,  1.72s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9302:  24% 59/246 [00:47<05:21,  1.72s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9302:  24% 60/246 [00:47<04:02,  1.30s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9122:  24% 60/246 [00:47<04:02,  1.30s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9122:  25% 61/246 [00:47<03:08,  1.02s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7071:  25% 61/246 [00:48<03:08,  1.02s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7071:  25% 62/246 [00:48<02:29,  1.23it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7904:  25% 62/246 [00:48<02:29,  1.23it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7904:  26% 63/246 [00:48<02:02,  1.49it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9575:  26% 63/246 [00:48<02:02,  1.49it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9575:  26% 64/246 [00:48<01:44,  1.75it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.3122:  26% 64/246 [00:49<01:44,  1.75it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.3122:  26% 65/246 [00:49<01:30,  2.01it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8148:  26% 65/246 [00:49<01:30,  2.01it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8148:  27% 66/246 [00:49<01:21,  2.21it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8670:  27% 66/246 [00:49<01:21,  2.21it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8670:  27% 67/246 [00:49<01:15,  2.39it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8079:  27% 67/246 [00:50<01:15,  2.39it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8079:  28% 68/246 [00:50<01:10,  2.52it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1234:  28% 68/246 [00:50<01:10,  2.52it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1234:  28% 69/246 [00:50<01:07,  2.60it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1171:  28% 69/246 [00:50<01:07,  2.60it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1171:  28% 70/246 [00:51<01:08,  2.58it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0547:  28% 70/246 [00:51<01:08,  2.58it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0547:  29% 71/246 [00:51<01:04,  2.73it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1300:  29% 71/246 [00:51<01:04,  2.73it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1300:  29% 72/246 [00:51<01:02,  2.80it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7096:  29% 72/246 [00:51<01:02,  2.80it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7096:  30% 73/246 [00:52<01:01,  2.79it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6464:  30% 73/246 [00:52<01:01,  2.79it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6464:  30% 74/246 [00:52<01:01,  2.80it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9532:  30% 74/246 [00:52<01:01,  2.80it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9532:  30% 75/246 [00:52<01:00,  2.82it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9702:  30% 75/246 [00:52<01:00,  2.82it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9702:  31% 76/246 [00:53<00:59,  2.87it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7788:  31% 76/246 [00:53<00:59,  2.87it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7788:  31% 77/246 [00:53<00:58,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8412:  31% 77/246 [00:53<00:58,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8412:  32% 78/246 [00:53<00:57,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8570:  32% 78/246 [00:53<00:57,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8570:  32% 79/246 [00:54<00:56,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8969:  32% 79/246 [00:54<00:56,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8969:  33% 80/246 [00:54<00:57,  2.88it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8005:  33% 80/246 [00:54<00:57,  2.88it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8005:  33% 81/246 [00:54<00:57,  2.88it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8389:  33% 81/246 [00:54<00:57,  2.88it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8389:  33% 82/246 [00:55<00:57,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1015:  33% 82/246 [00:55<00:57,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1015:  34% 83/246 [00:55<00:56,  2.88it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6660:  34% 83/246 [00:55<00:56,  2.88it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6660:  34% 84/246 [00:55<00:57,  2.82it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7494:  34% 84/246 [00:56<00:57,  2.82it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7494:  35% 85/246 [00:56<00:56,  2.84it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9412:  35% 85/246 [00:56<00:56,  2.84it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9412:  35% 86/246 [00:56<00:55,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9984:  35% 86/246 [00:56<00:55,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9984:  35% 87/246 [00:56<00:54,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9166:  35% 87/246 [00:57<00:54,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9166:  36% 88/246 [00:57<00:54,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7605:  36% 88/246 [00:57<00:54,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7605:  36% 89/246 [00:57<00:54,  2.88it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8601:  36% 89/246 [00:57<00:54,  2.88it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8601:  37% 90/246 [00:57<00:54,  2.87it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6728:  37% 90/246 [00:58<00:54,  2.87it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6728:  37% 91/246 [00:58<00:52,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1808:  37% 91/246 [00:58<00:52,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1808:  37% 92/246 [00:58<00:52,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8107:  37% 92/246 [00:58<00:52,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8107:  38% 93/246 [00:58<00:51,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7632:  38% 93/246 [00:59<00:51,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7632:  38% 94/246 [00:59<00:51,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1119:  38% 94/246 [00:59<00:51,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1119:  39% 95/246 [00:59<00:50,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9843:  39% 95/246 [00:59<00:50,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9843:  39% 96/246 [00:59<00:50,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8745:  39% 96/246 [01:00<00:50,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8745:  39% 97/246 [01:00<00:50,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6412:  39% 97/246 [01:00<00:50,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6412:  40% 98/246 [01:00<00:49,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1002:  40% 98/246 [01:00<00:49,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1002:  40% 99/246 [01:00<00:49,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8121:  40% 99/246 [01:01<00:49,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8121:  41% 100/246 [01:01<00:48,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8890:  41% 100/246 [01:01<00:48,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8890:  41% 101/246 [01:01<00:49,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7030:  41% 101/246 [01:01<00:49,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7030:  41% 102/246 [01:02<00:48,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7298:  41% 102/246 [01:02<00:48,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7298:  42% 103/246 [01:02<00:48,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9081:  42% 103/246 [01:02<00:48,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9081:  42% 104/246 [01:02<00:48,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7015:  42% 104/246 [01:02<00:48,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7015:  43% 105/246 [01:03<00:47,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6984:  43% 105/246 [01:03<00:47,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6984:  43% 106/246 [01:03<00:47,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8932:  43% 106/246 [01:03<00:47,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8932:  43% 107/246 [01:03<00:46,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7380:  43% 107/246 [01:03<00:46,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7380:  44% 108/246 [01:04<00:46,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9416:  44% 108/246 [01:04<00:46,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9416:  44% 109/246 [01:04<00:45,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8536:  44% 109/246 [01:04<00:45,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8536:  45% 110/246 [01:04<00:45,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7609:  45% 110/246 [01:04<00:45,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7609:  45% 111/246 [01:05<00:45,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9550:  45% 111/246 [01:05<00:45,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9550:  46% 112/246 [01:05<00:44,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0755:  46% 112/246 [01:05<00:44,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0755:  46% 113/246 [01:05<00:44,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0834:  46% 113/246 [01:05<00:44,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0834:  46% 114/246 [01:06<00:44,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9716:  46% 114/246 [01:06<00:44,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9716:  47% 115/246 [01:06<00:44,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5539:  47% 115/246 [01:06<00:44,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5539:  47% 116/246 [01:06<00:43,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9669:  47% 116/246 [01:06<00:43,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9669:  48% 117/246 [01:07<00:43,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8863:  48% 117/246 [01:07<00:43,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8863:  48% 118/246 [01:07<00:43,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7879:  48% 118/246 [01:07<00:43,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7879:  48% 119/246 [01:07<00:42,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7768:  48% 119/246 [01:07<00:42,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7768:  49% 120/246 [01:08<00:42,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0287:  49% 120/246 [01:08<00:42,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0287:  49% 121/246 [01:08<00:42,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8970:  49% 121/246 [01:08<00:42,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8970:  50% 122/246 [01:08<00:41,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8281:  50% 122/246 [01:08<00:41,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8281:  50% 123/246 [01:09<00:42,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7523:  50% 123/246 [01:09<00:42,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7523:  50% 124/246 [01:09<00:43,  2.83it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1255:  50% 124/246 [01:09<00:43,  2.83it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1255:  51% 125/246 [01:09<00:42,  2.84it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8177:  51% 125/246 [01:09<00:42,  2.84it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8177:  51% 126/246 [01:10<00:41,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8209:  51% 126/246 [01:10<00:41,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8209:  52% 127/246 [01:10<00:40,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6529:  52% 127/246 [01:10<00:40,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6529:  52% 128/246 [01:10<00:40,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7784:  52% 128/246 [01:10<00:40,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7784:  52% 129/246 [01:11<00:40,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6110:  52% 129/246 [01:11<00:40,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6110:  53% 130/246 [01:11<00:40,  2.88it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9753:  53% 130/246 [01:11<00:40,  2.88it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9753:  53% 131/246 [01:11<00:39,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8203:  53% 131/246 [01:12<00:39,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8203:  54% 132/246 [01:12<00:39,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0219:  54% 132/246 [01:12<00:39,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0219:  54% 133/246 [01:12<00:38,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0212:  54% 133/246 [01:12<00:38,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0212:  54% 134/246 [01:12<00:39,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8235:  54% 134/246 [01:13<00:39,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8235:  55% 135/246 [01:13<00:38,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8172:  55% 135/246 [01:13<00:38,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8172:  55% 136/246 [01:13<00:37,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7710:  55% 136/246 [01:13<00:37,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7710:  56% 137/246 [01:13<00:37,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0679:  56% 137/246 [01:14<00:37,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0679:  56% 138/246 [01:14<00:36,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8170:  56% 138/246 [01:14<00:36,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8170:  57% 139/246 [01:14<00:36,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6780:  57% 139/246 [01:14<00:36,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6780:  57% 140/246 [01:14<00:35,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6994:  57% 140/246 [01:15<00:35,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6994:  57% 141/246 [01:15<00:35,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8210:  57% 141/246 [01:15<00:35,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8210:  58% 142/246 [01:15<00:34,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8640:  58% 142/246 [01:15<00:34,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8640:  58% 143/246 [01:15<00:34,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1037:  58% 143/246 [01:16<00:34,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1037:  59% 144/246 [01:16<00:34,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1020:  59% 144/246 [01:16<00:34,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1020:  59% 145/246 [01:16<00:33,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9554:  59% 145/246 [01:16<00:33,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9554:  59% 146/246 [01:16<00:33,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9160:  59% 146/246 [01:17<00:33,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9160:  60% 147/246 [01:17<00:33,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6755:  60% 147/246 [01:17<00:33,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6755:  60% 148/246 [01:17<00:32,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9753:  60% 148/246 [01:17<00:32,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9753:  61% 149/246 [01:17<00:32,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8603:  61% 149/246 [01:18<00:32,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8603:  61% 150/246 [01:18<00:32,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7033:  61% 150/246 [01:18<00:32,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7033:  61% 151/246 [01:18<00:31,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8096:  61% 151/246 [01:18<00:31,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8096:  62% 152/246 [01:18<00:31,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8119:  62% 152/246 [01:19<00:31,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8119:  62% 153/246 [01:19<00:31,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0063:  62% 153/246 [01:19<00:31,  2.98it/s]\u001b[A\n","\n","  0% 0/600 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","  0% 1/600 [00:01<12:00,  1.20s/it]\u001b[A\u001b[A\n","\n","  0% 2/600 [00:01<07:02,  1.42it/s]\n","\n","Epochs 1/2. Running Loss:    1.0063:  63% 154/246 [01:49<14:06,  9.20s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.6328:  63% 154/246 [01:49<14:06,  9.20s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.6328:  63% 155/246 [01:49<09:56,  6.55s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.6986:  63% 155/246 [01:49<09:56,  6.55s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.6986:  63% 156/246 [01:49<07:02,  4.69s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9619:  63% 156/246 [01:50<07:02,  4.69s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9619:  64% 157/246 [01:50<05:00,  3.38s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9611:  64% 157/246 [01:50<05:00,  3.38s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9611:  64% 158/246 [01:50<03:37,  2.47s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9603:  64% 158/246 [01:50<03:37,  2.47s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9603:  65% 159/246 [01:50<02:40,  1.84s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9002:  65% 159/246 [01:51<02:40,  1.84s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9002:  65% 160/246 [01:51<01:59,  1.39s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8299:  65% 160/246 [01:51<01:59,  1.39s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8299:  65% 161/246 [01:51<01:32,  1.08s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8961:  65% 161/246 [01:51<01:32,  1.08s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8961:  66% 162/246 [01:51<01:11,  1.17it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7938:  66% 162/246 [01:52<01:11,  1.17it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7938:  66% 163/246 [01:52<00:57,  1.43it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9089:  66% 163/246 [01:52<00:57,  1.43it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9089:  67% 164/246 [01:52<00:48,  1.70it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9129:  67% 164/246 [01:52<00:48,  1.70it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9129:  67% 165/246 [01:52<00:41,  1.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9101:  67% 165/246 [01:53<00:41,  1.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9101:  67% 166/246 [01:53<00:36,  2.19it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8341:  67% 166/246 [01:53<00:36,  2.19it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8341:  68% 167/246 [01:53<00:33,  2.38it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9520:  68% 167/246 [01:53<00:33,  2.38it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9520:  68% 168/246 [01:53<00:30,  2.54it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6462:  68% 168/246 [01:54<00:30,  2.54it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6462:  69% 169/246 [01:54<00:29,  2.65it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6857:  69% 169/246 [01:54<00:29,  2.65it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6857:  69% 170/246 [01:54<00:27,  2.74it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0716:  69% 170/246 [01:54<00:27,  2.74it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0716:  70% 171/246 [01:54<00:26,  2.83it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6135:  70% 171/246 [01:55<00:26,  2.83it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6135:  70% 172/246 [01:55<00:25,  2.88it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7320:  70% 172/246 [01:55<00:25,  2.88it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7320:  70% 173/246 [01:55<00:25,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9963:  70% 173/246 [01:55<00:25,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9963:  71% 174/246 [01:55<00:24,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9156:  71% 174/246 [01:56<00:24,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9156:  71% 175/246 [01:56<00:23,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9242:  71% 175/246 [01:56<00:23,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9242:  72% 176/246 [01:56<00:23,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7490:  72% 176/246 [01:56<00:23,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7490:  72% 177/246 [01:56<00:23,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0151:  72% 177/246 [01:57<00:23,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0151:  72% 178/246 [01:57<00:22,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8144:  72% 178/246 [01:57<00:22,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8144:  73% 179/246 [01:57<00:22,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5909:  73% 179/246 [01:57<00:22,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5909:  73% 180/246 [01:57<00:22,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7400:  73% 180/246 [01:58<00:22,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7400:  74% 181/246 [01:58<00:21,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8501:  74% 181/246 [01:58<00:21,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8501:  74% 182/246 [01:58<00:21,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0577:  74% 182/246 [01:58<00:21,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0577:  74% 183/246 [01:59<00:21,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7645:  74% 183/246 [01:59<00:21,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7645:  75% 184/246 [01:59<00:20,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5848:  75% 184/246 [01:59<00:20,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5848:  75% 185/246 [01:59<00:20,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8129:  75% 185/246 [01:59<00:20,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8129:  76% 186/246 [02:00<00:20,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9041:  76% 186/246 [02:00<00:20,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9041:  76% 187/246 [02:00<00:19,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7984:  76% 187/246 [02:00<00:19,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7984:  76% 188/246 [02:00<00:19,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0420:  76% 188/246 [02:00<00:19,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0420:  77% 189/246 [02:01<00:19,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9188:  77% 189/246 [02:01<00:19,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9188:  77% 190/246 [02:01<00:18,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7879:  77% 190/246 [02:01<00:18,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7879:  78% 191/246 [02:01<00:18,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6713:  78% 191/246 [02:01<00:18,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6713:  78% 192/246 [02:02<00:18,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7432:  78% 192/246 [02:02<00:18,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7432:  78% 193/246 [02:02<00:17,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9616:  78% 193/246 [02:02<00:17,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9616:  79% 194/246 [02:02<00:17,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7247:  79% 194/246 [02:02<00:17,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7247:  79% 195/246 [02:03<00:17,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7917:  79% 195/246 [02:03<00:17,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7917:  80% 196/246 [02:03<00:16,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7555:  80% 196/246 [02:03<00:16,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7555:  80% 197/246 [02:03<00:16,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9656:  80% 197/246 [02:03<00:16,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9656:  80% 198/246 [02:04<00:16,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8062:  80% 198/246 [02:04<00:16,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8062:  81% 199/246 [02:04<00:15,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7381:  81% 199/246 [02:04<00:15,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7381:  81% 200/246 [02:04<00:15,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8026:  81% 200/246 [02:04<00:15,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8026:  82% 201/246 [02:05<00:15,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8719:  82% 201/246 [02:05<00:15,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8719:  82% 202/246 [02:05<00:14,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9942:  82% 202/246 [02:05<00:14,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9942:  83% 203/246 [02:05<00:14,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1503:  83% 203/246 [02:05<00:14,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1503:  83% 204/246 [02:06<00:14,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9735:  83% 204/246 [02:06<00:14,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9735:  83% 205/246 [02:06<00:13,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7831:  83% 205/246 [02:06<00:13,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7831:  84% 206/246 [02:06<00:13,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8994:  84% 206/246 [02:06<00:13,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8994:  84% 207/246 [02:07<00:13,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9250:  84% 207/246 [02:07<00:13,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9250:  85% 208/246 [02:07<00:12,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9978:  85% 208/246 [02:07<00:12,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9978:  85% 209/246 [02:07<00:12,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9251:  85% 209/246 [02:07<00:12,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9251:  85% 210/246 [02:08<00:12,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9134:  85% 210/246 [02:08<00:12,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9134:  86% 211/246 [02:08<00:11,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7421:  86% 211/246 [02:08<00:11,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7421:  86% 212/246 [02:08<00:11,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8246:  86% 212/246 [02:08<00:11,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8246:  87% 213/246 [02:09<00:11,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9133:  87% 213/246 [02:09<00:11,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9133:  87% 214/246 [02:09<00:10,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6906:  87% 214/246 [02:09<00:10,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6906:  87% 215/246 [02:09<00:10,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9676:  87% 215/246 [02:10<00:10,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9676:  88% 216/246 [02:10<00:10,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7333:  88% 216/246 [02:10<00:10,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7333:  88% 217/246 [02:10<00:09,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9089:  88% 217/246 [02:10<00:09,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9089:  89% 218/246 [02:10<00:09,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6689:  89% 218/246 [02:11<00:09,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6689:  89% 219/246 [02:11<00:09,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8353:  89% 219/246 [02:11<00:09,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8353:  89% 220/246 [02:11<00:08,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7427:  89% 220/246 [02:11<00:08,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7427:  90% 221/246 [02:11<00:08,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0685:  90% 221/246 [02:12<00:08,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0685:  90% 222/246 [02:12<00:08,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7959:  90% 222/246 [02:12<00:08,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7959:  91% 223/246 [02:12<00:07,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9386:  91% 223/246 [02:12<00:07,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9386:  91% 224/246 [02:12<00:07,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1093:  91% 224/246 [02:13<00:07,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1093:  91% 225/246 [02:13<00:07,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9853:  91% 225/246 [02:13<00:07,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9853:  92% 226/246 [02:13<00:06,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0866:  92% 226/246 [02:13<00:06,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0866:  92% 227/246 [02:13<00:06,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8111:  92% 227/246 [02:14<00:06,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8111:  93% 228/246 [02:14<00:06,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7515:  93% 228/246 [02:14<00:06,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7515:  93% 229/246 [02:14<00:05,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9630:  93% 229/246 [02:14<00:05,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9630:  93% 230/246 [02:14<00:05,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7733:  93% 230/246 [02:15<00:05,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7733:  94% 231/246 [02:15<00:05,  3.00it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6782:  94% 231/246 [02:15<00:05,  3.00it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6782:  94% 232/246 [02:15<00:04,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6338:  94% 232/246 [02:15<00:04,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6338:  95% 233/246 [02:15<00:04,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9054:  95% 233/246 [02:16<00:04,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9054:  95% 234/246 [02:16<00:04,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9162:  95% 234/246 [02:16<00:04,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9162:  96% 235/246 [02:16<00:03,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0119:  96% 235/246 [02:16<00:03,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0119:  96% 236/246 [02:16<00:03,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8722:  96% 236/246 [02:17<00:03,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8722:  96% 237/246 [02:17<00:03,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7554:  96% 237/246 [02:17<00:03,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7554:  97% 238/246 [02:17<00:02,  3.00it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8390:  97% 238/246 [02:17<00:02,  3.00it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8390:  97% 239/246 [02:17<00:02,  3.00it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8580:  97% 239/246 [02:18<00:02,  3.00it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8580:  98% 240/246 [02:18<00:02,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7688:  98% 240/246 [02:18<00:02,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7688:  98% 241/246 [02:18<00:01,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0987:  98% 241/246 [02:18<00:01,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0987:  98% 242/246 [02:18<00:01,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.3410:  98% 242/246 [02:19<00:01,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.3410:  99% 243/246 [02:19<00:01,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6686:  99% 243/246 [02:19<00:01,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6686:  99% 244/246 [02:19<00:00,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7405:  99% 244/246 [02:19<00:00,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7405: 100% 245/246 [02:19<00:00,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7762: 100% 245/246 [02:20<00:00,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7762: 100% 246/246 [02:20<00:00,  1.76it/s]\n","\n","  0% 0/600 [00:00<?, ?it/s]\u001b[A\n","  0% 1/600 [00:01<14:01,  1.41s/it]\u001b[A\n","  0% 2/600 [00:01<08:21,  1.19it/s]\n","Epoch 2 of 2: 100% 2/2 [06:04<00:00, 182.14s/it]\n","Fine-tuning\tbert\tbert-base-multilingual-cased\tv2\n","Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  0% 8/3927 [00:07<59:33,  1.10it/s]\n","Epoch 1 of 2:   0% 0/2 [00:00<?, ?it/s]\n","Running Epoch 0 of 2:   0% 0/246 [00:00<?, ?it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1658:   0% 0/246 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n","\n","Epochs 0/2. Running Loss:    1.1658:   0% 1/246 [00:00<01:19,  3.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1771:   0% 1/246 [00:00<01:19,  3.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1771:   1% 2/246 [00:00<01:10,  3.47it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2102:   1% 2/246 [00:00<01:10,  3.47it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2102:   1% 3/246 [00:00<01:10,  3.43it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1216:   1% 3/246 [00:01<01:10,  3.43it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1216:   2% 4/246 [00:01<01:12,  3.35it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1746:   2% 4/246 [00:01<01:12,  3.35it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1746:   2% 5/246 [00:01<01:14,  3.23it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1948:   2% 5/246 [00:01<01:14,  3.23it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1948:   2% 6/246 [00:01<01:16,  3.15it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1983:   2% 6/246 [00:02<01:16,  3.15it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1983:   3% 7/246 [00:02<01:16,  3.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1856:   3% 7/246 [00:02<01:16,  3.12it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1856:   3% 8/246 [00:02<01:13,  3.24it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1905:   3% 8/246 [00:02<01:13,  3.24it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1905:   4% 9/246 [00:02<01:14,  3.18it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1604:   4% 9/246 [00:02<01:14,  3.18it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1604:   4% 10/246 [00:03<01:15,  3.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1046:   4% 10/246 [00:03<01:15,  3.13it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1046:   4% 11/246 [00:03<01:16,  3.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1879:   4% 11/246 [00:03<01:16,  3.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1879:   5% 12/246 [00:03<01:16,  3.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1042:   5% 12/246 [00:03<01:16,  3.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1042:   5% 13/246 [00:04<01:16,  3.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1333:   5% 13/246 [00:04<01:16,  3.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1333:   6% 14/246 [00:04<01:16,  3.02it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1893:   6% 14/246 [00:04<01:16,  3.02it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1893:   6% 15/246 [00:04<01:16,  3.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1438:   6% 15/246 [00:04<01:16,  3.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1438:   7% 16/246 [00:05<01:16,  3.02it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1363:   7% 16/246 [00:05<01:16,  3.02it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1363:   7% 17/246 [00:05<01:15,  3.02it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1873:   7% 17/246 [00:05<01:15,  3.02it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1873:   7% 18/246 [00:05<01:15,  3.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1067:   7% 18/246 [00:05<01:15,  3.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1067:   8% 19/246 [00:06<01:15,  3.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1344:   8% 19/246 [00:06<01:15,  3.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1344:   8% 20/246 [00:06<01:15,  2.99it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1503:   8% 20/246 [00:06<01:15,  2.99it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1503:   9% 21/246 [00:06<01:15,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1409:   9% 21/246 [00:06<01:15,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1409:   9% 22/246 [00:07<01:14,  2.99it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0926:   9% 22/246 [00:07<01:14,  2.99it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0926:   9% 23/246 [00:07<01:14,  2.99it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0856:   9% 23/246 [00:07<01:14,  2.99it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0856:  10% 24/246 [00:07<01:14,  3.00it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1130:  10% 24/246 [00:07<01:14,  3.00it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1130:  10% 25/246 [00:08<01:14,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0583:  10% 25/246 [00:08<01:14,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0583:  11% 26/246 [00:08<01:14,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0760:  11% 26/246 [00:08<01:14,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0760:  11% 27/246 [00:08<01:14,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0618:  11% 27/246 [00:08<01:14,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0618:  11% 28/246 [00:09<01:13,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0275:  11% 28/246 [00:09<01:13,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0275:  12% 29/246 [00:09<01:13,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1245:  12% 29/246 [00:09<01:13,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1245:  12% 30/246 [00:09<01:14,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9333:  12% 30/246 [00:10<01:14,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9333:  13% 31/246 [00:10<01:13,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0134:  13% 31/246 [00:10<01:13,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0134:  13% 32/246 [00:10<01:13,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9995:  13% 32/246 [00:10<01:13,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9995:  13% 33/246 [00:10<01:12,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0461:  13% 33/246 [00:11<01:12,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0461:  14% 34/246 [00:11<01:11,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9486:  14% 34/246 [00:11<01:11,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9486:  14% 35/246 [00:11<01:11,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9257:  14% 35/246 [00:11<01:11,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9257:  15% 36/246 [00:11<01:10,  2.99it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0265:  15% 36/246 [00:12<01:10,  2.99it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0265:  15% 37/246 [00:12<01:09,  3.00it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9946:  15% 37/246 [00:12<01:09,  3.00it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9946:  15% 38/246 [00:12<01:09,  3.00it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0266:  15% 38/246 [00:12<01:09,  3.00it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0266:  16% 39/246 [00:12<01:08,  3.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8912:  16% 39/246 [00:13<01:08,  3.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8912:  16% 40/246 [00:13<01:08,  3.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9152:  16% 40/246 [00:13<01:08,  3.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9152:  17% 41/246 [00:13<01:08,  3.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9496:  17% 41/246 [00:13<01:08,  3.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9496:  17% 42/246 [00:13<01:07,  3.02it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1060:  17% 42/246 [00:13<01:07,  3.02it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1060:  17% 43/246 [00:14<01:04,  3.15it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2146:  17% 43/246 [00:14<01:04,  3.15it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2146:  18% 44/246 [00:14<01:05,  3.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0471:  18% 44/246 [00:14<01:05,  3.10it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0471:  18% 45/246 [00:14<01:05,  3.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9837:  18% 45/246 [00:14<01:05,  3.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9837:  19% 46/246 [00:15<01:05,  3.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9699:  19% 46/246 [00:15<01:05,  3.04it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9699:  19% 47/246 [00:15<01:05,  3.03it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9273:  19% 47/246 [00:15<01:05,  3.03it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9273:  20% 48/246 [00:15<01:05,  3.02it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8299:  20% 48/246 [00:15<01:05,  3.02it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8299:  20% 49/246 [00:16<01:05,  3.02it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8797:  20% 49/246 [00:16<01:05,  3.02it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8797:  20% 50/246 [00:16<01:05,  3.00it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8679:  20% 50/246 [00:16<01:05,  3.00it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8679:  21% 51/246 [00:16<01:04,  3.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8158:  21% 51/246 [00:16<01:04,  3.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8158:  21% 52/246 [00:17<01:04,  3.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1158:  21% 52/246 [00:17<01:04,  3.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1158:  22% 53/246 [00:17<01:04,  3.00it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2392:  22% 53/246 [00:17<01:04,  3.00it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2392:  22% 54/246 [00:17<01:04,  3.00it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9641:  22% 54/246 [00:17<01:04,  3.00it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9641:  22% 55/246 [00:18<01:03,  3.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0902:  22% 55/246 [00:18<01:03,  3.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0902:  23% 56/246 [00:18<01:04,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0612:  23% 56/246 [00:18<01:04,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0612:  23% 57/246 [00:18<01:04,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9947:  23% 57/246 [00:19<01:04,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9947:  24% 58/246 [00:19<01:03,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8422:  24% 58/246 [00:19<01:03,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8422:  24% 59/246 [00:19<01:03,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9385:  24% 59/246 [00:19<01:03,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9385:  24% 60/246 [00:19<01:02,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9865:  24% 60/246 [00:20<01:02,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9865:  25% 61/246 [00:20<01:02,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0191:  25% 61/246 [00:20<01:02,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0191:  25% 62/246 [00:20<01:01,  3.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0112:  25% 62/246 [00:20<01:01,  3.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0112:  26% 63/246 [00:20<01:00,  3.00it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8493:  26% 63/246 [00:21<01:00,  3.00it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8493:  26% 64/246 [00:21<01:01,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0402:  26% 64/246 [00:21<01:01,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0402:  26% 65/246 [00:21<01:00,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8974:  26% 65/246 [00:21<01:00,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8974:  27% 66/246 [00:21<01:01,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9597:  27% 66/246 [00:22<01:01,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9597:  27% 67/246 [00:22<01:00,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8444:  27% 67/246 [00:22<01:00,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8444:  28% 68/246 [00:22<01:02,  2.83it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9784:  28% 68/246 [00:22<01:02,  2.83it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9784:  28% 69/246 [00:22<01:00,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0696:  28% 69/246 [00:23<01:00,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0696:  28% 70/246 [00:23<00:59,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8586:  28% 70/246 [00:23<00:59,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8586:  29% 71/246 [00:23<00:58,  2.99it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0829:  29% 71/246 [00:23<00:58,  2.99it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0829:  29% 72/246 [00:23<00:56,  3.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9703:  29% 72/246 [00:24<00:56,  3.08it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9703:  30% 73/246 [00:24<00:56,  3.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8602:  30% 73/246 [00:24<00:56,  3.06it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8602:  30% 74/246 [00:24<00:56,  3.03it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8805:  30% 74/246 [00:24<00:56,  3.03it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8805:  30% 75/246 [00:24<00:56,  3.02it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8896:  30% 75/246 [00:25<00:56,  3.02it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8896:  31% 76/246 [00:25<00:56,  3.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9593:  31% 76/246 [00:25<00:56,  3.01it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9593:  31% 77/246 [00:25<00:56,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0241:  31% 77/246 [00:25<00:56,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0241:  32% 78/246 [00:25<00:56,  2.99it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9843:  32% 78/246 [00:26<00:56,  2.99it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9843:  32% 79/246 [00:26<00:56,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0311:  32% 79/246 [00:26<00:56,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0311:  33% 80/246 [00:26<00:56,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7795:  33% 80/246 [00:26<00:56,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7795:  33% 81/246 [00:26<00:56,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8805:  33% 81/246 [00:27<00:56,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8805:  33% 82/246 [00:27<00:55,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9657:  33% 82/246 [00:27<00:55,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9657:  34% 83/246 [00:27<00:55,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9380:  34% 83/246 [00:27<00:55,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9380:  34% 84/246 [00:27<00:55,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9092:  34% 84/246 [00:28<00:55,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9092:  35% 85/246 [00:28<00:54,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7756:  35% 85/246 [00:28<00:54,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7756:  35% 86/246 [00:28<00:54,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9930:  35% 86/246 [00:28<00:54,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9930:  35% 87/246 [00:28<00:53,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0838:  35% 87/246 [00:29<00:53,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0838:  36% 88/246 [00:29<00:53,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8206:  36% 88/246 [00:29<00:53,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8206:  36% 89/246 [00:29<00:53,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7945:  36% 89/246 [00:29<00:53,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7945:  37% 90/246 [00:29<00:52,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8274:  37% 90/246 [00:30<00:52,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8274:  37% 91/246 [00:30<00:51,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0381:  37% 91/246 [00:30<00:51,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0381:  37% 92/246 [00:30<00:51,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8567:  37% 92/246 [00:30<00:51,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8567:  38% 93/246 [00:30<00:51,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8881:  38% 93/246 [00:31<00:51,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8881:  38% 94/246 [00:31<00:50,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9332:  38% 94/246 [00:31<00:50,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9332:  39% 95/246 [00:31<00:50,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8710:  39% 95/246 [00:31<00:50,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8710:  39% 96/246 [00:31<00:50,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9951:  39% 96/246 [00:32<00:50,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9951:  39% 97/246 [00:32<00:50,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9038:  39% 97/246 [00:32<00:50,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9038:  40% 98/246 [00:32<00:50,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8963:  40% 98/246 [00:32<00:50,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8963:  40% 99/246 [00:33<00:49,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7731:  40% 99/246 [00:33<00:49,  2.96it/s]\u001b[A\n","\n","  0% 0/600 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","  0% 1/600 [00:00<09:22,  1.07it/s]\u001b[A\u001b[A\n","\n","  0% 2/600 [00:01<05:34,  1.79it/s]\n","\n","Epochs 0/2. Running Loss:    0.7731:  41% 100/246 [00:59<19:52,  8.17s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0984:  41% 100/246 [00:59<19:52,  8.17s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0984:  41% 101/246 [00:59<14:04,  5.82s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9147:  41% 101/246 [00:59<14:04,  5.82s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9147:  41% 102/246 [01:00<10:01,  4.18s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9059:  41% 102/246 [01:00<10:01,  4.18s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9059:  42% 103/246 [01:00<07:12,  3.02s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9700:  42% 103/246 [01:00<07:12,  3.02s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9700:  42% 104/246 [01:00<05:14,  2.22s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9293:  42% 104/246 [01:00<05:14,  2.22s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9293:  43% 105/246 [01:01<03:52,  1.65s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8746:  43% 105/246 [01:01<03:52,  1.65s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8746:  43% 106/246 [01:01<02:55,  1.26s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0176:  43% 106/246 [01:01<02:55,  1.26s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.0176:  43% 107/246 [01:01<02:16,  1.02it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8940:  43% 107/246 [01:01<02:16,  1.02it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8940:  44% 108/246 [01:02<01:48,  1.27it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.4186:  44% 108/246 [01:02<01:48,  1.27it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.4186:  44% 109/246 [01:02<01:29,  1.53it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9638:  44% 109/246 [01:02<01:29,  1.53it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9638:  45% 110/246 [01:02<01:16,  1.77it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.6499:  45% 110/246 [01:02<01:16,  1.77it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.6499:  45% 111/246 [01:03<01:07,  2.00it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0468:  45% 111/246 [01:03<01:07,  2.00it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0468:  46% 112/246 [01:03<01:00,  2.22it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8073:  46% 112/246 [01:03<01:00,  2.22it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8073:  46% 113/246 [01:03<00:55,  2.42it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9785:  46% 113/246 [01:03<00:55,  2.42it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9785:  46% 114/246 [01:04<00:51,  2.54it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9724:  46% 114/246 [01:04<00:51,  2.54it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9724:  47% 115/246 [01:04<00:48,  2.67it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0990:  47% 115/246 [01:04<00:48,  2.67it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0990:  47% 116/246 [01:04<00:47,  2.76it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2266:  47% 116/246 [01:05<00:47,  2.76it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.2266:  48% 117/246 [01:05<00:45,  2.82it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8176:  48% 117/246 [01:05<00:45,  2.82it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8176:  48% 118/246 [01:05<00:44,  2.89it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9904:  48% 118/246 [01:05<00:44,  2.89it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9904:  48% 119/246 [01:05<00:43,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9382:  48% 119/246 [01:06<00:43,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9382:  49% 120/246 [01:06<00:42,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9799:  49% 120/246 [01:06<00:42,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9799:  49% 121/246 [01:06<00:42,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1476:  49% 121/246 [01:06<00:42,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1476:  50% 122/246 [01:06<00:41,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8339:  50% 122/246 [01:07<00:41,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8339:  50% 123/246 [01:07<00:41,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9933:  50% 123/246 [01:07<00:41,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9933:  50% 124/246 [01:07<00:41,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8099:  50% 124/246 [01:07<00:41,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8099:  51% 125/246 [01:07<00:41,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9441:  51% 125/246 [01:08<00:41,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9441:  51% 126/246 [01:08<00:40,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0849:  51% 126/246 [01:08<00:40,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0849:  52% 127/246 [01:08<00:40,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9821:  52% 127/246 [01:08<00:40,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9821:  52% 128/246 [01:08<00:39,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8777:  52% 128/246 [01:09<00:39,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8777:  52% 129/246 [01:09<00:39,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9048:  52% 129/246 [01:09<00:39,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9048:  53% 130/246 [01:09<00:38,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8979:  53% 130/246 [01:09<00:38,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8979:  53% 131/246 [01:09<00:39,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8978:  53% 131/246 [01:10<00:39,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8978:  54% 132/246 [01:10<00:38,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8027:  54% 132/246 [01:10<00:38,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8027:  54% 133/246 [01:10<00:38,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0908:  54% 133/246 [01:10<00:38,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0908:  54% 134/246 [01:10<00:37,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7082:  54% 134/246 [01:11<00:37,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7082:  55% 135/246 [01:11<00:37,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7560:  55% 135/246 [01:11<00:37,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7560:  55% 136/246 [01:11<00:37,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0064:  55% 136/246 [01:11<00:37,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0064:  56% 137/246 [01:11<00:37,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9028:  56% 137/246 [01:12<00:37,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9028:  56% 138/246 [01:12<00:36,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7389:  56% 138/246 [01:12<00:36,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7389:  57% 139/246 [01:12<00:36,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7807:  57% 139/246 [01:12<00:36,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7807:  57% 140/246 [01:12<00:36,  2.89it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8429:  57% 140/246 [01:13<00:36,  2.89it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8429:  57% 141/246 [01:13<00:35,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0607:  57% 141/246 [01:13<00:35,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0607:  58% 142/246 [01:13<00:36,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1443:  58% 142/246 [01:13<00:36,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1443:  58% 143/246 [01:14<00:35,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9641:  58% 143/246 [01:14<00:35,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9641:  59% 144/246 [01:14<00:34,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9110:  59% 144/246 [01:14<00:34,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9110:  59% 145/246 [01:14<00:34,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1622:  59% 145/246 [01:14<00:34,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1622:  59% 146/246 [01:15<00:34,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8443:  59% 146/246 [01:15<00:34,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8443:  60% 147/246 [01:15<00:34,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9874:  60% 147/246 [01:15<00:34,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9874:  60% 148/246 [01:15<00:33,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7801:  60% 148/246 [01:15<00:33,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7801:  61% 149/246 [01:16<00:33,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8398:  61% 149/246 [01:16<00:33,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8398:  61% 150/246 [01:16<00:36,  2.61it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0928:  61% 150/246 [01:16<00:36,  2.61it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0928:  61% 151/246 [01:16<00:35,  2.67it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7522:  61% 151/246 [01:17<00:35,  2.67it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7522:  62% 152/246 [01:17<00:35,  2.66it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9417:  62% 152/246 [01:17<00:35,  2.66it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9417:  62% 153/246 [01:17<00:34,  2.66it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0148:  62% 153/246 [01:17<00:34,  2.66it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0148:  63% 154/246 [01:17<00:32,  2.83it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8447:  63% 154/246 [01:18<00:32,  2.83it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8447:  63% 155/246 [01:18<00:31,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0372:  63% 155/246 [01:18<00:31,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0372:  63% 156/246 [01:18<00:31,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0995:  63% 156/246 [01:18<00:31,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0995:  64% 157/246 [01:19<00:32,  2.70it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9854:  64% 157/246 [01:19<00:32,  2.70it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9854:  64% 158/246 [01:19<00:33,  2.60it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7546:  64% 158/246 [01:19<00:33,  2.60it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7546:  65% 159/246 [01:19<00:31,  2.74it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0261:  65% 159/246 [01:20<00:31,  2.74it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0261:  65% 160/246 [01:20<00:34,  2.52it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9411:  65% 160/246 [01:20<00:34,  2.52it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9411:  65% 161/246 [01:20<00:34,  2.47it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8351:  65% 161/246 [01:20<00:34,  2.47it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8351:  66% 162/246 [01:21<00:32,  2.58it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9541:  66% 162/246 [01:21<00:32,  2.58it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9541:  66% 163/246 [01:21<00:31,  2.66it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7970:  66% 163/246 [01:21<00:31,  2.66it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7970:  67% 164/246 [01:21<00:31,  2.63it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9478:  67% 164/246 [01:21<00:31,  2.63it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9478:  67% 165/246 [01:22<00:30,  2.67it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8752:  67% 165/246 [01:22<00:30,  2.67it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8752:  67% 166/246 [01:22<00:29,  2.73it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9194:  67% 166/246 [01:22<00:29,  2.73it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9194:  68% 167/246 [01:22<00:28,  2.79it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8679:  68% 167/246 [01:22<00:28,  2.79it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8679:  68% 168/246 [01:23<00:27,  2.80it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.3557:  68% 168/246 [01:23<00:27,  2.80it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.3557:  69% 169/246 [01:23<00:26,  2.85it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8271:  69% 169/246 [01:23<00:26,  2.85it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8271:  69% 170/246 [01:23<00:26,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0871:  69% 170/246 [01:24<00:26,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0871:  70% 171/246 [01:24<00:26,  2.84it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7328:  70% 171/246 [01:24<00:26,  2.84it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7328:  70% 172/246 [01:24<00:26,  2.81it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0033:  70% 172/246 [01:24<00:26,  2.81it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0033:  70% 173/246 [01:24<00:25,  2.82it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7873:  70% 173/246 [01:25<00:25,  2.82it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7873:  71% 174/246 [01:25<00:25,  2.81it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8091:  71% 174/246 [01:25<00:25,  2.81it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8091:  71% 175/246 [01:25<00:24,  2.84it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0184:  71% 175/246 [01:25<00:24,  2.84it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0184:  72% 176/246 [01:26<00:24,  2.81it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7332:  72% 176/246 [01:26<00:24,  2.81it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7332:  72% 177/246 [01:26<00:24,  2.83it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0197:  72% 177/246 [01:26<00:24,  2.83it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0197:  72% 178/246 [01:26<00:23,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8593:  72% 178/246 [01:26<00:23,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8593:  73% 179/246 [01:27<00:23,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8435:  73% 179/246 [01:27<00:23,  2.90it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8435:  73% 180/246 [01:27<00:22,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8450:  73% 180/246 [01:27<00:22,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8450:  74% 181/246 [01:27<00:22,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9256:  74% 181/246 [01:27<00:22,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9256:  74% 182/246 [01:28<00:21,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0459:  74% 182/246 [01:28<00:21,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0459:  74% 183/246 [01:28<00:21,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7027:  74% 183/246 [01:28<00:21,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7027:  75% 184/246 [01:28<00:20,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0859:  75% 184/246 [01:28<00:20,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0859:  75% 185/246 [01:29<00:20,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9086:  75% 185/246 [01:29<00:20,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9086:  76% 186/246 [01:29<00:20,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.6646:  76% 186/246 [01:29<00:20,  2.93it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.6646:  76% 187/246 [01:29<00:20,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8118:  76% 187/246 [01:29<00:20,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8118:  76% 188/246 [01:30<00:19,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8288:  76% 188/246 [01:30<00:19,  2.92it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8288:  77% 189/246 [01:30<00:19,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9534:  77% 189/246 [01:30<00:19,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9534:  77% 190/246 [01:30<00:18,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8106:  77% 190/246 [01:30<00:18,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8106:  78% 191/246 [01:31<00:18,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.6981:  78% 191/246 [01:31<00:18,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.6981:  78% 192/246 [01:31<00:18,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9121:  78% 192/246 [01:31<00:18,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9121:  78% 193/246 [01:31<00:17,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8088:  78% 193/246 [01:31<00:17,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8088:  79% 194/246 [01:32<00:17,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9559:  79% 194/246 [01:32<00:17,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9559:  79% 195/246 [01:32<00:17,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7756:  79% 195/246 [01:32<00:17,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7756:  80% 196/246 [01:32<00:16,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7213:  80% 196/246 [01:32<00:16,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7213:  80% 197/246 [01:33<00:16,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7279:  80% 197/246 [01:33<00:16,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7279:  80% 198/246 [01:33<00:16,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7358:  80% 198/246 [01:33<00:16,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7358:  81% 199/246 [01:33<00:15,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.1208:  81% 199/246 [01:33<00:15,  2.94it/s]\u001b[A\n","\n","  0% 0/600 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","  0% 1/600 [00:01<10:22,  1.04s/it]\u001b[A\u001b[A\n","\n","  0% 2/600 [00:01<05:57,  1.67it/s]\n","\n","Epochs 0/2. Running Loss:    1.1208:  81% 200/246 [02:03<06:55,  9.03s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9098:  81% 200/246 [02:03<06:55,  9.03s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9098:  82% 201/246 [02:03<04:50,  6.45s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.1774:  82% 201/246 [02:03<04:50,  6.45s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.1774:  82% 202/246 [02:03<03:23,  4.62s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.1532:  82% 202/246 [02:04<03:23,  4.62s/it]\u001b[A\n","Epochs 0/2. Running Loss:    1.1532:  83% 203/246 [02:04<02:23,  3.33s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9544:  83% 203/246 [02:04<02:23,  3.33s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.9544:  83% 204/246 [02:04<01:42,  2.44s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8074:  83% 204/246 [02:04<01:42,  2.44s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8074:  83% 205/246 [02:04<01:14,  1.81s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8122:  83% 205/246 [02:05<01:14,  1.81s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8122:  84% 206/246 [02:05<00:54,  1.37s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.7433:  84% 206/246 [02:05<00:54,  1.37s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.7433:  84% 207/246 [02:05<00:41,  1.05s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8860:  84% 207/246 [02:05<00:41,  1.05s/it]\u001b[A\n","Epochs 0/2. Running Loss:    0.8860:  85% 208/246 [02:05<00:31,  1.19it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9023:  85% 208/246 [02:06<00:31,  1.19it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9023:  85% 209/246 [02:06<00:25,  1.45it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8977:  85% 209/246 [02:06<00:25,  1.45it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8977:  85% 210/246 [02:06<00:20,  1.72it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9096:  85% 210/246 [02:06<00:20,  1.72it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9096:  86% 211/246 [02:06<00:17,  1.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0662:  86% 211/246 [02:07<00:17,  1.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0662:  86% 212/246 [02:07<00:15,  2.19it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8477:  86% 212/246 [02:07<00:15,  2.19it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8477:  87% 213/246 [02:07<00:14,  2.36it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9722:  87% 213/246 [02:07<00:14,  2.36it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9722:  87% 214/246 [02:07<00:12,  2.52it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8878:  87% 214/246 [02:08<00:12,  2.52it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8878:  87% 215/246 [02:08<00:11,  2.64it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8095:  87% 215/246 [02:08<00:11,  2.64it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8095:  88% 216/246 [02:08<00:10,  2.75it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9353:  88% 216/246 [02:08<00:10,  2.75it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9353:  88% 217/246 [02:08<00:10,  2.81it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.6865:  88% 217/246 [02:09<00:10,  2.81it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.6865:  89% 218/246 [02:09<00:09,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9370:  89% 218/246 [02:09<00:09,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9370:  89% 219/246 [02:09<00:09,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8764:  89% 219/246 [02:09<00:09,  2.87it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8764:  89% 220/246 [02:09<00:08,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9110:  89% 220/246 [02:10<00:08,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9110:  90% 221/246 [02:10<00:08,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.6364:  90% 221/246 [02:10<00:08,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.6364:  90% 222/246 [02:10<00:08,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8701:  90% 222/246 [02:10<00:08,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8701:  91% 223/246 [02:10<00:07,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7959:  91% 223/246 [02:11<00:07,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7959:  91% 224/246 [02:11<00:07,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7896:  91% 224/246 [02:11<00:07,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7896:  91% 225/246 [02:11<00:07,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8248:  91% 225/246 [02:11<00:07,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8248:  92% 226/246 [02:11<00:06,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7692:  92% 226/246 [02:12<00:06,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7692:  92% 227/246 [02:12<00:06,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9866:  92% 227/246 [02:12<00:06,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9866:  93% 228/246 [02:12<00:06,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7980:  93% 228/246 [02:12<00:06,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7980:  93% 229/246 [02:12<00:05,  2.99it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7641:  93% 229/246 [02:13<00:05,  2.99it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7641:  93% 230/246 [02:13<00:05,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9170:  93% 230/246 [02:13<00:05,  2.98it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9170:  94% 231/246 [02:13<00:05,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7404:  94% 231/246 [02:13<00:05,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7404:  94% 232/246 [02:13<00:04,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8835:  94% 232/246 [02:14<00:04,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8835:  95% 233/246 [02:14<00:04,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8901:  95% 233/246 [02:14<00:04,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8901:  95% 234/246 [02:14<00:04,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0082:  95% 234/246 [02:14<00:04,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    1.0082:  96% 235/246 [02:14<00:03,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8399:  96% 235/246 [02:15<00:03,  2.97it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8399:  96% 236/246 [02:15<00:03,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9498:  96% 236/246 [02:15<00:03,  2.94it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9498:  96% 237/246 [02:15<00:03,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9034:  96% 237/246 [02:15<00:03,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9034:  97% 238/246 [02:16<00:02,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9080:  97% 238/246 [02:16<00:02,  2.91it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9080:  97% 239/246 [02:16<00:02,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9951:  97% 239/246 [02:16<00:02,  2.96it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9951:  98% 240/246 [02:16<00:02,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.6808:  98% 240/246 [02:16<00:02,  2.95it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.6808:  98% 241/246 [02:17<00:01,  2.79it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7174:  98% 241/246 [02:17<00:01,  2.79it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7174:  98% 242/246 [02:17<00:01,  2.89it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9907:  98% 242/246 [02:17<00:01,  2.89it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9907:  99% 243/246 [02:17<00:01,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8450:  99% 243/246 [02:17<00:01,  2.88it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.8450:  99% 244/246 [02:18<00:00,  2.89it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9954:  99% 244/246 [02:18<00:00,  2.89it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.9954: 100% 245/246 [02:18<00:00,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7691: 100% 245/246 [02:18<00:00,  2.86it/s]\u001b[A\n","Epochs 0/2. Running Loss:    0.7691: 100% 246/246 [02:18<00:00,  1.77it/s]\n","\n","  0% 0/600 [00:00<?, ?it/s]\u001b[A\n","  0% 1/600 [00:01<12:27,  1.25s/it]\u001b[A\n","  0% 2/600 [00:01<06:47,  1.47it/s]\n","Epoch 2 of 2:  50% 1/2 [02:47<02:47, 167.48s/it]\n","Running Epoch 1 of 2:   0% 0/246 [00:00<?, ?it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6284:   0% 0/246 [00:00<?, ?it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6284:   0% 1/246 [00:00<01:27,  2.79it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0529:   0% 1/246 [00:00<01:27,  2.79it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0529:   1% 2/246 [00:00<01:23,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1126:   1% 2/246 [00:00<01:23,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1126:   1% 3/246 [00:01<01:22,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7956:   1% 3/246 [00:01<01:22,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7956:   2% 4/246 [00:01<01:21,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9523:   2% 4/246 [00:01<01:21,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9523:   2% 5/246 [00:01<01:20,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8723:   2% 5/246 [00:01<01:20,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8723:   2% 6/246 [00:02<01:20,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0162:   2% 6/246 [00:02<01:20,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0162:   3% 7/246 [00:02<01:20,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7835:   3% 7/246 [00:02<01:20,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7835:   3% 8/246 [00:02<01:19,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7005:   3% 8/246 [00:02<01:19,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7005:   4% 9/246 [00:03<01:19,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0449:   4% 9/246 [00:03<01:19,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0449:   4% 10/246 [00:03<01:19,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0102:   4% 10/246 [00:03<01:19,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0102:   4% 11/246 [00:03<01:18,  3.00it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8070:   4% 11/246 [00:03<01:18,  3.00it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8070:   5% 12/246 [00:04<01:18,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8474:   5% 12/246 [00:04<01:18,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8474:   5% 13/246 [00:04<01:18,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0506:   5% 13/246 [00:04<01:18,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0506:   6% 14/246 [00:04<01:18,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8643:   6% 14/246 [00:04<01:18,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8643:   6% 15/246 [00:05<01:17,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8170:   6% 15/246 [00:05<01:17,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8170:   7% 16/246 [00:05<01:19,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8923:   7% 16/246 [00:05<01:19,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8923:   7% 17/246 [00:05<01:18,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7779:   7% 17/246 [00:05<01:18,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7779:   7% 18/246 [00:06<01:16,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7910:   7% 18/246 [00:06<01:16,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7910:   8% 19/246 [00:06<01:16,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0678:   8% 19/246 [00:06<01:16,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0678:   8% 20/246 [00:06<01:16,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9451:   8% 20/246 [00:06<01:16,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9451:   9% 21/246 [00:07<01:15,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1109:   9% 21/246 [00:07<01:15,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1109:   9% 22/246 [00:07<01:16,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9965:   9% 22/246 [00:07<01:16,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9965:   9% 23/246 [00:07<01:16,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8270:   9% 23/246 [00:07<01:16,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8270:  10% 24/246 [00:08<01:16,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9875:  10% 24/246 [00:08<01:16,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9875:  10% 25/246 [00:08<01:16,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7967:  10% 25/246 [00:08<01:16,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7967:  11% 26/246 [00:08<01:15,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9491:  11% 26/246 [00:08<01:15,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9491:  11% 27/246 [00:09<01:15,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7653:  11% 27/246 [00:09<01:15,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7653:  11% 28/246 [00:09<01:15,  2.88it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6897:  11% 28/246 [00:09<01:15,  2.88it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6897:  12% 29/246 [00:09<01:13,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7747:  12% 29/246 [00:09<01:13,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7747:  12% 30/246 [00:10<01:14,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8943:  12% 30/246 [00:10<01:14,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8943:  13% 31/246 [00:10<01:13,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7246:  13% 31/246 [00:10<01:13,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7246:  13% 32/246 [00:10<01:12,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8018:  13% 32/246 [00:11<01:12,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8018:  13% 33/246 [00:11<01:12,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8926:  13% 33/246 [00:11<01:12,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8926:  14% 34/246 [00:11<01:12,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9391:  14% 34/246 [00:11<01:12,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9391:  14% 35/246 [00:11<01:11,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6957:  14% 35/246 [00:12<01:11,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6957:  15% 36/246 [00:12<01:12,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8185:  15% 36/246 [00:12<01:12,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8185:  15% 37/246 [00:12<01:11,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7850:  15% 37/246 [00:12<01:11,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7850:  15% 38/246 [00:12<01:10,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8718:  15% 38/246 [00:13<01:10,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8718:  16% 39/246 [00:13<01:09,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6403:  16% 39/246 [00:13<01:09,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6403:  16% 40/246 [00:13<01:10,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6597:  16% 40/246 [00:13<01:10,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6597:  17% 41/246 [00:13<01:09,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7878:  17% 41/246 [00:14<01:09,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7878:  17% 42/246 [00:14<01:09,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8703:  17% 42/246 [00:14<01:09,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8703:  17% 43/246 [00:14<01:08,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7709:  17% 43/246 [00:14<01:08,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7709:  18% 44/246 [00:14<01:08,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7222:  18% 44/246 [00:15<01:08,  2.96it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7222:  18% 45/246 [00:15<01:07,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6778:  18% 45/246 [00:15<01:07,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6778:  19% 46/246 [00:15<01:06,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9058:  19% 46/246 [00:15<01:06,  2.99it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9058:  19% 47/246 [00:15<01:06,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6965:  19% 47/246 [00:16<01:06,  2.98it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6965:  20% 48/246 [00:16<01:07,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5756:  20% 48/246 [00:16<01:07,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5756:  20% 49/246 [00:16<01:06,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9360:  20% 49/246 [00:16<01:06,  2.97it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9360:  20% 50/246 [00:16<01:06,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7863:  20% 50/246 [00:17<01:06,  2.94it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7863:  21% 51/246 [00:17<01:06,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8317:  21% 51/246 [00:17<01:06,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8317:  21% 52/246 [00:17<01:05,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8631:  21% 52/246 [00:17<01:05,  2.95it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8631:  22% 53/246 [00:18<01:05,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9446:  22% 53/246 [00:18<01:05,  2.93it/s]\u001b[A\n","\n","  0% 0/600 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","  0% 1/600 [00:01<10:36,  1.06s/it]\u001b[A\u001b[A\n","\n","  0% 2/600 [00:01<06:12,  1.60it/s]\n","\n","Epochs 1/2. Running Loss:    0.9446:  22% 54/246 [00:44<25:48,  8.07s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.0812:  22% 54/246 [00:44<25:48,  8.07s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.0812:  22% 55/246 [00:44<18:21,  5.76s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.0143:  22% 55/246 [00:44<18:21,  5.76s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.0143:  23% 56/246 [00:44<13:07,  4.14s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7778:  23% 56/246 [00:45<13:07,  4.14s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7778:  23% 57/246 [00:45<09:29,  3.01s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8400:  23% 57/246 [00:45<09:29,  3.01s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8400:  24% 58/246 [00:45<06:55,  2.21s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8504:  24% 58/246 [00:45<06:55,  2.21s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8504:  24% 59/246 [00:45<05:13,  1.68s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9705:  24% 59/246 [00:46<05:13,  1.68s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9705:  24% 60/246 [00:46<03:59,  1.29s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8304:  24% 60/246 [00:46<03:59,  1.29s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8304:  25% 61/246 [00:46<03:03,  1.01it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6649:  25% 61/246 [00:46<03:03,  1.01it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6649:  25% 62/246 [00:47<02:28,  1.24it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9008:  25% 62/246 [00:47<02:28,  1.24it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9008:  26% 63/246 [00:47<02:01,  1.51it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8531:  26% 63/246 [00:47<02:01,  1.51it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8531:  26% 64/246 [00:47<01:48,  1.67it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8645:  26% 64/246 [00:47<01:48,  1.67it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8645:  26% 65/246 [00:48<01:35,  1.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7372:  26% 65/246 [00:48<01:35,  1.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7372:  27% 66/246 [00:48<01:23,  2.15it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8137:  27% 66/246 [00:48<01:23,  2.15it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8137:  27% 67/246 [00:48<01:17,  2.30it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9359:  27% 67/246 [00:49<01:17,  2.30it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9359:  28% 68/246 [00:49<01:13,  2.43it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8335:  28% 68/246 [00:49<01:13,  2.43it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8335:  28% 69/246 [00:49<01:09,  2.54it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7102:  28% 69/246 [00:49<01:09,  2.54it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7102:  28% 70/246 [00:49<01:06,  2.63it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8539:  28% 70/246 [00:50<01:06,  2.63it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8539:  29% 71/246 [00:50<01:04,  2.70it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8483:  29% 71/246 [00:50<01:04,  2.70it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8483:  29% 72/246 [00:50<01:03,  2.75it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7192:  29% 72/246 [00:50<01:03,  2.75it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7192:  30% 73/246 [00:50<01:01,  2.81it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7052:  30% 73/246 [00:51<01:01,  2.81it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7052:  30% 74/246 [00:51<01:00,  2.84it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6272:  30% 74/246 [00:51<01:00,  2.84it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6272:  30% 75/246 [00:51<00:59,  2.87it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8307:  30% 75/246 [00:51<00:59,  2.87it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8307:  31% 76/246 [00:51<00:59,  2.84it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8563:  31% 76/246 [00:52<00:59,  2.84it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8563:  31% 77/246 [00:52<01:01,  2.77it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9531:  31% 77/246 [00:52<01:01,  2.77it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9531:  32% 78/246 [00:52<00:57,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7552:  32% 78/246 [00:52<00:57,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7552:  32% 79/246 [00:53<00:57,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8085:  32% 79/246 [00:53<00:57,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8085:  33% 80/246 [00:53<00:56,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1062:  33% 80/246 [00:53<00:56,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1062:  33% 81/246 [00:53<00:56,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8810:  33% 81/246 [00:53<00:56,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8810:  33% 82/246 [00:54<00:56,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7583:  33% 82/246 [00:54<00:56,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7583:  34% 83/246 [00:54<00:56,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0452:  34% 83/246 [00:54<00:56,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0452:  34% 84/246 [00:54<00:55,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9891:  34% 84/246 [00:54<00:55,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9891:  35% 85/246 [00:55<00:55,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8393:  35% 85/246 [00:55<00:55,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8393:  35% 86/246 [00:55<00:55,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1342:  35% 86/246 [00:55<00:55,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1342:  35% 87/246 [00:55<00:54,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8796:  35% 87/246 [00:55<00:54,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8796:  36% 88/246 [00:56<00:54,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9847:  36% 88/246 [00:56<00:54,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9847:  36% 89/246 [00:56<00:54,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7931:  36% 89/246 [00:56<00:54,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7931:  37% 90/246 [00:56<00:53,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9497:  37% 90/246 [00:56<00:53,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9497:  37% 91/246 [00:57<00:53,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7287:  37% 91/246 [00:57<00:53,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7287:  37% 92/246 [00:57<00:52,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6526:  37% 92/246 [00:57<00:52,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6526:  38% 93/246 [00:57<00:52,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7276:  38% 93/246 [00:57<00:52,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7276:  38% 94/246 [00:58<00:51,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9130:  38% 94/246 [00:58<00:51,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9130:  39% 95/246 [00:58<00:51,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9196:  39% 95/246 [00:58<00:51,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9196:  39% 96/246 [00:58<00:51,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6239:  39% 96/246 [00:59<00:51,  2.93it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6239:  39% 97/246 [00:59<00:51,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8455:  39% 97/246 [00:59<00:51,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8455:  40% 98/246 [00:59<00:50,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7024:  40% 98/246 [00:59<00:50,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7024:  40% 99/246 [00:59<00:50,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6953:  40% 99/246 [01:00<00:50,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6953:  41% 100/246 [01:00<00:49,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8531:  41% 100/246 [01:00<00:49,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8531:  41% 101/246 [01:00<00:49,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8431:  41% 101/246 [01:00<00:49,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8431:  41% 102/246 [01:00<00:49,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7153:  41% 102/246 [01:01<00:49,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7153:  42% 103/246 [01:01<00:48,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7629:  42% 103/246 [01:01<00:48,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7629:  42% 104/246 [01:01<00:48,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7113:  42% 104/246 [01:01<00:48,  2.92it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7113:  43% 105/246 [01:01<00:48,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6091:  43% 105/246 [01:02<00:48,  2.91it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6091:  43% 106/246 [01:02<00:48,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7151:  43% 106/246 [01:02<00:48,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7151:  43% 107/246 [01:02<00:47,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6829:  43% 107/246 [01:02<00:47,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6829:  44% 108/246 [01:02<00:47,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9300:  44% 108/246 [01:03<00:47,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9300:  44% 109/246 [01:03<00:47,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8336:  44% 109/246 [01:03<00:47,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8336:  45% 110/246 [01:03<00:47,  2.87it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8789:  45% 110/246 [01:03<00:47,  2.87it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8789:  45% 111/246 [01:04<00:48,  2.77it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8922:  45% 111/246 [01:04<00:48,  2.77it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8922:  46% 112/246 [01:04<00:48,  2.77it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9333:  46% 112/246 [01:04<00:48,  2.77it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9333:  46% 113/246 [01:04<00:46,  2.84it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6974:  46% 113/246 [01:04<00:46,  2.84it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6974:  46% 114/246 [01:05<00:46,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8971:  46% 114/246 [01:05<00:46,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8971:  47% 115/246 [01:05<00:46,  2.84it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9255:  47% 115/246 [01:05<00:46,  2.84it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9255:  47% 116/246 [01:05<00:45,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7208:  47% 116/246 [01:05<00:45,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7208:  48% 117/246 [01:06<00:45,  2.84it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8201:  48% 117/246 [01:06<00:45,  2.84it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8201:  48% 118/246 [01:06<00:45,  2.79it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8008:  48% 118/246 [01:06<00:45,  2.79it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8008:  48% 119/246 [01:06<00:44,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8461:  48% 119/246 [01:07<00:44,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8461:  49% 120/246 [01:07<00:43,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8147:  49% 120/246 [01:07<00:43,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8147:  49% 121/246 [01:07<00:44,  2.79it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8394:  49% 121/246 [01:07<00:44,  2.79it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8394:  50% 122/246 [01:07<00:44,  2.77it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7369:  50% 122/246 [01:08<00:44,  2.77it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7369:  50% 123/246 [01:08<00:42,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6178:  50% 123/246 [01:08<00:42,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6178:  50% 124/246 [01:08<00:42,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9298:  50% 124/246 [01:08<00:42,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9298:  51% 125/246 [01:09<00:42,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7861:  51% 125/246 [01:09<00:42,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7861:  51% 126/246 [01:09<00:42,  2.83it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7499:  51% 126/246 [01:09<00:42,  2.83it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7499:  52% 127/246 [01:09<00:41,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6818:  52% 127/246 [01:09<00:41,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6818:  52% 128/246 [01:10<00:41,  2.85it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8325:  52% 128/246 [01:10<00:41,  2.85it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8325:  52% 129/246 [01:10<00:40,  2.87it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9602:  52% 129/246 [01:10<00:40,  2.87it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9602:  53% 130/246 [01:10<00:40,  2.88it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9530:  53% 130/246 [01:10<00:40,  2.88it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9530:  53% 131/246 [01:11<00:39,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9575:  53% 131/246 [01:11<00:39,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9575:  54% 132/246 [01:11<00:39,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0049:  54% 132/246 [01:11<00:39,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0049:  54% 133/246 [01:11<00:39,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7227:  54% 133/246 [01:11<00:39,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7227:  54% 134/246 [01:12<00:38,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6965:  54% 134/246 [01:12<00:38,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6965:  55% 135/246 [01:12<00:38,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6453:  55% 135/246 [01:12<00:38,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6453:  55% 136/246 [01:12<00:38,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8087:  55% 136/246 [01:12<00:38,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8087:  56% 137/246 [01:13<00:37,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6746:  56% 137/246 [01:13<00:37,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6746:  56% 138/246 [01:13<00:37,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7166:  56% 138/246 [01:13<00:37,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7166:  57% 139/246 [01:13<00:36,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7423:  57% 139/246 [01:14<00:36,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7423:  57% 140/246 [01:14<00:36,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8029:  57% 140/246 [01:14<00:36,  2.90it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8029:  57% 141/246 [01:14<00:36,  2.88it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8890:  57% 141/246 [01:14<00:36,  2.88it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8890:  58% 142/246 [01:14<00:36,  2.88it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8864:  58% 142/246 [01:15<00:36,  2.88it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8864:  58% 143/246 [01:15<00:35,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7219:  58% 143/246 [01:15<00:35,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7219:  59% 144/246 [01:15<00:35,  2.87it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8072:  59% 144/246 [01:15<00:35,  2.87it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8072:  59% 145/246 [01:15<00:35,  2.85it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8026:  59% 145/246 [01:16<00:35,  2.85it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8026:  59% 146/246 [01:16<00:34,  2.87it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6604:  59% 146/246 [01:16<00:34,  2.87it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6604:  60% 147/246 [01:16<00:34,  2.88it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6915:  60% 147/246 [01:16<00:34,  2.88it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6915:  60% 148/246 [01:16<00:34,  2.87it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1303:  60% 148/246 [01:17<00:34,  2.87it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1303:  61% 149/246 [01:17<00:33,  2.88it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7815:  61% 149/246 [01:17<00:33,  2.88it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7815:  61% 150/246 [01:17<00:33,  2.87it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8488:  61% 150/246 [01:17<00:33,  2.87it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8488:  61% 151/246 [01:18<00:33,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7499:  61% 151/246 [01:18<00:33,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7499:  62% 152/246 [01:18<00:32,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7719:  62% 152/246 [01:18<00:32,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7719:  62% 153/246 [01:18<00:32,  2.86it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8302:  62% 153/246 [01:18<00:32,  2.86it/s]\u001b[A\n","\n","  0% 0/600 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","  0% 2/600 [00:01<07:55,  1.26it/s]\n","\n","Epochs 1/2. Running Loss:    0.8302:  63% 154/246 [01:35<08:08,  5.31s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9545:  63% 154/246 [01:35<08:08,  5.31s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.9545:  63% 155/246 [01:35<05:47,  3.82s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.0196:  63% 155/246 [01:36<05:47,  3.82s/it]\u001b[A\n","Epochs 1/2. Running Loss:    1.0196:  63% 156/246 [01:36<04:11,  2.79s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8153:  63% 156/246 [01:36<04:11,  2.79s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.8153:  64% 157/246 [01:36<03:02,  2.05s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.6997:  64% 157/246 [01:36<03:02,  2.05s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.6997:  64% 158/246 [01:37<02:15,  1.54s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7448:  64% 158/246 [01:37<02:15,  1.54s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7448:  65% 159/246 [01:37<01:42,  1.18s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7579:  65% 159/246 [01:37<01:42,  1.18s/it]\u001b[A\n","Epochs 1/2. Running Loss:    0.7579:  65% 160/246 [01:37<01:19,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1625:  65% 160/246 [01:37<01:19,  1.08it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.1625:  65% 161/246 [01:38<01:04,  1.33it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6597:  65% 161/246 [01:38<01:04,  1.33it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6597:  66% 162/246 [01:38<00:53,  1.58it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6857:  66% 162/246 [01:38<00:53,  1.58it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6857:  66% 163/246 [01:38<00:45,  1.83it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6837:  66% 163/246 [01:38<00:45,  1.83it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6837:  67% 164/246 [01:39<00:40,  2.02it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5587:  67% 164/246 [01:39<00:40,  2.02it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5587:  67% 165/246 [01:39<00:37,  2.14it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7351:  67% 165/246 [01:39<00:37,  2.14it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7351:  67% 166/246 [01:39<00:34,  2.31it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8518:  67% 166/246 [01:40<00:34,  2.31it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8518:  68% 167/246 [01:40<00:32,  2.43it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7195:  68% 167/246 [01:40<00:32,  2.43it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7195:  68% 168/246 [01:40<00:31,  2.49it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8472:  68% 168/246 [01:40<00:31,  2.49it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8472:  69% 169/246 [01:40<00:30,  2.55it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6763:  69% 169/246 [01:41<00:30,  2.55it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6763:  69% 170/246 [01:41<00:29,  2.59it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7923:  69% 170/246 [01:41<00:29,  2.59it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7923:  70% 171/246 [01:41<00:29,  2.57it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7272:  70% 171/246 [01:41<00:29,  2.57it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7272:  70% 172/246 [01:42<00:30,  2.40it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7108:  70% 172/246 [01:42<00:30,  2.40it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7108:  70% 173/246 [01:42<00:28,  2.53it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7106:  70% 173/246 [01:42<00:28,  2.53it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7106:  71% 174/246 [01:42<00:28,  2.56it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6171:  71% 174/246 [01:43<00:28,  2.56it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6171:  71% 175/246 [01:43<00:26,  2.63it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8107:  71% 175/246 [01:43<00:26,  2.63it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8107:  72% 176/246 [01:43<00:27,  2.58it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8245:  72% 176/246 [01:43<00:27,  2.58it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8245:  72% 177/246 [01:44<00:25,  2.66it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7036:  72% 177/246 [01:44<00:25,  2.66it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7036:  72% 178/246 [01:44<00:25,  2.62it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6767:  72% 178/246 [01:44<00:25,  2.62it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6767:  73% 179/246 [01:44<00:24,  2.70it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8418:  73% 179/246 [01:44<00:24,  2.70it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8418:  73% 180/246 [01:45<00:24,  2.70it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7946:  73% 180/246 [01:45<00:24,  2.70it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7946:  74% 181/246 [01:45<00:24,  2.65it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9392:  74% 181/246 [01:45<00:24,  2.65it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9392:  74% 182/246 [01:45<00:23,  2.69it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9934:  74% 182/246 [01:46<00:23,  2.69it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9934:  74% 183/246 [01:46<00:23,  2.68it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8132:  74% 183/246 [01:46<00:23,  2.68it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8132:  75% 184/246 [01:46<00:23,  2.66it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7471:  75% 184/246 [01:46<00:23,  2.66it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7471:  75% 185/246 [01:47<00:23,  2.60it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6307:  75% 185/246 [01:47<00:23,  2.60it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6307:  76% 186/246 [01:47<00:23,  2.61it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9377:  76% 186/246 [01:47<00:23,  2.61it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9377:  76% 187/246 [01:47<00:21,  2.71it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9508:  76% 187/246 [01:47<00:21,  2.71it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9508:  76% 188/246 [01:48<00:21,  2.71it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6543:  76% 188/246 [01:48<00:21,  2.71it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6543:  77% 189/246 [01:48<00:20,  2.73it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7086:  77% 189/246 [01:48<00:20,  2.73it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7086:  77% 190/246 [01:48<00:20,  2.76it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6777:  77% 190/246 [01:49<00:20,  2.76it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6777:  78% 191/246 [01:49<00:19,  2.81it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9181:  78% 191/246 [01:49<00:19,  2.81it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9181:  78% 192/246 [01:49<00:19,  2.81it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0351:  78% 192/246 [01:49<00:19,  2.81it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0351:  78% 193/246 [01:49<00:18,  2.82it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7638:  78% 193/246 [01:50<00:18,  2.82it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7638:  79% 194/246 [01:50<00:18,  2.78it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7310:  79% 194/246 [01:50<00:18,  2.78it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7310:  79% 195/246 [01:50<00:18,  2.81it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8753:  79% 195/246 [01:50<00:18,  2.81it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8753:  80% 196/246 [01:51<00:17,  2.82it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7755:  80% 196/246 [01:51<00:17,  2.82it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7755:  80% 197/246 [01:51<00:17,  2.79it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9168:  80% 197/246 [01:51<00:17,  2.79it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9168:  80% 198/246 [01:51<00:17,  2.81it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7095:  80% 198/246 [01:51<00:17,  2.81it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7095:  81% 199/246 [01:52<00:16,  2.81it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7055:  81% 199/246 [01:52<00:16,  2.81it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7055:  81% 200/246 [01:52<00:16,  2.82it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9982:  81% 200/246 [01:52<00:16,  2.82it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9982:  82% 201/246 [01:52<00:15,  2.83it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5468:  82% 201/246 [01:52<00:15,  2.83it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5468:  82% 202/246 [01:53<00:15,  2.85it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0549:  82% 202/246 [01:53<00:15,  2.85it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0549:  83% 203/246 [01:53<00:15,  2.83it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7338:  83% 203/246 [01:53<00:15,  2.83it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7338:  83% 204/246 [01:53<00:14,  2.84it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8310:  83% 204/246 [01:53<00:14,  2.84it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8310:  83% 205/246 [01:54<00:14,  2.85it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9339:  83% 205/246 [01:54<00:14,  2.85it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9339:  84% 206/246 [01:54<00:14,  2.80it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0718:  84% 206/246 [01:54<00:14,  2.80it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0718:  84% 207/246 [01:54<00:13,  2.85it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6469:  84% 207/246 [01:55<00:13,  2.85it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6469:  85% 208/246 [01:55<00:13,  2.85it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8117:  85% 208/246 [01:55<00:13,  2.85it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8117:  85% 209/246 [01:55<00:13,  2.82it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8270:  85% 209/246 [01:55<00:13,  2.82it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8270:  85% 210/246 [01:55<00:12,  2.79it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7605:  85% 210/246 [01:56<00:12,  2.79it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7605:  86% 211/246 [01:56<00:12,  2.79it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9019:  86% 211/246 [01:56<00:12,  2.79it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9019:  86% 212/246 [01:56<00:12,  2.81it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9507:  86% 212/246 [01:56<00:12,  2.81it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9507:  87% 213/246 [01:57<00:11,  2.81it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7286:  87% 213/246 [01:57<00:11,  2.81it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7286:  87% 214/246 [01:57<00:11,  2.81it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6929:  87% 214/246 [01:57<00:11,  2.81it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6929:  87% 215/246 [01:57<00:11,  2.82it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9274:  87% 215/246 [01:57<00:11,  2.82it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9274:  88% 216/246 [01:58<00:10,  2.82it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7710:  88% 216/246 [01:58<00:10,  2.82it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7710:  88% 217/246 [01:58<00:10,  2.69it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6518:  88% 217/246 [01:58<00:10,  2.69it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6518:  89% 218/246 [01:58<00:10,  2.68it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8463:  89% 218/246 [01:59<00:10,  2.68it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8463:  89% 219/246 [01:59<00:09,  2.84it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8960:  89% 219/246 [01:59<00:09,  2.84it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8960:  89% 220/246 [01:59<00:09,  2.78it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8754:  89% 220/246 [01:59<00:09,  2.78it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8754:  90% 221/246 [01:59<00:09,  2.69it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7842:  90% 221/246 [02:00<00:09,  2.69it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7842:  90% 222/246 [02:00<00:08,  2.69it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9339:  90% 222/246 [02:00<00:08,  2.69it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9339:  91% 223/246 [02:00<00:08,  2.63it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7417:  91% 223/246 [02:00<00:08,  2.63it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7417:  91% 224/246 [02:01<00:08,  2.70it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7940:  91% 224/246 [02:01<00:08,  2.70it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7940:  91% 225/246 [02:01<00:08,  2.59it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9174:  91% 225/246 [02:01<00:08,  2.59it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9174:  92% 226/246 [02:01<00:07,  2.67it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7214:  92% 226/246 [02:02<00:07,  2.67it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7214:  92% 227/246 [02:02<00:06,  2.72it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8479:  92% 227/246 [02:02<00:06,  2.72it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8479:  93% 228/246 [02:02<00:06,  2.75it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8121:  93% 228/246 [02:02<00:06,  2.75it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8121:  93% 229/246 [02:02<00:06,  2.74it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5941:  93% 229/246 [02:03<00:06,  2.74it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5941:  93% 230/246 [02:03<00:05,  2.75it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5905:  93% 230/246 [02:03<00:05,  2.75it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5905:  94% 231/246 [02:03<00:05,  2.79it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5821:  94% 231/246 [02:03<00:05,  2.79it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.5821:  94% 232/246 [02:03<00:04,  2.84it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9291:  94% 232/246 [02:04<00:04,  2.84it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9291:  95% 233/246 [02:04<00:04,  2.82it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6756:  95% 233/246 [02:04<00:04,  2.82it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6756:  95% 234/246 [02:04<00:04,  2.76it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0166:  95% 234/246 [02:04<00:04,  2.76it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0166:  96% 235/246 [02:05<00:04,  2.74it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7223:  96% 235/246 [02:05<00:04,  2.74it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7223:  96% 236/246 [02:05<00:03,  2.75it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8452:  96% 236/246 [02:05<00:03,  2.75it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8452:  96% 237/246 [02:05<00:03,  2.76it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8356:  96% 237/246 [02:05<00:03,  2.76it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8356:  97% 238/246 [02:06<00:02,  2.81it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8632:  97% 238/246 [02:06<00:02,  2.81it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8632:  97% 239/246 [02:06<00:02,  2.83it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9732:  97% 239/246 [02:06<00:02,  2.83it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.9732:  98% 240/246 [02:06<00:02,  2.84it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8940:  98% 240/246 [02:06<00:02,  2.84it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8940:  98% 241/246 [02:07<00:01,  2.85it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6951:  98% 241/246 [02:07<00:01,  2.85it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.6951:  98% 242/246 [02:07<00:01,  2.84it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7693:  98% 242/246 [02:07<00:01,  2.84it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7693:  99% 243/246 [02:07<00:01,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8795:  99% 243/246 [02:08<00:01,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.8795:  99% 244/246 [02:08<00:00,  2.88it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0693:  99% 244/246 [02:08<00:00,  2.88it/s]\u001b[A\n","Epochs 1/2. Running Loss:    1.0693: 100% 245/246 [02:08<00:00,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7753: 100% 245/246 [02:08<00:00,  2.89it/s]\u001b[A\n","Epochs 1/2. Running Loss:    0.7753: 100% 246/246 [02:08<00:00,  1.91it/s]\n","\n","  0% 0/600 [00:00<?, ?it/s]\u001b[A\n","  0% 1/600 [00:02<23:06,  2.31s/it]\u001b[A\n","  0% 2/600 [00:02<13:20,  1.34s/it]\n","Epoch 2 of 2: 100% 2/2 [05:14<00:00, 157.30s/it]\n"]}]},{"cell_type":"markdown","source":["# Results of each model on the English dev set of 1000 sentences and Fine-tuned on Multilingual train set of around 4000 sentences with noisy data"],"metadata":{"id":"oK9HEtmFs0qd"}},{"cell_type":"code","source":["!python eval.py"],"metadata":{"id":"EVOR4NBNLimF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683518359110,"user_tz":240,"elapsed":91470,"user":{"displayName":"stephen dias","userId":"16065654066476337384"}},"outputId":"01e2a9fe-5e77-442f-9d0b-fb8c0989139b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-05-08 03:57:52.032281: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-05-08 03:57:53.538677: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Evaluating roberta\troberta-large\tv1\n","  0% 2/1000 [00:01<12:42,  1.31it/s]\n","100% 20/20 [00:14<00:00,  1.35it/s]\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/content/drive/.shortcut-targets-by-id/1xD1ZIqHdatgu8BZr1nav5pFJeOkI8nOi/depression-detection-lt-edi-2022/models/eval.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  self.results = self.results.append(result, ignore_index=True)\n","Evaluating roberta\troberta-large\tv2\n","  0% 2/1000 [00:01<09:19,  1.78it/s]\n","100% 20/20 [00:14<00:00,  1.37it/s]\n","/content/drive/.shortcut-targets-by-id/1xD1ZIqHdatgu8BZr1nav5pFJeOkI8nOi/depression-detection-lt-edi-2022/models/eval.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  self.results = self.results.append(result, ignore_index=True)\n","roberta-large & 0.701 & 0.656 & 0.519 & 0.523 \\\\\n","Evaluating bert\tbert-base-multilingual-cased\tv1\n","  0% 2/1000 [00:01<14:16,  1.16it/s]\n","100% 20/20 [00:05<00:00,  3.82it/s]\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/content/drive/.shortcut-targets-by-id/1xD1ZIqHdatgu8BZr1nav5pFJeOkI8nOi/depression-detection-lt-edi-2022/models/eval.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  self.results = self.results.append(result, ignore_index=True)\n","Evaluating bert\tbert-base-multilingual-cased\tv2\n","  0% 2/1000 [00:01<11:13,  1.48it/s]\n","100% 20/20 [00:05<00:00,  3.80it/s]\n","/content/drive/.shortcut-targets-by-id/1xD1ZIqHdatgu8BZr1nav5pFJeOkI8nOi/depression-detection-lt-edi-2022/models/eval.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  self.results = self.results.append(result, ignore_index=True)\n","bert-base-multilingual-cased & 0.663 & 0.653 & 0.605 & 0.594 \\\\\n","Traceback (most recent call last):\n","  File \"/content/drive/.shortcut-targets-by-id/1xD1ZIqHdatgu8BZr1nav5pFJeOkI8nOi/depression-detection-lt-edi-2022/models/eval.py\", line 81, in <module>\n","    result = agg.get_result()\n","  File \"/content/drive/.shortcut-targets-by-id/1xD1ZIqHdatgu8BZr1nav5pFJeOkI8nOi/depression-detection-lt-edi-2022/models/eval.py\", line 25, in get_result\n","    values = self._get_best_result_by_main_metric(main_metric)\n","  File \"/content/drive/.shortcut-targets-by-id/1xD1ZIqHdatgu8BZr1nav5pFJeOkI8nOi/depression-detection-lt-edi-2022/models/eval.py\", line 32, in _get_best_result_by_main_metric\n","    return self.results.iloc[self.results[main_metric].idxmax()]\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\", line 2564, in idxmax\n","    i = self.argmax(axis, skipna, *args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/base.py\", line 655, in argmax\n","    return nanops.nanargmax(  # type: ignore[return-value]\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/nanops.py\", line 88, in _f\n","    raise TypeError(\n","TypeError: reduction operation 'argmax' not allowed for this dtype\n"]}]},{"cell_type":"markdown","source":["# Results of each model on the English dev set of 1000 sentences and Fine-tuned on Multilingual train set of around 4000 sentences"],"metadata":{"id":"9ji1mCxus-fh"}},{"cell_type":"code","source":["!python eval.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wPZE2qCTbz-w","executionInfo":{"status":"ok","timestamp":1683067253322,"user_tz":240,"elapsed":100104,"user":{"displayName":"stephen dias","userId":"16065654066476337384"}},"outputId":"4b0499ad-375b-42a5-9e74-d855236700f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-05-02 22:39:17.561911: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-05-02 22:39:18.898894: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Evaluating roberta\troberta-large\tv1\n","  0% 2/1000 [00:01<10:09,  1.64it/s]\n","100% 20/20 [00:14<00:00,  1.35it/s]\n","/content/drive/.shortcut-targets-by-id/1xD1ZIqHdatgu8BZr1nav5pFJeOkI8nOi/depression-detection-lt-edi-2022/models/eval.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  self.results = self.results.append(result, ignore_index=True)\n","Evaluating roberta\troberta-large\tv2\n","  0% 2/1000 [00:01<13:07,  1.27it/s]\n","100% 20/20 [00:14<00:00,  1.38it/s]\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/content/drive/.shortcut-targets-by-id/1xD1ZIqHdatgu8BZr1nav5pFJeOkI8nOi/depression-detection-lt-edi-2022/models/eval.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  self.results = self.results.append(result, ignore_index=True)\n","roberta-large & 0.706 & 0.839 & 0.51 & 0.503 \\\\\n","Evaluating bert\tbert-base-multilingual-cased\tv1\n","  0% 2/1000 [00:01<11:23,  1.46it/s]\n","100% 20/20 [00:05<00:00,  3.88it/s]\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/content/drive/.shortcut-targets-by-id/1xD1ZIqHdatgu8BZr1nav5pFJeOkI8nOi/depression-detection-lt-edi-2022/models/eval.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  self.results = self.results.append(result, ignore_index=True)\n","Evaluating bert\tbert-base-multilingual-cased\tv2\n","  0% 2/1000 [00:01<12:43,  1.31it/s]\n","100% 20/20 [00:05<00:00,  3.89it/s]\n","/content/drive/.shortcut-targets-by-id/1xD1ZIqHdatgu8BZr1nav5pFJeOkI8nOi/depression-detection-lt-edi-2022/models/eval.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  self.results = self.results.append(result, ignore_index=True)\n","bert-base-multilingual-cased & 0.672 & 0.659 & 0.569 & 0.579 \\\\\n","Traceback (most recent call last):\n","  File \"/content/drive/.shortcut-targets-by-id/1xD1ZIqHdatgu8BZr1nav5pFJeOkI8nOi/depression-detection-lt-edi-2022/models/eval.py\", line 81, in <module>\n","    print(agg.get_result())\n","  File \"/content/drive/.shortcut-targets-by-id/1xD1ZIqHdatgu8BZr1nav5pFJeOkI8nOi/depression-detection-lt-edi-2022/models/eval.py\", line 25, in get_result\n","    values = self._get_best_result_by_main_metric(main_metric)\n","  File \"/content/drive/.shortcut-targets-by-id/1xD1ZIqHdatgu8BZr1nav5pFJeOkI8nOi/depression-detection-lt-edi-2022/models/eval.py\", line 32, in _get_best_result_by_main_metric\n","    return self.results.iloc[self.results[main_metric].idxmax()]\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\", line 2564, in idxmax\n","    i = self.argmax(axis, skipna, *args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/base.py\", line 655, in argmax\n","    return nanops.nanargmax(  # type: ignore[return-value]\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/nanops.py\", line 88, in _f\n","    raise TypeError(\n","TypeError: reduction operation 'argmax' not allowed for this dtype\n"]}]},{"cell_type":"code","source":["!python predict.py"],"metadata":{"id":"t6qVPKW-Li3f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682997454489,"user_tz":240,"elapsed":193248,"user":{"displayName":"stephen dias","userId":"16065654066476337384"}},"outputId":"e5654f17-e85c-42ed-e214-1e354c25e439"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-05-02 03:14:26.433727: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-05-02 03:14:28.182058: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Generating predictions using roberta\trafalposwiata/roberta-large-depression\tNone\n","Downloading (…)lve/main/config.json: 100% 904/904 [00:00<00:00, 4.37MB/s]\n","Downloading pytorch_model.bin: 100% 1.42G/1.42G [00:16<00:00, 84.2MB/s]\n","Downloading (…)okenizer_config.json: 100% 352/352 [00:00<00:00, 1.59MB/s]\n","Downloading (…)olve/main/vocab.json: 100% 798k/798k [00:00<00:00, 8.99MB/s]\n","Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 90.0MB/s]\n","Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 49.6MB/s]\n","Downloading (…)cial_tokens_map.json: 100% 239/239 [00:00<00:00, 1.39MB/s]\n","  0% 7/3245 [00:03<25:53,  2.08it/s]\n","100% 406/406 [00:21<00:00, 18.69it/s]\n","Generating predictions using roberta\trafalposwiata/deproberta-large-depression\tNone\n","Downloading (…)lve/main/config.json: 100% 924/924 [00:00<00:00, 5.73MB/s]\n","Downloading pytorch_model.bin: 100% 1.42G/1.42G [00:22<00:00, 63.9MB/s]\n","Downloading (…)okenizer_config.json: 100% 1.17k/1.17k [00:00<00:00, 7.41MB/s]\n","Downloading (…)olve/main/vocab.json: 100% 798k/798k [00:00<00:00, 201MB/s]\n","Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 316MB/s]\n","Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 51.2MB/s]\n","Downloading (…)cial_tokens_map.json: 100% 772/772 [00:00<00:00, 4.36MB/s]\n","  0% 7/3245 [00:03<27:30,  1.96it/s]\n","100% 406/406 [00:22<00:00, 18.40it/s]\n","  0% 7/3245 [00:04<31:37,  1.71it/s]\n","100% 406/406 [00:22<00:00, 18.32it/s]\n","  0% 7/3245 [00:03<29:40,  1.82it/s]\n","100% 406/406 [00:22<00:00, 18.16it/s]\n"]}]},{"cell_type":"code","source":["!python reddit_depression_corpora.py"],"metadata":{"id":"GxGyT4Ru03av","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680832594562,"user_tz":240,"elapsed":109475,"user":{"displayName":"Aditya Limbekar","userId":"04179261916482495738"}},"outputId":"7d7c574f-dc17-48b4-f4ba-23b09ce4c347"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["100% 81/81 [01:44<00:00,  1.29s/it]\n","depression: 94039 (23.8%)\n","anxiety: 53788 (13.6%)\n","suicidewatch: 34737 (8.8%)\n","lonely: 20095 (5.1%)\n","socialanxiety: 19652 (5.0%)\n","adhd: 10000 (2.5%)\n","conspiracy: 10000 (2.5%)\n","divorce: 10000 (2.5%)\n","EDAnonymous: 10000 (2.5%)\n","fitness: 10000 (2.5%)\n","guns: 10000 (2.5%)\n","jokes: 10000 (2.5%)\n","legaladvice: 10000 (2.5%)\n","mentalhealth: 10000 (2.5%)\n","parenting: 10000 (2.5%)\n","personalfinance: 10000 (2.5%)\n","relationships: 10000 (2.5%)\n","meditation: 7851 (2.0%)\n","autism: 7753 (2.0%)\n","bpd: 7304 (1.8%)\n","schizophrenia: 6656 (1.7%)\n","healthanxiety: 5428 (1.4%)\n","bipolarreddit: 5193 (1.3%)\n","alcoholism: 4628 (1.2%)\n","teaching: 3515 (0.9%)\n","ptsd: 2093 (0.5%)\n","addiction: 1758 (0.4%)\n","COVID19: 978 (0.2%)\n","\n","Unique texts: 395468 (train: 98000 / validation: 2000).\n"]}]},{"cell_type":"markdown","source":["# Below sections translate the datasets, cleaning and creating the csv's and tsv's"],"metadata":{"id":"tWbo4dTBtL-r"}},{"cell_type":"code","source":["!pip install -U deep-translator"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wg7p_as2MzR_","executionInfo":{"status":"ok","timestamp":1682439437237,"user_tz":240,"elapsed":7314,"user":{"displayName":"stephen dias","userId":"16065654066476337384"}},"outputId":"2a2fc348-a484-4fac-bb4f-7816d2002a3c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting deep-translator\n","  Downloading deep_translator-1.10.1-py3-none-any.whl (35 kB)\n","Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.9/dist-packages (from deep-translator) (4.11.2)\n","Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.9/dist-packages (from deep-translator) (2.27.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.4.1)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.10)\n","Installing collected packages: deep-translator\n","Successfully installed deep-translator-1.10.1\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from deep_translator import GoogleTranslator\n","\n","\n","# Define languages to translate to\n","languages = ['en','de','es','hi']\n","\n","# Read TSV file into DataFrame\n","df = pd.read_csv('/content/drive/MyDrive/depression-detection-lt-edi-2022/data/preprocessed_dataset/train_1.csv', sep=',', encoding='latin1')\n","print(df)\n","# Iterate through each language and translate non-English sentences to that language\n","for lang in languages:\n","    # Create a new column in the DataFrame for the translated sentences\n","    # Iterate through each row and translate non-English sentences\n","    for index, row in df.iterrows():\n","      print(index ,lang)\n","      if(len(row['text'])) < 5000:\n","        translated = GoogleTranslator(source='auto', target=lang).translate(row['text'])\n","      #translated = translator.translate(row['sentence']).text\n","        df.loc[index, 'translated_text'] = translated\n","        df.loc[index, 'language_ch'] = lang\n","\n","    print(lang)\n","    # Append to file\n","    with open('/content/drive/MyDrive/depression-detection-lt-edi-2022/data/preprocessed_dataset/train_multi_lang.csv', 'a') as f:\n","        df.to_csv(f, sep='\\t', index=False, header=(lang=='en'), mode='a')"],"metadata":{"id":"Yw3qayCgD5IQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682445411100,"user_tz":240,"elapsed":5851154,"user":{"displayName":"stephen dias","userId":"16065654066476337384"}},"outputId":"3394c753-cc9f-4a2a-843b-25c28cedbfa5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","1003 de\n","1004 de\n","1005 de\n","1006 de\n","1007 de\n","1008 de\n","1009 de\n","1010 de\n","1011 de\n","1012 de\n","1013 de\n","1014 de\n","1015 de\n","1016 de\n","1017 de\n","1018 de\n","1019 de\n","1020 de\n","1021 de\n","1022 de\n","1023 de\n","1024 de\n","1025 de\n","1026 de\n","1027 de\n","1028 de\n","1029 de\n","1030 de\n","1031 de\n","1032 de\n","1033 de\n","1034 de\n","1035 de\n","1036 de\n","1037 de\n","1038 de\n","1039 de\n","1040 de\n","1041 de\n","1042 de\n","1043 de\n","1044 de\n","1045 de\n","1046 de\n","1047 de\n","1048 de\n","1049 de\n","1050 de\n","1051 de\n","1052 de\n","1053 de\n","1054 de\n","1055 de\n","1056 de\n","1057 de\n","1058 de\n","1059 de\n","1060 de\n","1061 de\n","1062 de\n","1063 de\n","1064 de\n","1065 de\n","1066 de\n","1067 de\n","1068 de\n","1069 de\n","1070 de\n","1071 de\n","1072 de\n","1073 de\n","1074 de\n","1075 de\n","1076 de\n","1077 de\n","1078 de\n","1079 de\n","1080 de\n","1081 de\n","1082 de\n","1083 de\n","1084 de\n","1085 de\n","1086 de\n","1087 de\n","1088 de\n","1089 de\n","1090 de\n","1091 de\n","1092 de\n","1093 de\n","1094 de\n","1095 de\n","1096 de\n","1097 de\n","1098 de\n","1099 de\n","1100 de\n","1101 de\n","1102 de\n","1103 de\n","1104 de\n","1105 de\n","1106 de\n","1107 de\n","1108 de\n","1109 de\n","1110 de\n","1111 de\n","1112 de\n","1113 de\n","1114 de\n","1115 de\n","1116 de\n","1117 de\n","1118 de\n","1119 de\n","1120 de\n","1121 de\n","1122 de\n","1123 de\n","1124 de\n","1125 de\n","1126 de\n","1127 de\n","1128 de\n","1129 de\n","1130 de\n","1131 de\n","1132 de\n","1133 de\n","1134 de\n","1135 de\n","1136 de\n","1137 de\n","1138 de\n","1139 de\n","1140 de\n","1141 de\n","1142 de\n","1143 de\n","1144 de\n","1145 de\n","1146 de\n","1147 de\n","1148 de\n","1149 de\n","1150 de\n","1151 de\n","1152 de\n","1153 de\n","1154 de\n","1155 de\n","1156 de\n","1157 de\n","1158 de\n","1159 de\n","1160 de\n","1161 de\n","1162 de\n","1163 de\n","1164 de\n","1165 de\n","1166 de\n","1167 de\n","1168 de\n","1169 de\n","1170 de\n","1171 de\n","1172 de\n","1173 de\n","1174 de\n","1175 de\n","1176 de\n","1177 de\n","1178 de\n","1179 de\n","1180 de\n","1181 de\n","1182 de\n","1183 de\n","1184 de\n","1185 de\n","1186 de\n","1187 de\n","1188 de\n","1189 de\n","1190 de\n","1191 de\n","1192 de\n","1193 de\n","1194 de\n","1195 de\n","1196 de\n","1197 de\n","1198 de\n","1199 de\n","1200 de\n","1201 de\n","1202 de\n","1203 de\n","1204 de\n","1205 de\n","1206 de\n","1207 de\n","1208 de\n","1209 de\n","1210 de\n","1211 de\n","1212 de\n","1213 de\n","1214 de\n","1215 de\n","1216 de\n","1217 de\n","1218 de\n","1219 de\n","1220 de\n","1221 de\n","1222 de\n","1223 de\n","1224 de\n","1225 de\n","1226 de\n","1227 de\n","1228 de\n","1229 de\n","1230 de\n","1231 de\n","1232 de\n","1233 de\n","1234 de\n","1235 de\n","1236 de\n","1237 de\n","1238 de\n","1239 de\n","1240 de\n","1241 de\n","1242 de\n","1243 de\n","1244 de\n","1245 de\n","1246 de\n","1247 de\n","1248 de\n","1249 de\n","1250 de\n","1251 de\n","1252 de\n","1253 de\n","1254 de\n","1255 de\n","1256 de\n","1257 de\n","1258 de\n","1259 de\n","1260 de\n","1261 de\n","1262 de\n","1263 de\n","1264 de\n","1265 de\n","1266 de\n","1267 de\n","1268 de\n","1269 de\n","1270 de\n","1271 de\n","1272 de\n","1273 de\n","1274 de\n","1275 de\n","1276 de\n","1277 de\n","1278 de\n","1279 de\n","1280 de\n","1281 de\n","1282 de\n","1283 de\n","1284 de\n","1285 de\n","1286 de\n","1287 de\n","1288 de\n","1289 de\n","1290 de\n","1291 de\n","1292 de\n","1293 de\n","1294 de\n","1295 de\n","1296 de\n","1297 de\n","1298 de\n","1299 de\n","1300 de\n","1301 de\n","1302 de\n","1303 de\n","1304 de\n","1305 de\n","1306 de\n","1307 de\n","1308 de\n","1309 de\n","1310 de\n","1311 de\n","1312 de\n","1313 de\n","1314 de\n","1315 de\n","1316 de\n","1317 de\n","1318 de\n","1319 de\n","1320 de\n","1321 de\n","1322 de\n","1323 de\n","1324 de\n","1325 de\n","1326 de\n","1327 de\n","1328 de\n","1329 de\n","1330 de\n","1331 de\n","1332 de\n","1333 de\n","1334 de\n","1335 de\n","1336 de\n","1337 de\n","1338 de\n","1339 de\n","1340 de\n","1341 de\n","1342 de\n","1343 de\n","1344 de\n","1345 de\n","1346 de\n","1347 de\n","1348 de\n","1349 de\n","1350 de\n","1351 de\n","1352 de\n","1353 de\n","1354 de\n","1355 de\n","1356 de\n","1357 de\n","1358 de\n","1359 de\n","1360 de\n","1361 de\n","1362 de\n","1363 de\n","1364 de\n","1365 de\n","1366 de\n","1367 de\n","1368 de\n","1369 de\n","1370 de\n","1371 de\n","1372 de\n","1373 de\n","1374 de\n","1375 de\n","1376 de\n","1377 de\n","1378 de\n","1379 de\n","1380 de\n","1381 de\n","1382 de\n","1383 de\n","1384 de\n","1385 de\n","1386 de\n","1387 de\n","1388 de\n","1389 de\n","1390 de\n","1391 de\n","1392 de\n","1393 de\n","1394 de\n","1395 de\n","1396 de\n","1397 de\n","1398 de\n","1399 de\n","1400 de\n","1401 de\n","1402 de\n","1403 de\n","1404 de\n","1405 de\n","1406 de\n","1407 de\n","1408 de\n","1409 de\n","1410 de\n","1411 de\n","1412 de\n","1413 de\n","1414 de\n","1415 de\n","1416 de\n","1417 de\n","1418 de\n","1419 de\n","1420 de\n","1421 de\n","1422 de\n","1423 de\n","1424 de\n","1425 de\n","1426 de\n","1427 de\n","1428 de\n","1429 de\n","1430 de\n","1431 de\n","1432 de\n","1433 de\n","1434 de\n","1435 de\n","1436 de\n","1437 de\n","1438 de\n","1439 de\n","1440 de\n","1441 de\n","1442 de\n","1443 de\n","1444 de\n","1445 de\n","1446 de\n","1447 de\n","1448 de\n","1449 de\n","1450 de\n","1451 de\n","1452 de\n","1453 de\n","1454 de\n","1455 de\n","1456 de\n","1457 de\n","1458 de\n","1459 de\n","1460 de\n","1461 de\n","1462 de\n","1463 de\n","1464 de\n","1465 de\n","1466 de\n","1467 de\n","1468 de\n","1469 de\n","1470 de\n","1471 de\n","1472 de\n","1473 de\n","1474 de\n","1475 de\n","1476 de\n","1477 de\n","1478 de\n","1479 de\n","1480 de\n","1481 de\n","1482 de\n","1483 de\n","1484 de\n","1485 de\n","1486 de\n","1487 de\n","1488 de\n","1489 de\n","1490 de\n","1491 de\n","1492 de\n","1493 de\n","1494 de\n","1495 de\n","1496 de\n","1497 de\n","1498 de\n","1499 de\n","1500 de\n","1501 de\n","1502 de\n","1503 de\n","1504 de\n","1505 de\n","1506 de\n","1507 de\n","1508 de\n","1509 de\n","1510 de\n","1511 de\n","1512 de\n","1513 de\n","1514 de\n","1515 de\n","1516 de\n","1517 de\n","1518 de\n","1519 de\n","1520 de\n","1521 de\n","1522 de\n","1523 de\n","1524 de\n","1525 de\n","1526 de\n","1527 de\n","1528 de\n","1529 de\n","1530 de\n","1531 de\n","1532 de\n","1533 de\n","1534 de\n","1535 de\n","1536 de\n","1537 de\n","1538 de\n","1539 de\n","1540 de\n","1541 de\n","1542 de\n","1543 de\n","1544 de\n","1545 de\n","1546 de\n","1547 de\n","1548 de\n","1549 de\n","1550 de\n","1551 de\n","1552 de\n","1553 de\n","1554 de\n","1555 de\n","1556 de\n","1557 de\n","1558 de\n","1559 de\n","1560 de\n","1561 de\n","1562 de\n","1563 de\n","1564 de\n","1565 de\n","1566 de\n","1567 de\n","1568 de\n","1569 de\n","1570 de\n","1571 de\n","1572 de\n","1573 de\n","1574 de\n","1575 de\n","1576 de\n","1577 de\n","1578 de\n","1579 de\n","1580 de\n","1581 de\n","1582 de\n","1583 de\n","1584 de\n","1585 de\n","1586 de\n","1587 de\n","1588 de\n","1589 de\n","1590 de\n","1591 de\n","1592 de\n","1593 de\n","1594 de\n","1595 de\n","1596 de\n","1597 de\n","1598 de\n","1599 de\n","1600 de\n","1601 de\n","1602 de\n","1603 de\n","1604 de\n","1605 de\n","1606 de\n","1607 de\n","1608 de\n","1609 de\n","1610 de\n","1611 de\n","1612 de\n","1613 de\n","1614 de\n","1615 de\n","1616 de\n","1617 de\n","1618 de\n","1619 de\n","1620 de\n","1621 de\n","1622 de\n","1623 de\n","1624 de\n","1625 de\n","1626 de\n","1627 de\n","1628 de\n","1629 de\n","1630 de\n","1631 de\n","1632 de\n","1633 de\n","1634 de\n","1635 de\n","1636 de\n","1637 de\n","1638 de\n","1639 de\n","1640 de\n","1641 de\n","1642 de\n","1643 de\n","1644 de\n","1645 de\n","1646 de\n","1647 de\n","1648 de\n","1649 de\n","1650 de\n","1651 de\n","1652 de\n","1653 de\n","1654 de\n","1655 de\n","1656 de\n","1657 de\n","1658 de\n","1659 de\n","1660 de\n","1661 de\n","1662 de\n","1663 de\n","1664 de\n","1665 de\n","1666 de\n","1667 de\n","1668 de\n","1669 de\n","1670 de\n","1671 de\n","1672 de\n","1673 de\n","1674 de\n","1675 de\n","1676 de\n","1677 de\n","1678 de\n","1679 de\n","1680 de\n","1681 de\n","1682 de\n","1683 de\n","1684 de\n","1685 de\n","1686 de\n","1687 de\n","1688 de\n","1689 de\n","1690 de\n","1691 de\n","1692 de\n","1693 de\n","1694 de\n","1695 de\n","1696 de\n","1697 de\n","1698 de\n","1699 de\n","1700 de\n","1701 de\n","1702 de\n","1703 de\n","1704 de\n","1705 de\n","1706 de\n","1707 de\n","1708 de\n","1709 de\n","1710 de\n","1711 de\n","1712 de\n","1713 de\n","1714 de\n","1715 de\n","1716 de\n","1717 de\n","1718 de\n","1719 de\n","1720 de\n","1721 de\n","1722 de\n","1723 de\n","1724 de\n","1725 de\n","1726 de\n","1727 de\n","1728 de\n","1729 de\n","1730 de\n","1731 de\n","1732 de\n","1733 de\n","1734 de\n","1735 de\n","1736 de\n","1737 de\n","1738 de\n","1739 de\n","1740 de\n","1741 de\n","1742 de\n","1743 de\n","1744 de\n","1745 de\n","1746 de\n","1747 de\n","1748 de\n","1749 de\n","1750 de\n","1751 de\n","1752 de\n","1753 de\n","1754 de\n","1755 de\n","1756 de\n","1757 de\n","1758 de\n","1759 de\n","1760 de\n","1761 de\n","1762 de\n","1763 de\n","1764 de\n","1765 de\n","1766 de\n","1767 de\n","1768 de\n","1769 de\n","1770 de\n","1771 de\n","1772 de\n","1773 de\n","1774 de\n","1775 de\n","1776 de\n","1777 de\n","1778 de\n","1779 de\n","1780 de\n","1781 de\n","1782 de\n","1783 de\n","1784 de\n","1785 de\n","1786 de\n","1787 de\n","1788 de\n","1789 de\n","1790 de\n","1791 de\n","1792 de\n","1793 de\n","1794 de\n","1795 de\n","1796 de\n","1797 de\n","1798 de\n","1799 de\n","1800 de\n","1801 de\n","1802 de\n","1803 de\n","1804 de\n","1805 de\n","1806 de\n","1807 de\n","1808 de\n","1809 de\n","1810 de\n","1811 de\n","1812 de\n","1813 de\n","1814 de\n","1815 de\n","1816 de\n","1817 de\n","1818 de\n","1819 de\n","1820 de\n","1821 de\n","1822 de\n","1823 de\n","1824 de\n","1825 de\n","1826 de\n","1827 de\n","1828 de\n","1829 de\n","1830 de\n","1831 de\n","1832 de\n","1833 de\n","1834 de\n","1835 de\n","1836 de\n","1837 de\n","1838 de\n","1839 de\n","1840 de\n","1841 de\n","1842 de\n","1843 de\n","1844 de\n","1845 de\n","1846 de\n","1847 de\n","1848 de\n","1849 de\n","1850 de\n","1851 de\n","1852 de\n","1853 de\n","1854 de\n","1855 de\n","1856 de\n","1857 de\n","1858 de\n","1859 de\n","1860 de\n","1861 de\n","1862 de\n","1863 de\n","1864 de\n","1865 de\n","1866 de\n","1867 de\n","1868 de\n","1869 de\n","1870 de\n","1871 de\n","1872 de\n","1873 de\n","1874 de\n","1875 de\n","1876 de\n","1877 de\n","1878 de\n","1879 de\n","1880 de\n","1881 de\n","1882 de\n","1883 de\n","1884 de\n","1885 de\n","1886 de\n","1887 de\n","1888 de\n","1889 de\n","1890 de\n","1891 de\n","1892 de\n","1893 de\n","1894 de\n","1895 de\n","1896 de\n","1897 de\n","1898 de\n","1899 de\n","1900 de\n","1901 de\n","1902 de\n","1903 de\n","1904 de\n","1905 de\n","1906 de\n","1907 de\n","1908 de\n","1909 de\n","1910 de\n","1911 de\n","1912 de\n","1913 de\n","1914 de\n","1915 de\n","1916 de\n","1917 de\n","1918 de\n","1919 de\n","1920 de\n","1921 de\n","1922 de\n","1923 de\n","1924 de\n","1925 de\n","1926 de\n","1927 de\n","1928 de\n","1929 de\n","1930 de\n","1931 de\n","1932 de\n","1933 de\n","1934 de\n","1935 de\n","1936 de\n","1937 de\n","1938 de\n","1939 de\n","1940 de\n","1941 de\n","1942 de\n","1943 de\n","1944 de\n","1945 de\n","1946 de\n","1947 de\n","1948 de\n","1949 de\n","1950 de\n","1951 de\n","1952 de\n","1953 de\n","1954 de\n","1955 de\n","1956 de\n","1957 de\n","1958 de\n","1959 de\n","1960 de\n","1961 de\n","1962 de\n","1963 de\n","1964 de\n","1965 de\n","1966 de\n","1967 de\n","1968 de\n","1969 de\n","1970 de\n","1971 de\n","1972 de\n","1973 de\n","1974 de\n","1975 de\n","1976 de\n","1977 de\n","1978 de\n","1979 de\n","1980 de\n","1981 de\n","1982 de\n","1983 de\n","1984 de\n","1985 de\n","1986 de\n","1987 de\n","1988 de\n","1989 de\n","1990 de\n","1991 de\n","1992 de\n","1993 de\n","1994 de\n","1995 de\n","1996 de\n","1997 de\n","1998 de\n","1999 de\n","de\n","0 es\n","1 es\n","2 es\n","3 es\n","4 es\n","5 es\n","6 es\n","7 es\n","8 es\n","9 es\n","10 es\n","11 es\n","12 es\n","13 es\n","14 es\n","15 es\n","16 es\n","17 es\n","18 es\n","19 es\n","20 es\n","21 es\n","22 es\n","23 es\n","24 es\n","25 es\n","26 es\n","27 es\n","28 es\n","29 es\n","30 es\n","31 es\n","32 es\n","33 es\n","34 es\n","35 es\n","36 es\n","37 es\n","38 es\n","39 es\n","40 es\n","41 es\n","42 es\n","43 es\n","44 es\n","45 es\n","46 es\n","47 es\n","48 es\n","49 es\n","50 es\n","51 es\n","52 es\n","53 es\n","54 es\n","55 es\n","56 es\n","57 es\n","58 es\n","59 es\n","60 es\n","61 es\n","62 es\n","63 es\n","64 es\n","65 es\n","66 es\n","67 es\n","68 es\n","69 es\n","70 es\n","71 es\n","72 es\n","73 es\n","74 es\n","75 es\n","76 es\n","77 es\n","78 es\n","79 es\n","80 es\n","81 es\n","82 es\n","83 es\n","84 es\n","85 es\n","86 es\n","87 es\n","88 es\n","89 es\n","90 es\n","91 es\n","92 es\n","93 es\n","94 es\n","95 es\n","96 es\n","97 es\n","98 es\n","99 es\n","100 es\n","101 es\n","102 es\n","103 es\n","104 es\n","105 es\n","106 es\n","107 es\n","108 es\n","109 es\n","110 es\n","111 es\n","112 es\n","113 es\n","114 es\n","115 es\n","116 es\n","117 es\n","118 es\n","119 es\n","120 es\n","121 es\n","122 es\n","123 es\n","124 es\n","125 es\n","126 es\n","127 es\n","128 es\n","129 es\n","130 es\n","131 es\n","132 es\n","133 es\n","134 es\n","135 es\n","136 es\n","137 es\n","138 es\n","139 es\n","140 es\n","141 es\n","142 es\n","143 es\n","144 es\n","145 es\n","146 es\n","147 es\n","148 es\n","149 es\n","150 es\n","151 es\n","152 es\n","153 es\n","154 es\n","155 es\n","156 es\n","157 es\n","158 es\n","159 es\n","160 es\n","161 es\n","162 es\n","163 es\n","164 es\n","165 es\n","166 es\n","167 es\n","168 es\n","169 es\n","170 es\n","171 es\n","172 es\n","173 es\n","174 es\n","175 es\n","176 es\n","177 es\n","178 es\n","179 es\n","180 es\n","181 es\n","182 es\n","183 es\n","184 es\n","185 es\n","186 es\n","187 es\n","188 es\n","189 es\n","190 es\n","191 es\n","192 es\n","193 es\n","194 es\n","195 es\n","196 es\n","197 es\n","198 es\n","199 es\n","200 es\n","201 es\n","202 es\n","203 es\n","204 es\n","205 es\n","206 es\n","207 es\n","208 es\n","209 es\n","210 es\n","211 es\n","212 es\n","213 es\n","214 es\n","215 es\n","216 es\n","217 es\n","218 es\n","219 es\n","220 es\n","221 es\n","222 es\n","223 es\n","224 es\n","225 es\n","226 es\n","227 es\n","228 es\n","229 es\n","230 es\n","231 es\n","232 es\n","233 es\n","234 es\n","235 es\n","236 es\n","237 es\n","238 es\n","239 es\n","240 es\n","241 es\n","242 es\n","243 es\n","244 es\n","245 es\n","246 es\n","247 es\n","248 es\n","249 es\n","250 es\n","251 es\n","252 es\n","253 es\n","254 es\n","255 es\n","256 es\n","257 es\n","258 es\n","259 es\n","260 es\n","261 es\n","262 es\n","263 es\n","264 es\n","265 es\n","266 es\n","267 es\n","268 es\n","269 es\n","270 es\n","271 es\n","272 es\n","273 es\n","274 es\n","275 es\n","276 es\n","277 es\n","278 es\n","279 es\n","280 es\n","281 es\n","282 es\n","283 es\n","284 es\n","285 es\n","286 es\n","287 es\n","288 es\n","289 es\n","290 es\n","291 es\n","292 es\n","293 es\n","294 es\n","295 es\n","296 es\n","297 es\n","298 es\n","299 es\n","300 es\n","301 es\n","302 es\n","303 es\n","304 es\n","305 es\n","306 es\n","307 es\n","308 es\n","309 es\n","310 es\n","311 es\n","312 es\n","313 es\n","314 es\n","315 es\n","316 es\n","317 es\n","318 es\n","319 es\n","320 es\n","321 es\n","322 es\n","323 es\n","324 es\n","325 es\n","326 es\n","327 es\n","328 es\n","329 es\n","330 es\n","331 es\n","332 es\n","333 es\n","334 es\n","335 es\n","336 es\n","337 es\n","338 es\n","339 es\n","340 es\n","341 es\n","342 es\n","343 es\n","344 es\n","345 es\n","346 es\n","347 es\n","348 es\n","349 es\n","350 es\n","351 es\n","352 es\n","353 es\n","354 es\n","355 es\n","356 es\n","357 es\n","358 es\n","359 es\n","360 es\n","361 es\n","362 es\n","363 es\n","364 es\n","365 es\n","366 es\n","367 es\n","368 es\n","369 es\n","370 es\n","371 es\n","372 es\n","373 es\n","374 es\n","375 es\n","376 es\n","377 es\n","378 es\n","379 es\n","380 es\n","381 es\n","382 es\n","383 es\n","384 es\n","385 es\n","386 es\n","387 es\n","388 es\n","389 es\n","390 es\n","391 es\n","392 es\n","393 es\n","394 es\n","395 es\n","396 es\n","397 es\n","398 es\n","399 es\n","400 es\n","401 es\n","402 es\n","403 es\n","404 es\n","405 es\n","406 es\n","407 es\n","408 es\n","409 es\n","410 es\n","411 es\n","412 es\n","413 es\n","414 es\n","415 es\n","416 es\n","417 es\n","418 es\n","419 es\n","420 es\n","421 es\n","422 es\n","423 es\n","424 es\n","425 es\n","426 es\n","427 es\n","428 es\n","429 es\n","430 es\n","431 es\n","432 es\n","433 es\n","434 es\n","435 es\n","436 es\n","437 es\n","438 es\n","439 es\n","440 es\n","441 es\n","442 es\n","443 es\n","444 es\n","445 es\n","446 es\n","447 es\n","448 es\n","449 es\n","450 es\n","451 es\n","452 es\n","453 es\n","454 es\n","455 es\n","456 es\n","457 es\n","458 es\n","459 es\n","460 es\n","461 es\n","462 es\n","463 es\n","464 es\n","465 es\n","466 es\n","467 es\n","468 es\n","469 es\n","470 es\n","471 es\n","472 es\n","473 es\n","474 es\n","475 es\n","476 es\n","477 es\n","478 es\n","479 es\n","480 es\n","481 es\n","482 es\n","483 es\n","484 es\n","485 es\n","486 es\n","487 es\n","488 es\n","489 es\n","490 es\n","491 es\n","492 es\n","493 es\n","494 es\n","495 es\n","496 es\n","497 es\n","498 es\n","499 es\n","500 es\n","501 es\n","502 es\n","503 es\n","504 es\n","505 es\n","506 es\n","507 es\n","508 es\n","509 es\n","510 es\n","511 es\n","512 es\n","513 es\n","514 es\n","515 es\n","516 es\n","517 es\n","518 es\n","519 es\n","520 es\n","521 es\n","522 es\n","523 es\n","524 es\n","525 es\n","526 es\n","527 es\n","528 es\n","529 es\n","530 es\n","531 es\n","532 es\n","533 es\n","534 es\n","535 es\n","536 es\n","537 es\n","538 es\n","539 es\n","540 es\n","541 es\n","542 es\n","543 es\n","544 es\n","545 es\n","546 es\n","547 es\n","548 es\n","549 es\n","550 es\n","551 es\n","552 es\n","553 es\n","554 es\n","555 es\n","556 es\n","557 es\n","558 es\n","559 es\n","560 es\n","561 es\n","562 es\n","563 es\n","564 es\n","565 es\n","566 es\n","567 es\n","568 es\n","569 es\n","570 es\n","571 es\n","572 es\n","573 es\n","574 es\n","575 es\n","576 es\n","577 es\n","578 es\n","579 es\n","580 es\n","581 es\n","582 es\n","583 es\n","584 es\n","585 es\n","586 es\n","587 es\n","588 es\n","589 es\n","590 es\n","591 es\n","592 es\n","593 es\n","594 es\n","595 es\n","596 es\n","597 es\n","598 es\n","599 es\n","600 es\n","601 es\n","602 es\n","603 es\n","604 es\n","605 es\n","606 es\n","607 es\n","608 es\n","609 es\n","610 es\n","611 es\n","612 es\n","613 es\n","614 es\n","615 es\n","616 es\n","617 es\n","618 es\n","619 es\n","620 es\n","621 es\n","622 es\n","623 es\n","624 es\n","625 es\n","626 es\n","627 es\n","628 es\n","629 es\n","630 es\n","631 es\n","632 es\n","633 es\n","634 es\n","635 es\n","636 es\n","637 es\n","638 es\n","639 es\n","640 es\n","641 es\n","642 es\n","643 es\n","644 es\n","645 es\n","646 es\n","647 es\n","648 es\n","649 es\n","650 es\n","651 es\n","652 es\n","653 es\n","654 es\n","655 es\n","656 es\n","657 es\n","658 es\n","659 es\n","660 es\n","661 es\n","662 es\n","663 es\n","664 es\n","665 es\n","666 es\n","667 es\n","668 es\n","669 es\n","670 es\n","671 es\n","672 es\n","673 es\n","674 es\n","675 es\n","676 es\n","677 es\n","678 es\n","679 es\n","680 es\n","681 es\n","682 es\n","683 es\n","684 es\n","685 es\n","686 es\n","687 es\n","688 es\n","689 es\n","690 es\n","691 es\n","692 es\n","693 es\n","694 es\n","695 es\n","696 es\n","697 es\n","698 es\n","699 es\n","700 es\n","701 es\n","702 es\n","703 es\n","704 es\n","705 es\n","706 es\n","707 es\n","708 es\n","709 es\n","710 es\n","711 es\n","712 es\n","713 es\n","714 es\n","715 es\n","716 es\n","717 es\n","718 es\n","719 es\n","720 es\n","721 es\n","722 es\n","723 es\n","724 es\n","725 es\n","726 es\n","727 es\n","728 es\n","729 es\n","730 es\n","731 es\n","732 es\n","733 es\n","734 es\n","735 es\n","736 es\n","737 es\n","738 es\n","739 es\n","740 es\n","741 es\n","742 es\n","743 es\n","744 es\n","745 es\n","746 es\n","747 es\n","748 es\n","749 es\n","750 es\n","751 es\n","752 es\n","753 es\n","754 es\n","755 es\n","756 es\n","757 es\n","758 es\n","759 es\n","760 es\n","761 es\n","762 es\n","763 es\n","764 es\n","765 es\n","766 es\n","767 es\n","768 es\n","769 es\n","770 es\n","771 es\n","772 es\n","773 es\n","774 es\n","775 es\n","776 es\n","777 es\n","778 es\n","779 es\n","780 es\n","781 es\n","782 es\n","783 es\n","784 es\n","785 es\n","786 es\n","787 es\n","788 es\n","789 es\n","790 es\n","791 es\n","792 es\n","793 es\n","794 es\n","795 es\n","796 es\n","797 es\n","798 es\n","799 es\n","800 es\n","801 es\n","802 es\n","803 es\n","804 es\n","805 es\n","806 es\n","807 es\n","808 es\n","809 es\n","810 es\n","811 es\n","812 es\n","813 es\n","814 es\n","815 es\n","816 es\n","817 es\n","818 es\n","819 es\n","820 es\n","821 es\n","822 es\n","823 es\n","824 es\n","825 es\n","826 es\n","827 es\n","828 es\n","829 es\n","830 es\n","831 es\n","832 es\n","833 es\n","834 es\n","835 es\n","836 es\n","837 es\n","838 es\n","839 es\n","840 es\n","841 es\n","842 es\n","843 es\n","844 es\n","845 es\n","846 es\n","847 es\n","848 es\n","849 es\n","850 es\n","851 es\n","852 es\n","853 es\n","854 es\n","855 es\n","856 es\n","857 es\n","858 es\n","859 es\n","860 es\n","861 es\n","862 es\n","863 es\n","864 es\n","865 es\n","866 es\n","867 es\n","868 es\n","869 es\n","870 es\n","871 es\n","872 es\n","873 es\n","874 es\n","875 es\n","876 es\n","877 es\n","878 es\n","879 es\n","880 es\n","881 es\n","882 es\n","883 es\n","884 es\n","885 es\n","886 es\n","887 es\n","888 es\n","889 es\n","890 es\n","891 es\n","892 es\n","893 es\n","894 es\n","895 es\n","896 es\n","897 es\n","898 es\n","899 es\n","900 es\n","901 es\n","902 es\n","903 es\n","904 es\n","905 es\n","906 es\n","907 es\n","908 es\n","909 es\n","910 es\n","911 es\n","912 es\n","913 es\n","914 es\n","915 es\n","916 es\n","917 es\n","918 es\n","919 es\n","920 es\n","921 es\n","922 es\n","923 es\n","924 es\n","925 es\n","926 es\n","927 es\n","928 es\n","929 es\n","930 es\n","931 es\n","932 es\n","933 es\n","934 es\n","935 es\n","936 es\n","937 es\n","938 es\n","939 es\n","940 es\n","941 es\n","942 es\n","943 es\n","944 es\n","945 es\n","946 es\n","947 es\n","948 es\n","949 es\n","950 es\n","951 es\n","952 es\n","953 es\n","954 es\n","955 es\n","956 es\n","957 es\n","958 es\n","959 es\n","960 es\n","961 es\n","962 es\n","963 es\n","964 es\n","965 es\n","966 es\n","967 es\n","968 es\n","969 es\n","970 es\n","971 es\n","972 es\n","973 es\n","974 es\n","975 es\n","976 es\n","977 es\n","978 es\n","979 es\n","980 es\n","981 es\n","982 es\n","983 es\n","984 es\n","985 es\n","986 es\n","987 es\n","988 es\n","989 es\n","990 es\n","991 es\n","992 es\n","993 es\n","994 es\n","995 es\n","996 es\n","997 es\n","998 es\n","999 es\n","1000 es\n","1001 es\n","1002 es\n","1003 es\n","1004 es\n","1005 es\n","1006 es\n","1007 es\n","1008 es\n","1009 es\n","1010 es\n","1011 es\n","1012 es\n","1013 es\n","1014 es\n","1015 es\n","1016 es\n","1017 es\n","1018 es\n","1019 es\n","1020 es\n","1021 es\n","1022 es\n","1023 es\n","1024 es\n","1025 es\n","1026 es\n","1027 es\n","1028 es\n","1029 es\n","1030 es\n","1031 es\n","1032 es\n","1033 es\n","1034 es\n","1035 es\n","1036 es\n","1037 es\n","1038 es\n","1039 es\n","1040 es\n","1041 es\n","1042 es\n","1043 es\n","1044 es\n","1045 es\n","1046 es\n","1047 es\n","1048 es\n","1049 es\n","1050 es\n","1051 es\n","1052 es\n","1053 es\n","1054 es\n","1055 es\n","1056 es\n","1057 es\n","1058 es\n","1059 es\n","1060 es\n","1061 es\n","1062 es\n","1063 es\n","1064 es\n","1065 es\n","1066 es\n","1067 es\n","1068 es\n","1069 es\n","1070 es\n","1071 es\n","1072 es\n","1073 es\n","1074 es\n","1075 es\n","1076 es\n","1077 es\n","1078 es\n","1079 es\n","1080 es\n","1081 es\n","1082 es\n","1083 es\n","1084 es\n","1085 es\n","1086 es\n","1087 es\n","1088 es\n","1089 es\n","1090 es\n","1091 es\n","1092 es\n","1093 es\n","1094 es\n","1095 es\n","1096 es\n","1097 es\n","1098 es\n","1099 es\n","1100 es\n","1101 es\n","1102 es\n","1103 es\n","1104 es\n","1105 es\n","1106 es\n","1107 es\n","1108 es\n","1109 es\n","1110 es\n","1111 es\n","1112 es\n","1113 es\n","1114 es\n","1115 es\n","1116 es\n","1117 es\n","1118 es\n","1119 es\n","1120 es\n","1121 es\n","1122 es\n","1123 es\n","1124 es\n","1125 es\n","1126 es\n","1127 es\n","1128 es\n","1129 es\n","1130 es\n","1131 es\n","1132 es\n","1133 es\n","1134 es\n","1135 es\n","1136 es\n","1137 es\n","1138 es\n","1139 es\n","1140 es\n","1141 es\n","1142 es\n","1143 es\n","1144 es\n","1145 es\n","1146 es\n","1147 es\n","1148 es\n","1149 es\n","1150 es\n","1151 es\n","1152 es\n","1153 es\n","1154 es\n","1155 es\n","1156 es\n","1157 es\n","1158 es\n","1159 es\n","1160 es\n","1161 es\n","1162 es\n","1163 es\n","1164 es\n","1165 es\n","1166 es\n","1167 es\n","1168 es\n","1169 es\n","1170 es\n","1171 es\n","1172 es\n","1173 es\n","1174 es\n","1175 es\n","1176 es\n","1177 es\n","1178 es\n","1179 es\n","1180 es\n","1181 es\n","1182 es\n","1183 es\n","1184 es\n","1185 es\n","1186 es\n","1187 es\n","1188 es\n","1189 es\n","1190 es\n","1191 es\n","1192 es\n","1193 es\n","1194 es\n","1195 es\n","1196 es\n","1197 es\n","1198 es\n","1199 es\n","1200 es\n","1201 es\n","1202 es\n","1203 es\n","1204 es\n","1205 es\n","1206 es\n","1207 es\n","1208 es\n","1209 es\n","1210 es\n","1211 es\n","1212 es\n","1213 es\n","1214 es\n","1215 es\n","1216 es\n","1217 es\n","1218 es\n","1219 es\n","1220 es\n","1221 es\n","1222 es\n","1223 es\n","1224 es\n","1225 es\n","1226 es\n","1227 es\n","1228 es\n","1229 es\n","1230 es\n","1231 es\n","1232 es\n","1233 es\n","1234 es\n","1235 es\n","1236 es\n","1237 es\n","1238 es\n","1239 es\n","1240 es\n","1241 es\n","1242 es\n","1243 es\n","1244 es\n","1245 es\n","1246 es\n","1247 es\n","1248 es\n","1249 es\n","1250 es\n","1251 es\n","1252 es\n","1253 es\n","1254 es\n","1255 es\n","1256 es\n","1257 es\n","1258 es\n","1259 es\n","1260 es\n","1261 es\n","1262 es\n","1263 es\n","1264 es\n","1265 es\n","1266 es\n","1267 es\n","1268 es\n","1269 es\n","1270 es\n","1271 es\n","1272 es\n","1273 es\n","1274 es\n","1275 es\n","1276 es\n","1277 es\n","1278 es\n","1279 es\n","1280 es\n","1281 es\n","1282 es\n","1283 es\n","1284 es\n","1285 es\n","1286 es\n","1287 es\n","1288 es\n","1289 es\n","1290 es\n","1291 es\n","1292 es\n","1293 es\n","1294 es\n","1295 es\n","1296 es\n","1297 es\n","1298 es\n","1299 es\n","1300 es\n","1301 es\n","1302 es\n","1303 es\n","1304 es\n","1305 es\n","1306 es\n","1307 es\n","1308 es\n","1309 es\n","1310 es\n","1311 es\n","1312 es\n","1313 es\n","1314 es\n","1315 es\n","1316 es\n","1317 es\n","1318 es\n","1319 es\n","1320 es\n","1321 es\n","1322 es\n","1323 es\n","1324 es\n","1325 es\n","1326 es\n","1327 es\n","1328 es\n","1329 es\n","1330 es\n","1331 es\n","1332 es\n","1333 es\n","1334 es\n","1335 es\n","1336 es\n","1337 es\n","1338 es\n","1339 es\n","1340 es\n","1341 es\n","1342 es\n","1343 es\n","1344 es\n","1345 es\n","1346 es\n","1347 es\n","1348 es\n","1349 es\n","1350 es\n","1351 es\n","1352 es\n","1353 es\n","1354 es\n","1355 es\n","1356 es\n","1357 es\n","1358 es\n","1359 es\n","1360 es\n","1361 es\n","1362 es\n","1363 es\n","1364 es\n","1365 es\n","1366 es\n","1367 es\n","1368 es\n","1369 es\n","1370 es\n","1371 es\n","1372 es\n","1373 es\n","1374 es\n","1375 es\n","1376 es\n","1377 es\n","1378 es\n","1379 es\n","1380 es\n","1381 es\n","1382 es\n","1383 es\n","1384 es\n","1385 es\n","1386 es\n","1387 es\n","1388 es\n","1389 es\n","1390 es\n","1391 es\n","1392 es\n","1393 es\n","1394 es\n","1395 es\n","1396 es\n","1397 es\n","1398 es\n","1399 es\n","1400 es\n","1401 es\n","1402 es\n","1403 es\n","1404 es\n","1405 es\n","1406 es\n","1407 es\n","1408 es\n","1409 es\n","1410 es\n","1411 es\n","1412 es\n","1413 es\n","1414 es\n","1415 es\n","1416 es\n","1417 es\n","1418 es\n","1419 es\n","1420 es\n","1421 es\n","1422 es\n","1423 es\n","1424 es\n","1425 es\n","1426 es\n","1427 es\n","1428 es\n","1429 es\n","1430 es\n","1431 es\n","1432 es\n","1433 es\n","1434 es\n","1435 es\n","1436 es\n","1437 es\n","1438 es\n","1439 es\n","1440 es\n","1441 es\n","1442 es\n","1443 es\n","1444 es\n","1445 es\n","1446 es\n","1447 es\n","1448 es\n","1449 es\n","1450 es\n","1451 es\n","1452 es\n","1453 es\n","1454 es\n","1455 es\n","1456 es\n","1457 es\n","1458 es\n","1459 es\n","1460 es\n","1461 es\n","1462 es\n","1463 es\n","1464 es\n","1465 es\n","1466 es\n","1467 es\n","1468 es\n","1469 es\n","1470 es\n","1471 es\n","1472 es\n","1473 es\n","1474 es\n","1475 es\n","1476 es\n","1477 es\n","1478 es\n","1479 es\n","1480 es\n","1481 es\n","1482 es\n","1483 es\n","1484 es\n","1485 es\n","1486 es\n","1487 es\n","1488 es\n","1489 es\n","1490 es\n","1491 es\n","1492 es\n","1493 es\n","1494 es\n","1495 es\n","1496 es\n","1497 es\n","1498 es\n","1499 es\n","1500 es\n","1501 es\n","1502 es\n","1503 es\n","1504 es\n","1505 es\n","1506 es\n","1507 es\n","1508 es\n","1509 es\n","1510 es\n","1511 es\n","1512 es\n","1513 es\n","1514 es\n","1515 es\n","1516 es\n","1517 es\n","1518 es\n","1519 es\n","1520 es\n","1521 es\n","1522 es\n","1523 es\n","1524 es\n","1525 es\n","1526 es\n","1527 es\n","1528 es\n","1529 es\n","1530 es\n","1531 es\n","1532 es\n","1533 es\n","1534 es\n","1535 es\n","1536 es\n","1537 es\n","1538 es\n","1539 es\n","1540 es\n","1541 es\n","1542 es\n","1543 es\n","1544 es\n","1545 es\n","1546 es\n","1547 es\n","1548 es\n","1549 es\n","1550 es\n","1551 es\n","1552 es\n","1553 es\n","1554 es\n","1555 es\n","1556 es\n","1557 es\n","1558 es\n","1559 es\n","1560 es\n","1561 es\n","1562 es\n","1563 es\n","1564 es\n","1565 es\n","1566 es\n","1567 es\n","1568 es\n","1569 es\n","1570 es\n","1571 es\n","1572 es\n","1573 es\n","1574 es\n","1575 es\n","1576 es\n","1577 es\n","1578 es\n","1579 es\n","1580 es\n","1581 es\n","1582 es\n","1583 es\n","1584 es\n","1585 es\n","1586 es\n","1587 es\n","1588 es\n","1589 es\n","1590 es\n","1591 es\n","1592 es\n","1593 es\n","1594 es\n","1595 es\n","1596 es\n","1597 es\n","1598 es\n","1599 es\n","1600 es\n","1601 es\n","1602 es\n","1603 es\n","1604 es\n","1605 es\n","1606 es\n","1607 es\n","1608 es\n","1609 es\n","1610 es\n","1611 es\n","1612 es\n","1613 es\n","1614 es\n","1615 es\n","1616 es\n","1617 es\n","1618 es\n","1619 es\n","1620 es\n","1621 es\n","1622 es\n","1623 es\n","1624 es\n","1625 es\n","1626 es\n","1627 es\n","1628 es\n","1629 es\n","1630 es\n","1631 es\n","1632 es\n","1633 es\n","1634 es\n","1635 es\n","1636 es\n","1637 es\n","1638 es\n","1639 es\n","1640 es\n","1641 es\n","1642 es\n","1643 es\n","1644 es\n","1645 es\n","1646 es\n","1647 es\n","1648 es\n","1649 es\n","1650 es\n","1651 es\n","1652 es\n","1653 es\n","1654 es\n","1655 es\n","1656 es\n","1657 es\n","1658 es\n","1659 es\n","1660 es\n","1661 es\n","1662 es\n","1663 es\n","1664 es\n","1665 es\n","1666 es\n","1667 es\n","1668 es\n","1669 es\n","1670 es\n","1671 es\n","1672 es\n","1673 es\n","1674 es\n","1675 es\n","1676 es\n","1677 es\n","1678 es\n","1679 es\n","1680 es\n","1681 es\n","1682 es\n","1683 es\n","1684 es\n","1685 es\n","1686 es\n","1687 es\n","1688 es\n","1689 es\n","1690 es\n","1691 es\n","1692 es\n","1693 es\n","1694 es\n","1695 es\n","1696 es\n","1697 es\n","1698 es\n","1699 es\n","1700 es\n","1701 es\n","1702 es\n","1703 es\n","1704 es\n","1705 es\n","1706 es\n","1707 es\n","1708 es\n","1709 es\n","1710 es\n","1711 es\n","1712 es\n","1713 es\n","1714 es\n","1715 es\n","1716 es\n","1717 es\n","1718 es\n","1719 es\n","1720 es\n","1721 es\n","1722 es\n","1723 es\n","1724 es\n","1725 es\n","1726 es\n","1727 es\n","1728 es\n","1729 es\n","1730 es\n","1731 es\n","1732 es\n","1733 es\n","1734 es\n","1735 es\n","1736 es\n","1737 es\n","1738 es\n","1739 es\n","1740 es\n","1741 es\n","1742 es\n","1743 es\n","1744 es\n","1745 es\n","1746 es\n","1747 es\n","1748 es\n","1749 es\n","1750 es\n","1751 es\n","1752 es\n","1753 es\n","1754 es\n","1755 es\n","1756 es\n","1757 es\n","1758 es\n","1759 es\n","1760 es\n","1761 es\n","1762 es\n","1763 es\n","1764 es\n","1765 es\n","1766 es\n","1767 es\n","1768 es\n","1769 es\n","1770 es\n","1771 es\n","1772 es\n","1773 es\n","1774 es\n","1775 es\n","1776 es\n","1777 es\n","1778 es\n","1779 es\n","1780 es\n","1781 es\n","1782 es\n","1783 es\n","1784 es\n","1785 es\n","1786 es\n","1787 es\n","1788 es\n","1789 es\n","1790 es\n","1791 es\n","1792 es\n","1793 es\n","1794 es\n","1795 es\n","1796 es\n","1797 es\n","1798 es\n","1799 es\n","1800 es\n","1801 es\n","1802 es\n","1803 es\n","1804 es\n","1805 es\n","1806 es\n","1807 es\n","1808 es\n","1809 es\n","1810 es\n","1811 es\n","1812 es\n","1813 es\n","1814 es\n","1815 es\n","1816 es\n","1817 es\n","1818 es\n","1819 es\n","1820 es\n","1821 es\n","1822 es\n","1823 es\n","1824 es\n","1825 es\n","1826 es\n","1827 es\n","1828 es\n","1829 es\n","1830 es\n","1831 es\n","1832 es\n","1833 es\n","1834 es\n","1835 es\n","1836 es\n","1837 es\n","1838 es\n","1839 es\n","1840 es\n","1841 es\n","1842 es\n","1843 es\n","1844 es\n","1845 es\n","1846 es\n","1847 es\n","1848 es\n","1849 es\n","1850 es\n","1851 es\n","1852 es\n","1853 es\n","1854 es\n","1855 es\n","1856 es\n","1857 es\n","1858 es\n","1859 es\n","1860 es\n","1861 es\n","1862 es\n","1863 es\n","1864 es\n","1865 es\n","1866 es\n","1867 es\n","1868 es\n","1869 es\n","1870 es\n","1871 es\n","1872 es\n","1873 es\n","1874 es\n","1875 es\n","1876 es\n","1877 es\n","1878 es\n","1879 es\n","1880 es\n","1881 es\n","1882 es\n","1883 es\n","1884 es\n","1885 es\n","1886 es\n","1887 es\n","1888 es\n","1889 es\n","1890 es\n","1891 es\n","1892 es\n","1893 es\n","1894 es\n","1895 es\n","1896 es\n","1897 es\n","1898 es\n","1899 es\n","1900 es\n","1901 es\n","1902 es\n","1903 es\n","1904 es\n","1905 es\n","1906 es\n","1907 es\n","1908 es\n","1909 es\n","1910 es\n","1911 es\n","1912 es\n","1913 es\n","1914 es\n","1915 es\n","1916 es\n","1917 es\n","1918 es\n","1919 es\n","1920 es\n","1921 es\n","1922 es\n","1923 es\n","1924 es\n","1925 es\n","1926 es\n","1927 es\n","1928 es\n","1929 es\n","1930 es\n","1931 es\n","1932 es\n","1933 es\n","1934 es\n","1935 es\n","1936 es\n","1937 es\n","1938 es\n","1939 es\n","1940 es\n","1941 es\n","1942 es\n","1943 es\n","1944 es\n","1945 es\n","1946 es\n","1947 es\n","1948 es\n","1949 es\n","1950 es\n","1951 es\n","1952 es\n","1953 es\n","1954 es\n","1955 es\n","1956 es\n","1957 es\n","1958 es\n","1959 es\n","1960 es\n","1961 es\n","1962 es\n","1963 es\n","1964 es\n","1965 es\n","1966 es\n","1967 es\n","1968 es\n","1969 es\n","1970 es\n","1971 es\n","1972 es\n","1973 es\n","1974 es\n","1975 es\n","1976 es\n","1977 es\n","1978 es\n","1979 es\n","1980 es\n","1981 es\n","1982 es\n","1983 es\n","1984 es\n","1985 es\n","1986 es\n","1987 es\n","1988 es\n","1989 es\n","1990 es\n","1991 es\n","1992 es\n","1993 es\n","1994 es\n","1995 es\n","1996 es\n","1997 es\n","1998 es\n","1999 es\n","es\n","0 hi\n","1 hi\n","2 hi\n","3 hi\n","4 hi\n","5 hi\n","6 hi\n","7 hi\n","8 hi\n","9 hi\n","10 hi\n","11 hi\n","12 hi\n","13 hi\n","14 hi\n","15 hi\n","16 hi\n","17 hi\n","18 hi\n","19 hi\n","20 hi\n","21 hi\n","22 hi\n","23 hi\n","24 hi\n","25 hi\n","26 hi\n","27 hi\n","28 hi\n","29 hi\n","30 hi\n","31 hi\n","32 hi\n","33 hi\n","34 hi\n","35 hi\n","36 hi\n","37 hi\n","38 hi\n","39 hi\n","40 hi\n","41 hi\n","42 hi\n","43 hi\n","44 hi\n","45 hi\n","46 hi\n","47 hi\n","48 hi\n","49 hi\n","50 hi\n","51 hi\n","52 hi\n","53 hi\n","54 hi\n","55 hi\n","56 hi\n","57 hi\n","58 hi\n","59 hi\n","60 hi\n","61 hi\n","62 hi\n","63 hi\n","64 hi\n","65 hi\n","66 hi\n","67 hi\n","68 hi\n","69 hi\n","70 hi\n","71 hi\n","72 hi\n","73 hi\n","74 hi\n","75 hi\n","76 hi\n","77 hi\n","78 hi\n","79 hi\n","80 hi\n","81 hi\n","82 hi\n","83 hi\n","84 hi\n","85 hi\n","86 hi\n","87 hi\n","88 hi\n","89 hi\n","90 hi\n","91 hi\n","92 hi\n","93 hi\n","94 hi\n","95 hi\n","96 hi\n","97 hi\n","98 hi\n","99 hi\n","100 hi\n","101 hi\n","102 hi\n","103 hi\n","104 hi\n","105 hi\n","106 hi\n","107 hi\n","108 hi\n","109 hi\n","110 hi\n","111 hi\n","112 hi\n","113 hi\n","114 hi\n","115 hi\n","116 hi\n","117 hi\n","118 hi\n","119 hi\n","120 hi\n","121 hi\n","122 hi\n","123 hi\n","124 hi\n","125 hi\n","126 hi\n","127 hi\n","128 hi\n","129 hi\n","130 hi\n","131 hi\n","132 hi\n","133 hi\n","134 hi\n","135 hi\n","136 hi\n","137 hi\n","138 hi\n","139 hi\n","140 hi\n","141 hi\n","142 hi\n","143 hi\n","144 hi\n","145 hi\n","146 hi\n","147 hi\n","148 hi\n","149 hi\n","150 hi\n","151 hi\n","152 hi\n","153 hi\n","154 hi\n","155 hi\n","156 hi\n","157 hi\n","158 hi\n","159 hi\n","160 hi\n","161 hi\n","162 hi\n","163 hi\n","164 hi\n","165 hi\n","166 hi\n","167 hi\n","168 hi\n","169 hi\n","170 hi\n","171 hi\n","172 hi\n","173 hi\n","174 hi\n","175 hi\n","176 hi\n","177 hi\n","178 hi\n","179 hi\n","180 hi\n","181 hi\n","182 hi\n","183 hi\n","184 hi\n","185 hi\n","186 hi\n","187 hi\n","188 hi\n","189 hi\n","190 hi\n","191 hi\n","192 hi\n","193 hi\n","194 hi\n","195 hi\n","196 hi\n","197 hi\n","198 hi\n","199 hi\n","200 hi\n","201 hi\n","202 hi\n","203 hi\n","204 hi\n","205 hi\n","206 hi\n","207 hi\n","208 hi\n","209 hi\n","210 hi\n","211 hi\n","212 hi\n","213 hi\n","214 hi\n","215 hi\n","216 hi\n","217 hi\n","218 hi\n","219 hi\n","220 hi\n","221 hi\n","222 hi\n","223 hi\n","224 hi\n","225 hi\n","226 hi\n","227 hi\n","228 hi\n","229 hi\n","230 hi\n","231 hi\n","232 hi\n","233 hi\n","234 hi\n","235 hi\n","236 hi\n","237 hi\n","238 hi\n","239 hi\n","240 hi\n","241 hi\n","242 hi\n","243 hi\n","244 hi\n","245 hi\n","246 hi\n","247 hi\n","248 hi\n","249 hi\n","250 hi\n","251 hi\n","252 hi\n","253 hi\n","254 hi\n","255 hi\n","256 hi\n","257 hi\n","258 hi\n","259 hi\n","260 hi\n","261 hi\n","262 hi\n","263 hi\n","264 hi\n","265 hi\n","266 hi\n","267 hi\n","268 hi\n","269 hi\n","270 hi\n","271 hi\n","272 hi\n","273 hi\n","274 hi\n","275 hi\n","276 hi\n","277 hi\n","278 hi\n","279 hi\n","280 hi\n","281 hi\n","282 hi\n","283 hi\n","284 hi\n","285 hi\n","286 hi\n","287 hi\n","288 hi\n","289 hi\n","290 hi\n","291 hi\n","292 hi\n","293 hi\n","294 hi\n","295 hi\n","296 hi\n","297 hi\n","298 hi\n","299 hi\n","300 hi\n","301 hi\n","302 hi\n","303 hi\n","304 hi\n","305 hi\n","306 hi\n","307 hi\n","308 hi\n","309 hi\n","310 hi\n","311 hi\n","312 hi\n","313 hi\n","314 hi\n","315 hi\n","316 hi\n","317 hi\n","318 hi\n","319 hi\n","320 hi\n","321 hi\n","322 hi\n","323 hi\n","324 hi\n","325 hi\n","326 hi\n","327 hi\n","328 hi\n","329 hi\n","330 hi\n","331 hi\n","332 hi\n","333 hi\n","334 hi\n","335 hi\n","336 hi\n","337 hi\n","338 hi\n","339 hi\n","340 hi\n","341 hi\n","342 hi\n","343 hi\n","344 hi\n","345 hi\n","346 hi\n","347 hi\n","348 hi\n","349 hi\n","350 hi\n","351 hi\n","352 hi\n","353 hi\n","354 hi\n","355 hi\n","356 hi\n","357 hi\n","358 hi\n","359 hi\n","360 hi\n","361 hi\n","362 hi\n","363 hi\n","364 hi\n","365 hi\n","366 hi\n","367 hi\n","368 hi\n","369 hi\n","370 hi\n","371 hi\n","372 hi\n","373 hi\n","374 hi\n","375 hi\n","376 hi\n","377 hi\n","378 hi\n","379 hi\n","380 hi\n","381 hi\n","382 hi\n","383 hi\n","384 hi\n","385 hi\n","386 hi\n","387 hi\n","388 hi\n","389 hi\n","390 hi\n","391 hi\n","392 hi\n","393 hi\n","394 hi\n","395 hi\n","396 hi\n","397 hi\n","398 hi\n","399 hi\n","400 hi\n","401 hi\n","402 hi\n","403 hi\n","404 hi\n","405 hi\n","406 hi\n","407 hi\n","408 hi\n","409 hi\n","410 hi\n","411 hi\n","412 hi\n","413 hi\n","414 hi\n","415 hi\n","416 hi\n","417 hi\n","418 hi\n","419 hi\n","420 hi\n","421 hi\n","422 hi\n","423 hi\n","424 hi\n","425 hi\n","426 hi\n","427 hi\n","428 hi\n","429 hi\n","430 hi\n","431 hi\n","432 hi\n","433 hi\n","434 hi\n","435 hi\n","436 hi\n","437 hi\n","438 hi\n","439 hi\n","440 hi\n","441 hi\n","442 hi\n","443 hi\n","444 hi\n","445 hi\n","446 hi\n","447 hi\n","448 hi\n","449 hi\n","450 hi\n","451 hi\n","452 hi\n","453 hi\n","454 hi\n","455 hi\n","456 hi\n","457 hi\n","458 hi\n","459 hi\n","460 hi\n","461 hi\n","462 hi\n","463 hi\n","464 hi\n","465 hi\n","466 hi\n","467 hi\n","468 hi\n","469 hi\n","470 hi\n","471 hi\n","472 hi\n","473 hi\n","474 hi\n","475 hi\n","476 hi\n","477 hi\n","478 hi\n","479 hi\n","480 hi\n","481 hi\n","482 hi\n","483 hi\n","484 hi\n","485 hi\n","486 hi\n","487 hi\n","488 hi\n","489 hi\n","490 hi\n","491 hi\n","492 hi\n","493 hi\n","494 hi\n","495 hi\n","496 hi\n","497 hi\n","498 hi\n","499 hi\n","500 hi\n","501 hi\n","502 hi\n","503 hi\n","504 hi\n","505 hi\n","506 hi\n","507 hi\n","508 hi\n","509 hi\n","510 hi\n","511 hi\n","512 hi\n","513 hi\n","514 hi\n","515 hi\n","516 hi\n","517 hi\n","518 hi\n","519 hi\n","520 hi\n","521 hi\n","522 hi\n","523 hi\n","524 hi\n","525 hi\n","526 hi\n","527 hi\n","528 hi\n","529 hi\n","530 hi\n","531 hi\n","532 hi\n","533 hi\n","534 hi\n","535 hi\n","536 hi\n","537 hi\n","538 hi\n","539 hi\n","540 hi\n","541 hi\n","542 hi\n","543 hi\n","544 hi\n","545 hi\n","546 hi\n","547 hi\n","548 hi\n","549 hi\n","550 hi\n","551 hi\n","552 hi\n","553 hi\n","554 hi\n","555 hi\n","556 hi\n","557 hi\n","558 hi\n","559 hi\n","560 hi\n","561 hi\n","562 hi\n","563 hi\n","564 hi\n","565 hi\n","566 hi\n","567 hi\n","568 hi\n","569 hi\n","570 hi\n","571 hi\n","572 hi\n","573 hi\n","574 hi\n","575 hi\n","576 hi\n","577 hi\n","578 hi\n","579 hi\n","580 hi\n","581 hi\n","582 hi\n","583 hi\n","584 hi\n","585 hi\n","586 hi\n","587 hi\n","588 hi\n","589 hi\n","590 hi\n","591 hi\n","592 hi\n","593 hi\n","594 hi\n","595 hi\n","596 hi\n","597 hi\n","598 hi\n","599 hi\n","600 hi\n","601 hi\n","602 hi\n","603 hi\n","604 hi\n","605 hi\n","606 hi\n","607 hi\n","608 hi\n","609 hi\n","610 hi\n","611 hi\n","612 hi\n","613 hi\n","614 hi\n","615 hi\n","616 hi\n","617 hi\n","618 hi\n","619 hi\n","620 hi\n","621 hi\n","622 hi\n","623 hi\n","624 hi\n","625 hi\n","626 hi\n","627 hi\n","628 hi\n","629 hi\n","630 hi\n","631 hi\n","632 hi\n","633 hi\n","634 hi\n","635 hi\n","636 hi\n","637 hi\n","638 hi\n","639 hi\n","640 hi\n","641 hi\n","642 hi\n","643 hi\n","644 hi\n","645 hi\n","646 hi\n","647 hi\n","648 hi\n","649 hi\n","650 hi\n","651 hi\n","652 hi\n","653 hi\n","654 hi\n","655 hi\n","656 hi\n","657 hi\n","658 hi\n","659 hi\n","660 hi\n","661 hi\n","662 hi\n","663 hi\n","664 hi\n","665 hi\n","666 hi\n","667 hi\n","668 hi\n","669 hi\n","670 hi\n","671 hi\n","672 hi\n","673 hi\n","674 hi\n","675 hi\n","676 hi\n","677 hi\n","678 hi\n","679 hi\n","680 hi\n","681 hi\n","682 hi\n","683 hi\n","684 hi\n","685 hi\n","686 hi\n","687 hi\n","688 hi\n","689 hi\n","690 hi\n","691 hi\n","692 hi\n","693 hi\n","694 hi\n","695 hi\n","696 hi\n","697 hi\n","698 hi\n","699 hi\n","700 hi\n","701 hi\n","702 hi\n","703 hi\n","704 hi\n","705 hi\n","706 hi\n","707 hi\n","708 hi\n","709 hi\n","710 hi\n","711 hi\n","712 hi\n","713 hi\n","714 hi\n","715 hi\n","716 hi\n","717 hi\n","718 hi\n","719 hi\n","720 hi\n","721 hi\n","722 hi\n","723 hi\n","724 hi\n","725 hi\n","726 hi\n","727 hi\n","728 hi\n","729 hi\n","730 hi\n","731 hi\n","732 hi\n","733 hi\n","734 hi\n","735 hi\n","736 hi\n","737 hi\n","738 hi\n","739 hi\n","740 hi\n","741 hi\n","742 hi\n","743 hi\n","744 hi\n","745 hi\n","746 hi\n","747 hi\n","748 hi\n","749 hi\n","750 hi\n","751 hi\n","752 hi\n","753 hi\n","754 hi\n","755 hi\n","756 hi\n","757 hi\n","758 hi\n","759 hi\n","760 hi\n","761 hi\n","762 hi\n","763 hi\n","764 hi\n","765 hi\n","766 hi\n","767 hi\n","768 hi\n","769 hi\n","770 hi\n","771 hi\n","772 hi\n","773 hi\n","774 hi\n","775 hi\n","776 hi\n","777 hi\n","778 hi\n","779 hi\n","780 hi\n","781 hi\n","782 hi\n","783 hi\n","784 hi\n","785 hi\n","786 hi\n","787 hi\n","788 hi\n","789 hi\n","790 hi\n","791 hi\n","792 hi\n","793 hi\n","794 hi\n","795 hi\n","796 hi\n","797 hi\n","798 hi\n","799 hi\n","800 hi\n","801 hi\n","802 hi\n","803 hi\n","804 hi\n","805 hi\n","806 hi\n","807 hi\n","808 hi\n","809 hi\n","810 hi\n","811 hi\n","812 hi\n","813 hi\n","814 hi\n","815 hi\n","816 hi\n","817 hi\n","818 hi\n","819 hi\n","820 hi\n","821 hi\n","822 hi\n","823 hi\n","824 hi\n","825 hi\n","826 hi\n","827 hi\n","828 hi\n","829 hi\n","830 hi\n","831 hi\n","832 hi\n","833 hi\n","834 hi\n","835 hi\n","836 hi\n","837 hi\n","838 hi\n","839 hi\n","840 hi\n","841 hi\n","842 hi\n","843 hi\n","844 hi\n","845 hi\n","846 hi\n","847 hi\n","848 hi\n","849 hi\n","850 hi\n","851 hi\n","852 hi\n","853 hi\n","854 hi\n","855 hi\n","856 hi\n","857 hi\n","858 hi\n","859 hi\n","860 hi\n","861 hi\n","862 hi\n","863 hi\n","864 hi\n","865 hi\n","866 hi\n","867 hi\n","868 hi\n","869 hi\n","870 hi\n","871 hi\n","872 hi\n","873 hi\n","874 hi\n","875 hi\n","876 hi\n","877 hi\n","878 hi\n","879 hi\n","880 hi\n","881 hi\n","882 hi\n","883 hi\n","884 hi\n","885 hi\n","886 hi\n","887 hi\n","888 hi\n","889 hi\n","890 hi\n","891 hi\n","892 hi\n","893 hi\n","894 hi\n","895 hi\n","896 hi\n","897 hi\n","898 hi\n","899 hi\n","900 hi\n","901 hi\n","902 hi\n","903 hi\n","904 hi\n","905 hi\n","906 hi\n","907 hi\n","908 hi\n","909 hi\n","910 hi\n","911 hi\n","912 hi\n","913 hi\n","914 hi\n","915 hi\n","916 hi\n","917 hi\n","918 hi\n","919 hi\n","920 hi\n","921 hi\n","922 hi\n","923 hi\n","924 hi\n","925 hi\n","926 hi\n","927 hi\n","928 hi\n","929 hi\n","930 hi\n","931 hi\n","932 hi\n","933 hi\n","934 hi\n","935 hi\n","936 hi\n","937 hi\n","938 hi\n","939 hi\n","940 hi\n","941 hi\n","942 hi\n","943 hi\n","944 hi\n","945 hi\n","946 hi\n","947 hi\n","948 hi\n","949 hi\n","950 hi\n","951 hi\n","952 hi\n","953 hi\n","954 hi\n","955 hi\n","956 hi\n","957 hi\n","958 hi\n","959 hi\n","960 hi\n","961 hi\n","962 hi\n","963 hi\n","964 hi\n","965 hi\n","966 hi\n","967 hi\n","968 hi\n","969 hi\n","970 hi\n","971 hi\n","972 hi\n","973 hi\n","974 hi\n","975 hi\n","976 hi\n","977 hi\n","978 hi\n","979 hi\n","980 hi\n","981 hi\n","982 hi\n","983 hi\n","984 hi\n","985 hi\n","986 hi\n","987 hi\n","988 hi\n","989 hi\n","990 hi\n","991 hi\n","992 hi\n","993 hi\n","994 hi\n","995 hi\n","996 hi\n","997 hi\n","998 hi\n","999 hi\n","1000 hi\n","1001 hi\n","1002 hi\n","1003 hi\n","1004 hi\n","1005 hi\n","1006 hi\n","1007 hi\n","1008 hi\n","1009 hi\n","1010 hi\n","1011 hi\n","1012 hi\n","1013 hi\n","1014 hi\n","1015 hi\n","1016 hi\n","1017 hi\n","1018 hi\n","1019 hi\n","1020 hi\n","1021 hi\n","1022 hi\n","1023 hi\n","1024 hi\n","1025 hi\n","1026 hi\n","1027 hi\n","1028 hi\n","1029 hi\n","1030 hi\n","1031 hi\n","1032 hi\n","1033 hi\n","1034 hi\n","1035 hi\n","1036 hi\n","1037 hi\n","1038 hi\n","1039 hi\n","1040 hi\n","1041 hi\n","1042 hi\n","1043 hi\n","1044 hi\n","1045 hi\n","1046 hi\n","1047 hi\n","1048 hi\n","1049 hi\n","1050 hi\n","1051 hi\n","1052 hi\n","1053 hi\n","1054 hi\n","1055 hi\n","1056 hi\n","1057 hi\n","1058 hi\n","1059 hi\n","1060 hi\n","1061 hi\n","1062 hi\n","1063 hi\n","1064 hi\n","1065 hi\n","1066 hi\n","1067 hi\n","1068 hi\n","1069 hi\n","1070 hi\n","1071 hi\n","1072 hi\n","1073 hi\n","1074 hi\n","1075 hi\n","1076 hi\n","1077 hi\n","1078 hi\n","1079 hi\n","1080 hi\n","1081 hi\n","1082 hi\n","1083 hi\n","1084 hi\n","1085 hi\n","1086 hi\n","1087 hi\n","1088 hi\n","1089 hi\n","1090 hi\n","1091 hi\n","1092 hi\n","1093 hi\n","1094 hi\n","1095 hi\n","1096 hi\n","1097 hi\n","1098 hi\n","1099 hi\n","1100 hi\n","1101 hi\n","1102 hi\n","1103 hi\n","1104 hi\n","1105 hi\n","1106 hi\n","1107 hi\n","1108 hi\n","1109 hi\n","1110 hi\n","1111 hi\n","1112 hi\n","1113 hi\n","1114 hi\n","1115 hi\n","1116 hi\n","1117 hi\n","1118 hi\n","1119 hi\n","1120 hi\n","1121 hi\n","1122 hi\n","1123 hi\n","1124 hi\n","1125 hi\n","1126 hi\n","1127 hi\n","1128 hi\n","1129 hi\n","1130 hi\n","1131 hi\n","1132 hi\n","1133 hi\n","1134 hi\n","1135 hi\n","1136 hi\n","1137 hi\n","1138 hi\n","1139 hi\n","1140 hi\n","1141 hi\n","1142 hi\n","1143 hi\n","1144 hi\n","1145 hi\n","1146 hi\n","1147 hi\n","1148 hi\n","1149 hi\n","1150 hi\n","1151 hi\n","1152 hi\n","1153 hi\n","1154 hi\n","1155 hi\n","1156 hi\n","1157 hi\n","1158 hi\n","1159 hi\n","1160 hi\n","1161 hi\n","1162 hi\n","1163 hi\n","1164 hi\n","1165 hi\n","1166 hi\n","1167 hi\n","1168 hi\n","1169 hi\n","1170 hi\n","1171 hi\n","1172 hi\n","1173 hi\n","1174 hi\n","1175 hi\n","1176 hi\n","1177 hi\n","1178 hi\n","1179 hi\n","1180 hi\n","1181 hi\n","1182 hi\n","1183 hi\n","1184 hi\n","1185 hi\n","1186 hi\n","1187 hi\n","1188 hi\n","1189 hi\n","1190 hi\n","1191 hi\n","1192 hi\n","1193 hi\n","1194 hi\n","1195 hi\n","1196 hi\n","1197 hi\n","1198 hi\n","1199 hi\n","1200 hi\n","1201 hi\n","1202 hi\n","1203 hi\n","1204 hi\n","1205 hi\n","1206 hi\n","1207 hi\n","1208 hi\n","1209 hi\n","1210 hi\n","1211 hi\n","1212 hi\n","1213 hi\n","1214 hi\n","1215 hi\n","1216 hi\n","1217 hi\n","1218 hi\n","1219 hi\n","1220 hi\n","1221 hi\n","1222 hi\n","1223 hi\n","1224 hi\n","1225 hi\n","1226 hi\n","1227 hi\n","1228 hi\n","1229 hi\n","1230 hi\n","1231 hi\n","1232 hi\n","1233 hi\n","1234 hi\n","1235 hi\n","1236 hi\n","1237 hi\n","1238 hi\n","1239 hi\n","1240 hi\n","1241 hi\n","1242 hi\n","1243 hi\n","1244 hi\n","1245 hi\n","1246 hi\n","1247 hi\n","1248 hi\n","1249 hi\n","1250 hi\n","1251 hi\n","1252 hi\n","1253 hi\n","1254 hi\n","1255 hi\n","1256 hi\n","1257 hi\n","1258 hi\n","1259 hi\n","1260 hi\n","1261 hi\n","1262 hi\n","1263 hi\n","1264 hi\n","1265 hi\n","1266 hi\n","1267 hi\n","1268 hi\n","1269 hi\n","1270 hi\n","1271 hi\n","1272 hi\n","1273 hi\n","1274 hi\n","1275 hi\n","1276 hi\n","1277 hi\n","1278 hi\n","1279 hi\n","1280 hi\n","1281 hi\n","1282 hi\n","1283 hi\n","1284 hi\n","1285 hi\n","1286 hi\n","1287 hi\n","1288 hi\n","1289 hi\n","1290 hi\n","1291 hi\n","1292 hi\n","1293 hi\n","1294 hi\n","1295 hi\n","1296 hi\n","1297 hi\n","1298 hi\n","1299 hi\n","1300 hi\n","1301 hi\n","1302 hi\n","1303 hi\n","1304 hi\n","1305 hi\n","1306 hi\n","1307 hi\n","1308 hi\n","1309 hi\n","1310 hi\n","1311 hi\n","1312 hi\n","1313 hi\n","1314 hi\n","1315 hi\n","1316 hi\n","1317 hi\n","1318 hi\n","1319 hi\n","1320 hi\n","1321 hi\n","1322 hi\n","1323 hi\n","1324 hi\n","1325 hi\n","1326 hi\n","1327 hi\n","1328 hi\n","1329 hi\n","1330 hi\n","1331 hi\n","1332 hi\n","1333 hi\n","1334 hi\n","1335 hi\n","1336 hi\n","1337 hi\n","1338 hi\n","1339 hi\n","1340 hi\n","1341 hi\n","1342 hi\n","1343 hi\n","1344 hi\n","1345 hi\n","1346 hi\n","1347 hi\n","1348 hi\n","1349 hi\n","1350 hi\n","1351 hi\n","1352 hi\n","1353 hi\n","1354 hi\n","1355 hi\n","1356 hi\n","1357 hi\n","1358 hi\n","1359 hi\n","1360 hi\n","1361 hi\n","1362 hi\n","1363 hi\n","1364 hi\n","1365 hi\n","1366 hi\n","1367 hi\n","1368 hi\n","1369 hi\n","1370 hi\n","1371 hi\n","1372 hi\n","1373 hi\n","1374 hi\n","1375 hi\n","1376 hi\n","1377 hi\n","1378 hi\n","1379 hi\n","1380 hi\n","1381 hi\n","1382 hi\n","1383 hi\n","1384 hi\n","1385 hi\n","1386 hi\n","1387 hi\n","1388 hi\n","1389 hi\n","1390 hi\n","1391 hi\n","1392 hi\n","1393 hi\n","1394 hi\n","1395 hi\n","1396 hi\n","1397 hi\n","1398 hi\n","1399 hi\n","1400 hi\n","1401 hi\n","1402 hi\n","1403 hi\n","1404 hi\n","1405 hi\n","1406 hi\n","1407 hi\n","1408 hi\n","1409 hi\n","1410 hi\n","1411 hi\n","1412 hi\n","1413 hi\n","1414 hi\n","1415 hi\n","1416 hi\n","1417 hi\n","1418 hi\n","1419 hi\n","1420 hi\n","1421 hi\n","1422 hi\n","1423 hi\n","1424 hi\n","1425 hi\n","1426 hi\n","1427 hi\n","1428 hi\n","1429 hi\n","1430 hi\n","1431 hi\n","1432 hi\n","1433 hi\n","1434 hi\n","1435 hi\n","1436 hi\n","1437 hi\n","1438 hi\n","1439 hi\n","1440 hi\n","1441 hi\n","1442 hi\n","1443 hi\n","1444 hi\n","1445 hi\n","1446 hi\n","1447 hi\n","1448 hi\n","1449 hi\n","1450 hi\n","1451 hi\n","1452 hi\n","1453 hi\n","1454 hi\n","1455 hi\n","1456 hi\n","1457 hi\n","1458 hi\n","1459 hi\n","1460 hi\n","1461 hi\n","1462 hi\n","1463 hi\n","1464 hi\n","1465 hi\n","1466 hi\n","1467 hi\n","1468 hi\n","1469 hi\n","1470 hi\n","1471 hi\n","1472 hi\n","1473 hi\n","1474 hi\n","1475 hi\n","1476 hi\n","1477 hi\n","1478 hi\n","1479 hi\n","1480 hi\n","1481 hi\n","1482 hi\n","1483 hi\n","1484 hi\n","1485 hi\n","1486 hi\n","1487 hi\n","1488 hi\n","1489 hi\n","1490 hi\n","1491 hi\n","1492 hi\n","1493 hi\n","1494 hi\n","1495 hi\n","1496 hi\n","1497 hi\n","1498 hi\n","1499 hi\n","1500 hi\n","1501 hi\n","1502 hi\n","1503 hi\n","1504 hi\n","1505 hi\n","1506 hi\n","1507 hi\n","1508 hi\n","1509 hi\n","1510 hi\n","1511 hi\n","1512 hi\n","1513 hi\n","1514 hi\n","1515 hi\n","1516 hi\n","1517 hi\n","1518 hi\n","1519 hi\n","1520 hi\n","1521 hi\n","1522 hi\n","1523 hi\n","1524 hi\n","1525 hi\n","1526 hi\n","1527 hi\n","1528 hi\n","1529 hi\n","1530 hi\n","1531 hi\n","1532 hi\n","1533 hi\n","1534 hi\n","1535 hi\n","1536 hi\n","1537 hi\n","1538 hi\n","1539 hi\n","1540 hi\n","1541 hi\n","1542 hi\n","1543 hi\n","1544 hi\n","1545 hi\n","1546 hi\n","1547 hi\n","1548 hi\n","1549 hi\n","1550 hi\n","1551 hi\n","1552 hi\n","1553 hi\n","1554 hi\n","1555 hi\n","1556 hi\n","1557 hi\n","1558 hi\n","1559 hi\n","1560 hi\n","1561 hi\n","1562 hi\n","1563 hi\n","1564 hi\n","1565 hi\n","1566 hi\n","1567 hi\n","1568 hi\n","1569 hi\n","1570 hi\n","1571 hi\n","1572 hi\n","1573 hi\n","1574 hi\n","1575 hi\n","1576 hi\n","1577 hi\n","1578 hi\n","1579 hi\n","1580 hi\n","1581 hi\n","1582 hi\n","1583 hi\n","1584 hi\n","1585 hi\n","1586 hi\n","1587 hi\n","1588 hi\n","1589 hi\n","1590 hi\n","1591 hi\n","1592 hi\n","1593 hi\n","1594 hi\n","1595 hi\n","1596 hi\n","1597 hi\n","1598 hi\n","1599 hi\n","1600 hi\n","1601 hi\n","1602 hi\n","1603 hi\n","1604 hi\n","1605 hi\n","1606 hi\n","1607 hi\n","1608 hi\n","1609 hi\n","1610 hi\n","1611 hi\n","1612 hi\n","1613 hi\n","1614 hi\n","1615 hi\n","1616 hi\n","1617 hi\n","1618 hi\n","1619 hi\n","1620 hi\n","1621 hi\n","1622 hi\n","1623 hi\n","1624 hi\n","1625 hi\n","1626 hi\n","1627 hi\n","1628 hi\n","1629 hi\n","1630 hi\n","1631 hi\n","1632 hi\n","1633 hi\n","1634 hi\n","1635 hi\n","1636 hi\n","1637 hi\n","1638 hi\n","1639 hi\n","1640 hi\n","1641 hi\n","1642 hi\n","1643 hi\n","1644 hi\n","1645 hi\n","1646 hi\n","1647 hi\n","1648 hi\n","1649 hi\n","1650 hi\n","1651 hi\n","1652 hi\n","1653 hi\n","1654 hi\n","1655 hi\n","1656 hi\n","1657 hi\n","1658 hi\n","1659 hi\n","1660 hi\n","1661 hi\n","1662 hi\n","1663 hi\n","1664 hi\n","1665 hi\n","1666 hi\n","1667 hi\n","1668 hi\n","1669 hi\n","1670 hi\n","1671 hi\n","1672 hi\n","1673 hi\n","1674 hi\n","1675 hi\n","1676 hi\n","1677 hi\n","1678 hi\n","1679 hi\n","1680 hi\n","1681 hi\n","1682 hi\n","1683 hi\n","1684 hi\n","1685 hi\n","1686 hi\n","1687 hi\n","1688 hi\n","1689 hi\n","1690 hi\n","1691 hi\n","1692 hi\n","1693 hi\n","1694 hi\n","1695 hi\n","1696 hi\n","1697 hi\n","1698 hi\n","1699 hi\n","1700 hi\n","1701 hi\n","1702 hi\n","1703 hi\n","1704 hi\n","1705 hi\n","1706 hi\n","1707 hi\n","1708 hi\n","1709 hi\n","1710 hi\n","1711 hi\n","1712 hi\n","1713 hi\n","1714 hi\n","1715 hi\n","1716 hi\n","1717 hi\n","1718 hi\n","1719 hi\n","1720 hi\n","1721 hi\n","1722 hi\n","1723 hi\n","1724 hi\n","1725 hi\n","1726 hi\n","1727 hi\n","1728 hi\n","1729 hi\n","1730 hi\n","1731 hi\n","1732 hi\n","1733 hi\n","1734 hi\n","1735 hi\n","1736 hi\n","1737 hi\n","1738 hi\n","1739 hi\n","1740 hi\n","1741 hi\n","1742 hi\n","1743 hi\n","1744 hi\n","1745 hi\n","1746 hi\n","1747 hi\n","1748 hi\n","1749 hi\n","1750 hi\n","1751 hi\n","1752 hi\n","1753 hi\n","1754 hi\n","1755 hi\n","1756 hi\n","1757 hi\n","1758 hi\n","1759 hi\n","1760 hi\n","1761 hi\n","1762 hi\n","1763 hi\n","1764 hi\n","1765 hi\n","1766 hi\n","1767 hi\n","1768 hi\n","1769 hi\n","1770 hi\n","1771 hi\n","1772 hi\n","1773 hi\n","1774 hi\n","1775 hi\n","1776 hi\n","1777 hi\n","1778 hi\n","1779 hi\n","1780 hi\n","1781 hi\n","1782 hi\n","1783 hi\n","1784 hi\n","1785 hi\n","1786 hi\n","1787 hi\n","1788 hi\n","1789 hi\n","1790 hi\n","1791 hi\n","1792 hi\n","1793 hi\n","1794 hi\n","1795 hi\n","1796 hi\n","1797 hi\n","1798 hi\n","1799 hi\n","1800 hi\n","1801 hi\n","1802 hi\n","1803 hi\n","1804 hi\n","1805 hi\n","1806 hi\n","1807 hi\n","1808 hi\n","1809 hi\n","1810 hi\n","1811 hi\n","1812 hi\n","1813 hi\n","1814 hi\n","1815 hi\n","1816 hi\n","1817 hi\n","1818 hi\n","1819 hi\n","1820 hi\n","1821 hi\n","1822 hi\n","1823 hi\n","1824 hi\n","1825 hi\n","1826 hi\n","1827 hi\n","1828 hi\n","1829 hi\n","1830 hi\n","1831 hi\n","1832 hi\n","1833 hi\n","1834 hi\n","1835 hi\n","1836 hi\n","1837 hi\n","1838 hi\n","1839 hi\n","1840 hi\n","1841 hi\n","1842 hi\n","1843 hi\n","1844 hi\n","1845 hi\n","1846 hi\n","1847 hi\n","1848 hi\n","1849 hi\n","1850 hi\n","1851 hi\n","1852 hi\n","1853 hi\n","1854 hi\n","1855 hi\n","1856 hi\n","1857 hi\n","1858 hi\n","1859 hi\n","1860 hi\n","1861 hi\n","1862 hi\n","1863 hi\n","1864 hi\n","1865 hi\n","1866 hi\n","1867 hi\n","1868 hi\n","1869 hi\n","1870 hi\n","1871 hi\n","1872 hi\n","1873 hi\n","1874 hi\n","1875 hi\n","1876 hi\n","1877 hi\n","1878 hi\n","1879 hi\n","1880 hi\n","1881 hi\n","1882 hi\n","1883 hi\n","1884 hi\n","1885 hi\n","1886 hi\n","1887 hi\n","1888 hi\n","1889 hi\n","1890 hi\n","1891 hi\n","1892 hi\n","1893 hi\n","1894 hi\n","1895 hi\n","1896 hi\n","1897 hi\n","1898 hi\n","1899 hi\n","1900 hi\n","1901 hi\n","1902 hi\n","1903 hi\n","1904 hi\n","1905 hi\n","1906 hi\n","1907 hi\n","1908 hi\n","1909 hi\n","1910 hi\n","1911 hi\n","1912 hi\n","1913 hi\n","1914 hi\n","1915 hi\n","1916 hi\n","1917 hi\n","1918 hi\n","1919 hi\n","1920 hi\n","1921 hi\n","1922 hi\n","1923 hi\n","1924 hi\n","1925 hi\n","1926 hi\n","1927 hi\n","1928 hi\n","1929 hi\n","1930 hi\n","1931 hi\n","1932 hi\n","1933 hi\n","1934 hi\n","1935 hi\n","1936 hi\n","1937 hi\n","1938 hi\n","1939 hi\n","1940 hi\n","1941 hi\n","1942 hi\n","1943 hi\n","1944 hi\n","1945 hi\n","1946 hi\n","1947 hi\n","1948 hi\n","1949 hi\n","1950 hi\n","1951 hi\n","1952 hi\n","1953 hi\n","1954 hi\n","1955 hi\n","1956 hi\n","1957 hi\n","1958 hi\n","1959 hi\n","1960 hi\n","1961 hi\n","1962 hi\n","1963 hi\n","1964 hi\n","1965 hi\n","1966 hi\n","1967 hi\n","1968 hi\n","1969 hi\n","1970 hi\n","1971 hi\n","1972 hi\n","1973 hi\n","1974 hi\n","1975 hi\n","1976 hi\n","1977 hi\n","1978 hi\n","1979 hi\n","1980 hi\n","1981 hi\n","1982 hi\n","1983 hi\n","1984 hi\n","1985 hi\n","1986 hi\n","1987 hi\n","1988 hi\n","1989 hi\n","1990 hi\n","1991 hi\n","1992 hi\n","1993 hi\n","1994 hi\n","1995 hi\n","1996 hi\n","1997 hi\n","1998 hi\n","1999 hi\n","hi\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# read the TSV file into a pandas dataframe\n","# df = pd.read_csv('/content/drive/MyDrive/depression-detection-lt-edi-2022/data/preprocessed_dataset/train_multilang.csv', sep='\\t')\n","\n","# en_or_de_column = df[df[\"pid\",\"language_ch\",\"labels\"].isin([\"en\", \"de\"])][\"language_ch\"]\n","# print(en_or_de_column)\n","# # remove rows where the text column is empty\n","# df = df[df['text'].notna()]\n","# #df = df.rename(columns={\"Text\": \"sentence\"})\n","# #df = df.rename(columns={\"final_label\": \"label_name\"})\n","\n","# print(df.info())\n","\n","# # write the modified dataframe back to TSV file\n","# df.to_csv('/content/drive/MyDrive/depression-detection-lt-edi-2022/data/original_dataset/train_multi_clean.tsv', sep='\\t', index=False)\n","import pandas as pd\n","\n","# Load the CSV file into a pandas DataFrame\n","df = pd.read_csv('/content/drive/MyDrive/depression-detection-lt-edi-2022/data/preprocessed_dataset/train_multilang.csv', sep='\\t')\n","\n","# Filter the DataFrame to only include rows where \"language_ch\" is \"en\" or \"de\"\n","filtered_df = df[df[\"language_ch\"].isin([\"en\"])]\n","\n","new_df = filtered_df[[\"pid\", \"translated_text\", \"labels\"]]\n","df = new_df.rename(columns={\"translated_text\": \"text\"})\n","df = df.rename(columns={\"labels\": \"Label\"})\n","df = df[df['text'].notna()]\n","\n","# Write the filtered DataFrame to a new CSV file\n","df.to_csv(\"/content/drive/MyDrive/depression-detection-lt-edi-2022/data/original_dataset/eng_de.tsv\" ,sep='\\t', index=False)"],"metadata":{"id":"JeP3Pzk6dVvP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","# read the TSV file into a pandas dataframe\n","# df = pd.read_csv('/content/drive/MyDrive/depression-detection-lt-edi-2022/data/preprocessed_dataset/train_multilang.csv', sep='\\t')\n","\n","# en_or_de_column = df[df[\"pid\",\"language_ch\",\"labels\"].isin([\"en\", \"de\"])][\"language_ch\"]\n","# print(en_or_de_column)\n","# # remove rows where the text column is empty\n","# df = df[df['text'].notna()]\n","# #df = df.rename(columns={\"Text\": \"sentence\"})\n","# #df = df.rename(columns={\"final_label\": \"label_name\"})\n","\n","# print(df.info())\n","\n","# # write the modified dataframe back to TSV file\n","# df.to_csv('/content/drive/MyDrive/depression-detection-lt-edi-2022/data/original_dataset/train_multi_clean.tsv', sep='\\t', index=False)\n","import pandas as pd\n","\n","# Load the CSV file into a pandas DataFrame\n","df = pd.read_csv('/content/drive/MyDrive/depression-detection-lt-edi-2022/data/original_dataset/train_multilingual_robustness.tsv', sep='\\t')\n","\n","# Filter the DataFrame to only include rows where \"language_ch\" is \"en\" or \"de\"\n","# filtered_df = df[df[\"language_ch\"].isin([\"de\"])]\n","\n","# new_df = filtered_df[[\"pid\", \"translated_text\", \"labels\"]]\n","# df = new_df.rename(columns={\"translated_text\": \"text\"})\n","df = df.rename(columns={\"labels\": \"Label\"})\n","df = df[df['text'].notna()]\n","# num_rows = len(dev)\n","# random_order = np.random.permutation(num_rows)\n","# shuffled_df = dev.iloc[random_order]\n","# dev = shuffled_df.sample(frac=1, replace=False).head(1000)\n","# Write the filtered DataFrame to a new CSV file\n","df.to_csv(\"/content/drive/MyDrive/depression-detection-lt-edi-2022/data/original_dataset/train_multilingual_robust.tsv\" ,sep='\\t', index=False)"],"metadata":{"id":"rlnvSxcXnu4d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","import pandas as pd\n","\n","# Read the CSV files and concatenate them into one DataFrame\n","df1 = pd.read_csv('/content/drive/MyDrive/depression-detection-lt-edi-2022/data/original_dataset/en.tsv', sep='\\t')\n","#df2 = pd.read_csv('/content/drive/MyDrive/depression-detection-lt-edi-2022/data/original_dataset/de.tsv', sep='\\t')\n","df3 = pd.read_csv('/content/drive/MyDrive/depression-detection-lt-edi-2022/data/original_dataset/es.tsv', sep='\\t')\n","df4 = pd.read_csv('/content/drive/MyDrive/depression-detection-lt-edi-2022/data/original_dataset/hi.tsv', sep='\\t')\n","\n","df = pd.concat([df1, df3, df4], ignore_index=True)\n","\n","# Choose 500 random rows from the DataFrame\n","random_indices = random.sample(range(len(df)), 2000)\n","random_df = df.iloc[random_indices]\n","\n","# Append the selected rows to a new CSV file\n","#random_df.to_csv('/content/drive/MyDrive/depression-detection-lt-edi-2022/data/original_dataset/dev_out.tsv', 'a', index=False, header=not bool(random_df.index))\n","random_df.to_csv('/content/drive/MyDrive/depression-detection-lt-edi-2022/data/original_dataset/dev_out.tsv', mode='a', index=False, header=bool(not random_df.index.empty))\n"],"metadata":{"id":"VDm9abr7p8Xc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python deproberta.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"65C8mWLP0kd7","executionInfo":{"status":"ok","timestamp":1680755753584,"user_tz":240,"elapsed":399910,"user":{"displayName":"Aditya Limbekar","userId":"04179261916482495738"}},"outputId":"4d207fea-b57e-4dac-fe38-6aebbc59492e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-04-06 04:29:17.765418: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-04-06 04:29:19.079574: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","INFO:simpletransformers.language_modeling.language_modeling_utils: Creating features from dataset file at trained_models/deproberta_v1/cache\n","100% 263924/263924 [05:53<00:00, 747.60it/s]\n","INFO:simpletransformers.language_modeling.language_modeling_utils: Saving features into cached file trained_models/deproberta_v1/cache/roberta_cached_lm_125_reddit_depression_corpora_train.txt\n","/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","INFO:simpletransformers.language_modeling.language_modeling_model: Training started\n","Epoch 1 of 10:   0% 0/10 [00:00<?, ?it/s]\n","Running Epoch 0 of 10:   0% 0/13516 [00:00<?, ?it/s]\u001b[A\n","Epochs 0/10. Running Loss:    1.5115:   0% 0/13516 [00:01<?, ?it/s]\u001b[A/usr/local/lib/python3.9/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n","\n","Epochs 0/10. Running Loss:    1.5115:   0% 1/13516 [00:02<7:38:22,  2.03s/it]\u001b[A\n","Epochs 0/10. Running Loss:    1.7326:   0% 1/13516 [00:02<9:42:43,  2.59s/it]\n","Epoch 1 of 10:   0% 0/10 [00:02<?, ?it/s]\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/Colab Notebooks/Project_NLP/depression-detection-lt-edi-2022/models/deproberta.py\", line 52, in <module>\n","    deproberta.train()\n","  File \"/content/drive/MyDrive/Colab Notebooks/Project_NLP/depression-detection-lt-edi-2022/models/deproberta.py\", line 34, in train\n","    self.model.train_model(train_data, eval_file=val_data)\n","  File \"/usr/local/lib/python3.9/dist-packages/simpletransformers/language_modeling/language_modeling_model.py\", line 463, in train_model\n","    global_step, training_details = self.train(\n","  File \"/usr/local/lib/python3.9/dist-packages/simpletransformers/language_modeling/language_modeling_model.py\", line 854, in train\n","    scaler.scale(loss).backward()\n","  File \"/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\", line 487, in backward\n","    torch.autograd.backward(\n","  File \"/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\", line 200, in backward\n","    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.20 GiB (GPU 0; 14.75 GiB total capacity; 12.79 GiB already allocated; 1.03 GiB free; 13.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"]}]},{"cell_type":"code","source":["# Define a dictionary to map labels to label IDs\n","label_id_map = {\n","    0: \"severe\",\n","    1: \"moderate\",\n","    2: \"not depression\"\n","}\n","\n","import pandas as pd\n","\n","# Read the TSV file into a pandas DataFrame\n","df = pd.read_csv(\"/content/drive/MyDrive/depression-detection-lt-edi-2022/data/preprocessed_dataset/train_multilingual_robustness.tsv\", sep=\"\\t\")\n","print(df.columns)\n","# Update the column header \"text\" to \"sentence\"\n","df = df.rename(columns={\"labels\": \"Label\"})\n","df['Label'] = df['Label'].map(label_id_map)\n","print(df.info())\n","\n","# Save the updated DataFrame to a new TSV file\n","df.to_csv(\"/content/drive/MyDrive/depression-detection-lt-edi-2022/data/preprocessed_dataset/train_multilingual_robustness_1.csv\", sep=\"\\t\", index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gD9I12Cn8cXn","executionInfo":{"status":"ok","timestamp":1683514309078,"user_tz":240,"elapsed":815,"user":{"displayName":"stephen dias","userId":"16065654066476337384"}},"outputId":"e8e03033-d496-4466-9858-b2301504f243"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['pid', 'text', 'labels'], dtype='object')\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 4027 entries, 0 to 4026\n","Data columns (total 3 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   pid     4027 non-null   object\n"," 1   text    4027 non-null   object\n"," 2   Label   4027 non-null   object\n","dtypes: object(3)\n","memory usage: 94.5+ KB\n","None\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# read the TSV file into a pandas dataframe\n","df = pd.read_csv('/content/drive/MyDrive/depression-detection-lt-edi-2022/data/original_dataset/train_multi.tsv', sep='\\t')\n","\n","# remove rows where the text column is empty\n","df = df[df['text'].notna()]\n","\n","# write the modified dataframe back to TSV file\n","df.to_csv('/content/drive/MyDrive/depression-detection-lt-edi-2022/data/original_dataset/train_multi_clean.tsv', sep='\\t', index=False)"],"metadata":{"id":"-7fhnI2W8cVJ"},"execution_count":null,"outputs":[]}]}